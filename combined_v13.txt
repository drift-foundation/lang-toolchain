[==== FILE LIST ====]
staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_call/acme/lib/lib.drift
staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_call/main.drift
staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_struct_ok/acme/point/lib.drift
staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_throw_catch/acme/boom/lib.drift
staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_throw_catch/main.drift
staged/lang2/codegen/tests/e2e/const_import_basic/lib.drift
staged/lang2/codegen/tests/e2e/const_import_basic/main.drift
staged/lang2/codegen/tests/e2e/const_module_alias_access/lib.drift
staged/lang2/codegen/tests/e2e/cycle_direct/a.drift
staged/lang2/codegen/tests/e2e/cycle_direct/b.drift
staged/lang2/codegen/tests/e2e/cycle_direct/main.drift
staged/lang2/codegen/tests/e2e/cycle_indirect_3way/a.drift
staged/lang2/codegen/tests/e2e/cycle_indirect_3way/b.drift
staged/lang2/codegen/tests/e2e/cycle_indirect_3way/c.drift
staged/lang2/codegen/tests/e2e/cycle_indirect_3way/main.drift
staged/lang2/codegen/tests/e2e/duplicate_export_in_same_module/a.drift
staged/lang2/codegen/tests/e2e/duplicate_export_in_same_module/b.drift
staged/lang2/codegen/tests/e2e/export_ambiguous_const_and_fn/main.drift
staged/lang2/codegen/tests/e2e/import_alias_avoids_conflict/lib.drift
staged/lang2/codegen/tests/e2e/import_alias_avoids_conflict/main.drift
staged/lang2/codegen/tests/e2e/import_alias/lib.drift
staged/lang2/codegen/tests/e2e/import_alias/main.drift
staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/a/lib.drift
staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/b/lib.drift
staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/expected.json
staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/main.drift
staged/lang2/codegen/tests/e2e/import_basic_one_symbol/lib.drift
staged/lang2/codegen/tests/e2e/import_basic_one_symbol/main.drift
staged/lang2/codegen/tests/e2e/import_chain/a.drift
staged/lang2/codegen/tests/e2e/import_chain/b.drift
staged/lang2/codegen/tests/e2e/import_chain/main.drift
staged/lang2/codegen/tests/e2e/import_conflict_with_local_const/expected.json
staged/lang2/codegen/tests/e2e/import_conflict_with_local_const/lib.drift
staged/lang2/codegen/tests/e2e/import_conflict_with_local_const/main.drift
staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/a.drift
staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/b.drift
staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/lib.drift
staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/main.drift
staged/lang2/codegen/tests/e2e/import_missing_module/main.drift
staged/lang2/codegen/tests/e2e/import_missing_symbol/lib.drift
staged/lang2/codegen/tests/e2e/import_missing_symbol/main.drift
staged/lang2/codegen/tests/e2e/import_module_alias_call/lib/lib.drift
staged/lang2/codegen/tests/e2e/import_multiple_files_same_module/lib_part1.drift
staged/lang2/codegen/tests/e2e/import_multiple_files_same_module/lib_part2.drift
staged/lang2/codegen/tests/e2e/import_multiple_files_same_module/main.drift
staged/lang2/codegen/tests/e2e/import_private_const_symbol/lib.drift
staged/lang2/codegen/tests/e2e/import_private_const_symbol/main.drift
staged/lang2/codegen/tests/e2e/import_private_symbol/lib.drift
staged/lang2/codegen/tests/e2e/import_private_symbol/main.drift
staged/lang2/codegen/tests/e2e/import_shadowing_local_decl/lib.drift
staged/lang2/codegen/tests/e2e/import_shadowing_local_decl/main.drift
staged/lang2/codegen/tests/e2e/import_uses_type_and_value_namespaces/lib.drift
staged/lang2/codegen/tests/e2e/import_uses_type_and_value_namespaces/main.drift
staged/lang2/codegen/tests/e2e/local_fn_then_import_conflict/lib.drift
staged/lang2/codegen/tests/e2e/local_fn_then_import_conflict/main.drift
staged/lang2/codegen/tests/e2e/module_scoped_method_names/a/geom/point.drift
staged/lang2/codegen/tests/e2e/module_scoped_method_names/b/geom/point.drift
staged/lang2/codegen/tests/e2e/module_scoped_struct_names/a/geom/point.drift
staged/lang2/codegen/tests/e2e/module_scoped_struct_names/b/geom/point.drift
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/a.drift
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/b1.drift
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/b2.drift
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/b3.drift
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/c.drift
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/expected.json
staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/main.drift
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/a.drift
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/b1.drift
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/b2.drift
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/b3.drift
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/c.drift
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/expected.json
staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/main.drift
staged/lang2/codegen/tests/e2e/reexport_const_smoke/a.drift
staged/lang2/codegen/tests/e2e/reexport_const_smoke/b.drift
staged/lang2/codegen/tests/e2e/reexport_smoke/a.drift
staged/lang2/codegen/tests/e2e/reexport_smoke/b.drift
staged/lang2/codegen/tests/e2e/reexport_smoke/main.drift
staged/lang2/codegen/tests/e2e/reexport_type_smoke/a.drift
staged/lang2/codegen/tests/e2e/reexport_type_smoke/b.drift
staged/lang2/codegen/tests/e2e/reexport_type_smoke/main.drift
staged/lang2/codegen/tests/e2e/two_imports_conflict/expected.json
staged/lang2/codegen/tests/e2e/two_imports_conflict/lib.drift
staged/lang2/codegen/tests/e2e/two_imports_conflict/main.drift
staged/lang2/codegen/tests/e2e/two_imports_conflict/other.drift
staged/lang2/driftc/driftc.py
staged/lang2/driftc/packages/provisional_dmir_v0.py
staged/lang2/driftc/parser/ast.py
staged/lang2/driftc/parser/grammar.lark
staged/lang2/driftc/parser/__init__.py
staged/lang2/driftc/parser/parser.py
staged/lang2/tests/driver/tests/test_abi_boundary_calls.py
staged/lang2/tests/driver/tests/test_driftc_package_v0.py
staged/lang2/tests/driver/tests/test_drift_doctor.py
staged/lang2/tests/driver/tests/test_drift_multisig_policy.py
staged/lang2/tests/driver/tests/test_drift_publish_fetch_vendor.py
staged/lang2/tests/driver/tests/test_drift_sign_cli.py
staged/lang2/tests/driver/tests/test_drift_trust_cli.py
staged/lang2/tests/driver/tests/test_method_resolution_multimodule.py
staged/lang2/tests/driver/tests/test_overload_resolution_multimodule.py
staged/lang2/tests/packages/test_package_root_import_compile.py
staged/lang2/tests/packages/test_package_v0_determinism.py
staged/lang2/tests/packages/test_package_v0_loader.py
staged/lang2/tests/packages/test_package_v0_negative_cases.py
staged/lang2/tests/parser/test_parse_import_export.py
staged/lang2/tests/parser/tests/test_parser_exports.py
staged/work/method-match-x-module/work-progress.md


[==== File: staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_call/acme/lib/lib.drift =====]
module acme.lib

export { answer }

pub fn answer() returns Int {
	return 42
}


[==== File: staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_call/main.drift =====]
module main
import acme.lib as lib


fn main() returns Int {
	return lib.answer();
}


[==== File: staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_struct_ok/acme/point/lib.drift =====]
module acme.point

export { Point, make_point }

pub struct Point(x: Int, y: Int)

pub fn make_point() returns Point {
	return Point(x = 1, y = 2);
}


[==== File: staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_throw_catch/acme/boom/lib.drift =====]
module acme.boom

export { boom }

exception Boom()

pub fn boom() returns Int {
	throw Boom()
}


[==== File: staged/lang2/codegen/tests/e2e/abi_entrypoint_cross_module_throw_catch/main.drift =====]
module main
import acme.boom as boom


fn main() returns Int {
	try boom.boom() catch {
		return 0
	}
	return 1
}


[==== File: staged/lang2/codegen/tests/e2e/const_import_basic/lib.drift =====]
module lib

export { ANSWER }

pub const ANSWER: Int = 42;

[==== File: staged/lang2/codegen/tests/e2e/const_import_basic/main.drift =====]
module main
import lib as lib


fn main() returns Int {
	return lib.ANSWER;
}

[==== File: staged/lang2/codegen/tests/e2e/const_module_alias_access/lib.drift =====]
module lib

export { ANSWER }

pub const ANSWER: Int = 42;

[==== File: staged/lang2/codegen/tests/e2e/cycle_direct/a.drift =====]
module a
import b as b
export { afn }
pub fn afn() returns Int { return b.bfn(); }


[==== File: staged/lang2/codegen/tests/e2e/cycle_direct/b.drift =====]
module b
import a as a
export { bfn }
pub fn bfn() returns Int { return a.afn(); }


[==== File: staged/lang2/codegen/tests/e2e/cycle_direct/main.drift =====]
module main
import a as a
fn main() returns Int { return a.afn(); }


[==== File: staged/lang2/codegen/tests/e2e/cycle_indirect_3way/a.drift =====]
module a
import b as b
export { afn }
pub fn afn() returns Int { return b.bfn(); }


[==== File: staged/lang2/codegen/tests/e2e/cycle_indirect_3way/b.drift =====]
module b
import c as c
export { bfn }
pub fn bfn() returns Int { return c.cfn(); }


[==== File: staged/lang2/codegen/tests/e2e/cycle_indirect_3way/c.drift =====]
module c
import a as a
export { cfn }
pub fn cfn() returns Int { return a.afn(); }


[==== File: staged/lang2/codegen/tests/e2e/cycle_indirect_3way/main.drift =====]
module main
import a as a
fn main() returns Int { return a.afn(); }


[==== File: staged/lang2/codegen/tests/e2e/duplicate_export_in_same_module/a.drift =====]
module m

export { add }

pub fn add() returns Int { return 1; }


[==== File: staged/lang2/codegen/tests/e2e/duplicate_export_in_same_module/b.drift =====]
module m

export { add }

pub fn add() returns Int { return 2; }


[==== File: staged/lang2/codegen/tests/e2e/export_ambiguous_const_and_fn/main.drift =====]
module main

pub const X: Int = 1;

pub fn X() returns Int { return 2; }

export { X }

fn main() returns Int { return 0; }

[==== File: staged/lang2/codegen/tests/e2e/import_alias_avoids_conflict/lib.drift =====]
module lib

export { add }

pub fn add() returns Int { return 42; }

[==== File: staged/lang2/codegen/tests/e2e/import_alias_avoids_conflict/main.drift =====]
module main
import lib as lib


fn add() returns Int { return 0; }

fn main() returns Int { return add() + lib.add(); }

[==== File: staged/lang2/codegen/tests/e2e/import_alias/lib.drift =====]
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int { return a + b; }


[==== File: staged/lang2/codegen/tests/e2e/import_alias/main.drift =====]
module main
import lib as lib


fn main() returns Int {
	return lib.add(40, 2);
}


[==== File: staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/a/lib.drift =====]
module a
export { foo }
pub fn foo() returns Int { return 1; }


[==== File: staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/b/lib.drift =====]
module b
export { foo }
pub fn foo() returns Int { return 2; }


[==== File: staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/expected.json =====]
{
  "exit_code": 1,
  "diagnostics": [
    { "phase": "parser", "message_contains": "import alias" }
  ],
  "module_paths": ["."]
}

[==== File: staged/lang2/codegen/tests/e2e/import_ambiguous_symbol_due_to_glob/main.drift =====]
module main
import a as m
import b as m

fn main() returns Int { return 0; }

[==== File: staged/lang2/codegen/tests/e2e/import_basic_one_symbol/lib.drift =====]
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b;
}


[==== File: staged/lang2/codegen/tests/e2e/import_basic_one_symbol/main.drift =====]
module main
import lib as lib


fn main() returns Int {
	return lib.add(40, 2);
}


[==== File: staged/lang2/codegen/tests/e2e/import_chain/a.drift =====]
module a
import b as b

export { afn }

pub fn afn() returns Int { return b.bfn(); }


[==== File: staged/lang2/codegen/tests/e2e/import_chain/b.drift =====]
module b

export { bfn }

pub fn bfn() returns Int { return 42; }


[==== File: staged/lang2/codegen/tests/e2e/import_chain/main.drift =====]
module main
import a as a


fn main() returns Int { return a.afn(); }


[==== File: staged/lang2/codegen/tests/e2e/import_conflict_with_local_const/expected.json =====]
{
  "exit_code": 42,
  "stdout": "",
  "stderr": ""
}

[==== File: staged/lang2/codegen/tests/e2e/import_conflict_with_local_const/lib.drift =====]
module lib

export { ANSWER }

pub const ANSWER: Int = 42;

[==== File: staged/lang2/codegen/tests/e2e/import_conflict_with_local_const/main.drift =====]
module main
import lib as lib


const ANSWER: Int = 0;

fn main() returns Int {
	return lib.ANSWER;
}

[==== File: staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/a.drift =====]
module main
import lib as lib


fn a() returns Int {
	return lib.add(1, 2);
}

[==== File: staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/b.drift =====]
module main
import lib as lib


fn b() returns Int {
	return lib.sub(10, 3);
}

[==== File: staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/lib.drift =====]
module lib

export { add, sub }

pub fn add(a: Int, b: Int) returns Int { return a + b; }
pub fn sub(a: Int, b: Int) returns Int { return a - b; }

[==== File: staged/lang2/codegen/tests/e2e/import_from_alias_conflict_across_files_ok/main.drift =====]
module main

export { main }

pub fn main() returns Int {
	return a() + b();
}

[==== File: staged/lang2/codegen/tests/e2e/import_missing_module/main.drift =====]
module main
import does_not_exist as does_not_exist


fn main() returns Int { return 0; }


[==== File: staged/lang2/codegen/tests/e2e/import_missing_symbol/lib.drift =====]
module lib

export { ok }

pub fn ok() returns Int { return 1; }


[==== File: staged/lang2/codegen/tests/e2e/import_missing_symbol/main.drift =====]
module main
import lib as lib


fn main() returns Int { return lib.nope; }

[==== File: staged/lang2/codegen/tests/e2e/import_module_alias_call/lib/lib.drift =====]
export { Point, make }

pub struct Point(x: Int, y: Int)

pub fn make(x: Int, y: Int) returns Point {
	return Point(x = x, y = y);
}

[==== File: staged/lang2/codegen/tests/e2e/import_multiple_files_same_module/lib_part1.drift =====]
module lib

export { a }

pub fn a() returns Int { return 1; }


[==== File: staged/lang2/codegen/tests/e2e/import_multiple_files_same_module/lib_part2.drift =====]
module lib

export { b }

pub fn b() returns Int { return 2; }


[==== File: staged/lang2/codegen/tests/e2e/import_multiple_files_same_module/main.drift =====]
module main
import lib as lib


fn main() returns Int {
	return lib.a() + lib.b();
}


[==== File: staged/lang2/codegen/tests/e2e/import_private_const_symbol/lib.drift =====]
module lib

export { PUBLIC }

const SECRET: Int = 1;
pub const PUBLIC: Int = 2;

[==== File: staged/lang2/codegen/tests/e2e/import_private_const_symbol/main.drift =====]
module main
import lib as lib


fn main() returns Int { return lib.SECRET; }

[==== File: staged/lang2/codegen/tests/e2e/import_private_symbol/lib.drift =====]
module lib

export { exposed }

fn hidden() returns Int { return 1; }
pub fn exposed() returns Int { return 2; }


[==== File: staged/lang2/codegen/tests/e2e/import_private_symbol/main.drift =====]
module main
import lib as lib


fn main() returns Int { return lib.hidden(); }


[==== File: staged/lang2/codegen/tests/e2e/import_shadowing_local_decl/lib.drift =====]
module lib
export { add }
pub fn add() returns Int { return 1; }


[==== File: staged/lang2/codegen/tests/e2e/import_shadowing_local_decl/main.drift =====]
module main
import lib as lib


fn lib.add() returns Int { return 0; }
fn main() returns Int { return lib.add(); }


[==== File: staged/lang2/codegen/tests/e2e/import_uses_type_and_value_namespaces/lib.drift =====]
module lib

export { Point, make }

pub struct Point(x: Int, y: Int)

pub fn make() returns Point { return Point(1, 2); }


[==== File: staged/lang2/codegen/tests/e2e/import_uses_type_and_value_namespaces/main.drift =====]
module main
import lib as lib


fn main() returns Int {
	val p: lib.Point = lib.make();
	return 0;
}


[==== File: staged/lang2/codegen/tests/e2e/local_fn_then_import_conflict/lib.drift =====]
module lib

export { add }

pub fn add() returns Int { return 123; }

[==== File: staged/lang2/codegen/tests/e2e/local_fn_then_import_conflict/main.drift =====]
module main
import lib as lib

fn lib.add() returns Int { return 0; }


fn main() returns Int { return lib.add(); }

[==== File: staged/lang2/codegen/tests/e2e/module_scoped_method_names/a/geom/point.drift =====]
module a.geom

export { Point, make }

pub struct Point { x: Int, y: Int }

implement Point {
	fn move_by(self: &mut Point, dx: Int, dy: Int) returns Void {
		self->x += dx
		self->y += dy
	}
}

pub fn make() returns Point {
	return Point(x = 1, y = 0)
}

[==== File: staged/lang2/codegen/tests/e2e/module_scoped_method_names/b/geom/point.drift =====]
module b.geom

export { Point, make }

pub struct Point { x: Int, y: Int }

implement Point {
	fn move_by(self: &mut Point, dx: Int, dy: Int) returns Void {
		self->x += dx
		self->y += dy
	}
}

pub fn make() returns Point {
	return Point(x = 2, y = 0)
}

[==== File: staged/lang2/codegen/tests/e2e/module_scoped_struct_names/a/geom/point.drift =====]
module a.geom

export { Point, make }

pub struct Point { x: Int, y: Int }

pub fn make() returns Point {
	return Point(x = 1, y = 0)
}

[==== File: staged/lang2/codegen/tests/e2e/module_scoped_struct_names/b/geom/point.drift =====]
module b.geom

export { Point, make }

pub struct Point { x: Int, y: Int }

pub fn make() returns Point {
	return Point(x = 2, y = 0)
}

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/a.drift =====]
module a

export { foo }

pub fn foo() returns Int { return 1; }

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/b1.drift =====]
module b
import a as a

export { a.* }

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/b2.drift =====]
module b
import c as c

export { c.* }

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/b3.drift =====]
module b

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/c.drift =====]
module c

export { foo }

pub fn foo() returns Int { return 2; }

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/expected.json =====]
{
  "exit_code": 1,
  "phase": "parser",
  "diagnostics": [
    { "phase": "parser", "message_contains": "ambiguous due to re-exports" }
  ]
}

[==== File: staged/lang2/codegen/tests/e2e/reexport_ambiguous_import_conflict/main.drift =====]
module main
import b as b


fn main() returns Int { return b.foo(); }

[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/a.drift =====]
module a

export { ANSWER }

pub const ANSWER: Int = 1;


[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/b1.drift =====]
module b
import a as a

export { a.* }


[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/b2.drift =====]
module b
import c as c

export { c.* }


[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/b3.drift =====]
module b

[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/c.drift =====]
module c

export { ANSWER }

pub const ANSWER: Int = 2;


[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/expected.json =====]
{
  "exit_code": 1,
  "phase": "parser",
  "diagnostics": [
    { "phase": "parser", "message_contains": "ambiguous due to re-exports" }
  ]
}

[==== File: staged/lang2/codegen/tests/e2e/reexport_const_ambiguous_import_conflict/main.drift =====]
module main
import b as b


fn main() returns Int {
	return b.ANSWER;
}

[==== File: staged/lang2/codegen/tests/e2e/reexport_const_smoke/a.drift =====]
module a

export { ANSWER }

pub const ANSWER: Int = 42;

[==== File: staged/lang2/codegen/tests/e2e/reexport_const_smoke/b.drift =====]
module b
import a as a

export { a.* }

[==== File: staged/lang2/codegen/tests/e2e/reexport_smoke/a.drift =====]
module a
import b as b

export { b.* }

[==== File: staged/lang2/codegen/tests/e2e/reexport_smoke/b.drift =====]
module b

export { add }

pub fn add(a: Int, b: Int) returns Int { return a + b; }

[==== File: staged/lang2/codegen/tests/e2e/reexport_smoke/main.drift =====]
module main
import a as a


fn main() returns Int { return a.add(40, 2); }

[==== File: staged/lang2/codegen/tests/e2e/reexport_type_smoke/a.drift =====]
module a

pub struct Point(x: Int, y: Int)

export { Point, make }

pub fn make() returns Point {
	return Point(x = 1, y = 2);
}

[==== File: staged/lang2/codegen/tests/e2e/reexport_type_smoke/b.drift =====]
module b
import a as a

export { a.* }

[==== File: staged/lang2/codegen/tests/e2e/reexport_type_smoke/main.drift =====]
module main
import b as b


fn main() returns Int {
	val p: b.Point = b.make();
	return p.x + p.y;
}


[==== File: staged/lang2/codegen/tests/e2e/two_imports_conflict/expected.json =====]
{
  "phase": "parser",
  "exit_code": 1,
  "diagnostics": [
    { "phase": "parser", "message_contains": "import alias 'm' conflicts" }
  ]
}

[==== File: staged/lang2/codegen/tests/e2e/two_imports_conflict/lib.drift =====]
module lib

export { add }

pub fn add() returns Int { return 1; }

[==== File: staged/lang2/codegen/tests/e2e/two_imports_conflict/main.drift =====]
module main
import lib as m
import other as m


fn main() returns Int { return 0; }

[==== File: staged/lang2/codegen/tests/e2e/two_imports_conflict/other.drift =====]
module other

export { add }

pub fn add() returns Int { return 2; }

[==== File: staged/lang2/driftc/driftc.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
# author: Sławomir Liszniański; created: 2025-12-04
"""
lang2 driftc stub (checker/driver scaffolding).

This is **not** a full compiler. It exists to document how the lang2 pipeline
should be orchestrated once a real parser/type checker lands:

AST -> HIR (stage0/1)
   -> normalize_hir (stage1) for HIR normalization (no result-try sugar)
   -> HIR->MIR (stage2)
   -> MIR pre-analysis + throw summaries (stage3)
   -> throw checks (stage4) using `declared_can_throw` from the checker

When the real parser/checker is available, this file should grow proper CLI
handling and diagnostics. For now it exposes a single helper
`compile_stubbed_funcs` to drive the existing stages in tests or prototypes.
"""

from __future__ import annotations

import argparse
import json
import sys
import shutil
import subprocess
from pathlib import Path
from dataclasses import replace
from typing import Any, Dict, Mapping, List, Tuple

# Repository root (lang2 lives under this).
ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
	sys.path.insert(0, str(ROOT))

from lang2.driftc import stage1 as H
from lang2.driftc.stage1 import normalize_hir
from lang2.driftc.stage1.lambda_validate import validate_lambdas_non_escaping
from lang2.driftc.stage2 import HIRToMIR, MirBuilder, mir_nodes as M
from lang2.driftc.stage3.throw_summary import ThrowSummaryBuilder
from lang2.driftc.stage4 import run_throw_checks
from lang2.driftc.stage4 import MirToSSA
from lang2.driftc.checker.type_env_builder import build_minimal_checker_type_env
from lang2.driftc.checker import Checker, CheckedProgram, FnSignature, FnInfo
from lang2.driftc.borrow_checker_pass import BorrowChecker
from lang2.driftc.borrow_checker import PlaceBase, PlaceKind
from lang2.driftc.core.diagnostics import Diagnostic
from lang2.driftc.core.types_core import TypeTable
from lang2.driftc.core.function_id import FunctionId, function_symbol
from lang2.driftc.traits.enforce import collect_used_type_keys, enforce_struct_requires, enforce_fn_requires
from lang2.codegen.llvm import lower_module_to_llvm
from lang2.drift_core.runtime import get_runtime_sources
from lang2.driftc.parser import parse_drift_to_hir, parse_drift_files_to_hir, parse_drift_workspace_to_hir
from lang2.driftc.type_resolver import resolve_program_signatures
from lang2.driftc.core.type_resolve_common import resolve_opaque_type
from lang2.driftc.type_checker import TypeChecker
from lang2.driftc.method_registry import CallableRegistry, CallableSignature, Visibility, SelfMode
from lang2.driftc.fake_decl import FakeDecl
from lang2.driftc.packages.dmir_pkg_v0 import canonical_json_bytes, sha256_hex, write_dmir_pkg_v0
from lang2.driftc.packages.provisional_dmir_v0 import encode_module_payload_v0, decode_mir_funcs, type_table_fingerprint
from lang2.driftc.packages.type_table_link_v0 import import_type_tables_and_build_typeid_maps
from lang2.driftc.packages.provider_v0 import (
	PackageTrustPolicy,
	collect_external_exports,
	discover_package_files,
	load_package_v0,
	load_package_v0_with_policy,
)
from lang2.driftc.packages.trust_v0 import TrustStore, load_trust_store_json, merge_trust_stores


def _remap_tid(tid_map: dict[int, int], tid: object) -> object:
	"""
	Remap a TypeId-like integer using `tid_map`.

	This helper is intentionally tiny and defensive. Only fields that are known
	to be TypeIds are remapped, so we don't accidentally rewrite non-TypeId ints
	(e.g., tag values or indices).
	"""
	if isinstance(tid, int):
		return tid_map.get(tid, tid)
	return tid


def _remap_mir_func_typeids(fn: M.MirFunc, tid_map: dict[int, int]) -> None:
	"""
	Remap TypeId fields in a MirFunc in-place.

	Package payloads are produced independently, so their TypeIds must be mapped
	into the host link-time TypeTable before SSA/LLVM lowering.
	"""
	for block in fn.blocks.values():
		for instr in block.instructions:
			if isinstance(instr, M.ZeroValue):
				instr.ty = int(_remap_tid(tid_map, instr.ty))  # type: ignore[assignment]
			elif isinstance(instr, (M.AddrOfArrayElem, M.LoadRef, M.StoreRef)):
				instr.inner_ty = int(_remap_tid(tid_map, instr.inner_ty))  # type: ignore[assignment]
			elif isinstance(instr, M.ConstructStruct):
				instr.struct_ty = int(_remap_tid(tid_map, instr.struct_ty))  # type: ignore[assignment]
			elif isinstance(instr, M.ConstructVariant):
				instr.variant_ty = int(_remap_tid(tid_map, instr.variant_ty))  # type: ignore[assignment]
			elif isinstance(instr, M.VariantTag):
				instr.variant_ty = int(_remap_tid(tid_map, instr.variant_ty))  # type: ignore[assignment]
			elif isinstance(instr, M.VariantGetField):
				instr.variant_ty = int(_remap_tid(tid_map, instr.variant_ty))  # type: ignore[assignment]
				instr.field_ty = int(_remap_tid(tid_map, instr.field_ty))  # type: ignore[assignment]
			elif isinstance(instr, M.StructGetField):
				instr.struct_ty = int(_remap_tid(tid_map, instr.struct_ty))  # type: ignore[assignment]
				instr.field_ty = int(_remap_tid(tid_map, instr.field_ty))  # type: ignore[assignment]
			elif isinstance(instr, M.AddrOfField):
				instr.struct_ty = int(_remap_tid(tid_map, instr.struct_ty))  # type: ignore[assignment]
				instr.field_ty = int(_remap_tid(tid_map, instr.field_ty))  # type: ignore[assignment]
			elif isinstance(instr, (M.ArrayLit, M.ArrayIndexLoad, M.ArrayIndexStore)):
				instr.elem_ty = int(_remap_tid(tid_map, instr.elem_ty))  # type: ignore[assignment]


def _inject_prelude(
	signatures: dict[FunctionId, FnSignature],
	fn_ids_by_name: dict[str, list[FunctionId]],
	type_table: TypeTable,
) -> None:
	"""
	Ensure the lang.core prelude trio is present in the signatures map.

	These are pure functions (not macros) that write UTF-8 text to stdout/stderr.
	They return Void (v2 wires a real Void type through the pipeline).
	"""
	string_id = type_table.ensure_string()
	void_id = type_table.ensure_void()
	for name in ("print", "println", "eprintln"):
		sym_name = name
		# Keyed by short name; module carries qualification.
		if fn_ids_by_name.get(sym_name):
			continue
		fn_id = FunctionId(module="lang.core", name=name, ordinal=0)
		fn_ids_by_name.setdefault(sym_name, []).append(fn_id)
		signatures[fn_id] = FnSignature(
			name=name,
			method_name=name,
			param_names=["text"],
			param_type_ids=[string_id],
			return_type_id=void_id,
			is_method=False,
			module="lang.core",
		)


def _normalize_func_maps(
	func_hirs: Mapping[FunctionId | str, H.HBlock],
	signatures: Mapping[FunctionId | str, FnSignature] | None,
) -> tuple[dict[FunctionId, H.HBlock], dict[FunctionId, FnSignature], dict[str, list[FunctionId]]]:
	if not func_hirs:
		return {}, {}, {}
	first_key = next(iter(func_hirs.keys()))
	if isinstance(first_key, FunctionId):
		fn_ids_by_name: dict[str, list[FunctionId]] = {}
		for fid in func_hirs:
			fn_ids_by_name.setdefault(fid.name, []).append(fid)
		signatures_by_id: dict[FunctionId, FnSignature] = {}
		if signatures:
			signatures_by_id = dict(signatures)  # type: ignore[assignment]
		return dict(func_hirs), signatures_by_id, fn_ids_by_name
	func_hirs_by_id: dict[FunctionId, H.HBlock] = {}
	fn_ids_by_name: dict[str, list[FunctionId]] = {}
	name_ord: dict[str, int] = {}
	for name, block in func_hirs.items():
		ordinal = name_ord.get(name, 0)
		name_ord[name] = ordinal + 1
		fid = FunctionId(module="main", name=name, ordinal=ordinal)
		func_hirs_by_id[fid] = block
		fn_ids_by_name.setdefault(name, []).append(fid)
	signatures_by_id: dict[FunctionId, FnSignature] = {}
	if signatures:
		name_ord.clear()
		for name, sig in signatures.items():
			ids = fn_ids_by_name.get(name, [])
			if ids:
				idx = name_ord.get(name, 0)
				if idx >= len(ids):
					idx = len(ids) - 1
				fid = ids[idx]
			else:
				ordinal = name_ord.get(name, 0)
				fid = FunctionId(module="main", name=name, ordinal=ordinal)
				fn_ids_by_name.setdefault(name, []).append(fid)
			name_ord[name] = name_ord.get(name, 0) + 1
			signatures_by_id[fid] = sig
	return func_hirs_by_id, signatures_by_id, fn_ids_by_name


def _display_name_for_fn_id(fn_id: FunctionId) -> str:
	# Match parser qualification rules: the default `main` module stays
	# unqualified, other modules use `module::name`.
	if fn_id.module == "main":
		return fn_id.name
	return f"{fn_id.module}::{fn_id.name}"


def compile_stubbed_funcs(
	func_hirs: Mapping[FunctionId | str, H.HBlock],
	declared_can_throw: Mapping[str, bool] | None = None,
	signatures: Mapping[FunctionId | str, FnSignature] | None = None,
	exc_env: Mapping[str, int] | None = None,
	return_checked: bool = False,
	build_ssa: bool = False,
	return_ssa: bool = False,
	type_table: "TypeTable | None" = None,
	run_borrow_check: bool = False,
) -> (
	Dict[str, M.MirFunc]
	| tuple[Dict[str, M.MirFunc], CheckedProgram]
	| tuple[Dict[str, M.MirFunc], CheckedProgram, Dict[str, "MirToSSA.SsaFunc"] | None]
):
	"""
	Lower a set of HIR function bodies through the lang2 pipeline and run throw checks.

	Args:
	  func_hirs: mapping of function name -> HIR block (body).
	  declared_can_throw: optional mapping of fn name -> bool; **legacy test shim**.
	    Prefer `signatures` for new tests and treat this as deprecated.
	  signatures: optional mapping of fn name -> FnSignature. The real checker will
	    use parsed/type-checked signatures to derive throw intent; this parameter
	    lets tests mimic that shape without a full parser/type checker.
	  exc_env: optional exception environment (event name -> code) passed to HIRToMIR.
	  return_checked: when True, also return the CheckedProgram produced by the
	    checker so diagnostics/fn_infos can be asserted in integration tests.
	  build_ssa: when True, also run MIR→SSA and derive a TypeEnv from SSA +
	    signatures so the type-aware throw check path is exercised. Loops/backedges
	    are still rejected by the SSA pass. The preferred path is for the checker
	    to supply `checked.type_env`; when absent we ask the checker to infer one
	    from SSA using its TypeTable/signatures.
	  return_ssa: when True (and return_checked=True), also return the SSA funcs
	    computed here. This keeps downstream helpers (e.g., LLVM codegen tests)
	    from re-running MIR→SSA and ensures they share the same SSA graph used
	    in throw checks.
	  run_borrow_check: when True, run the borrow checker on HIR blocks and append
	    diagnostics; this is a stubbed integration path (coarse regions).
	  # TODO: drop declared_can_throw once all callers provide signatures/parsing.

	Returns:
	  dict of function name -> lowered MIR function. When `return_checked` is
	  True, returns a `(mir_funcs, checked_program)` tuple.

	Notes:
	  In the driver path, throw-check violations are appended to
	  `checked.diagnostics`; direct calls to `run_throw_checks` without a
	  diagnostics sink still raise RuntimeError in tests. This helper exists
	  for tests/prototypes; a real CLI will build signatures and diagnostics
	  from parsed sources instead of the shims here.
	"""
	func_hirs_by_id, signatures_by_id, fn_ids_by_name = _normalize_func_maps(func_hirs, signatures)

	# Guard: signatures with TypeIds must come with a shared TypeTable so TypeKind
	# queries stay coherent end-to-end.
	if signatures_by_id and type_table is None:
		for sig in signatures_by_id.values():
			if sig.return_type_id is not None or sig.param_type_ids is not None:
				raise ValueError("signatures with TypeIds require a shared type_table")

	# Important: run the checker on the original HIR (pre-normalization) so it can
	# diagnose surface constructs that are later desugared/rewritten during
	# normalization (structural only). We normalize only after the checker runs,
	# and normalization copies checker-produced annotations (like match binder
	# field indices) forward via `getattr(..., "binder_field_indices", ...)`.

	# If no signatures were supplied, resolve basic signatures from the original HIR.
	shared_type_table = type_table
	if not signatures_by_id:
		shared_type_table, signatures_by_id = resolve_program_signatures(
			_fake_decls_from_hirs(func_hirs_by_id),
			table=shared_type_table,
		)
	else:
		# Ensure TypeIds are resolved on supplied signatures using a shared table.
		if shared_type_table is None:
			shared_type_table = TypeTable()
		for sig in signatures_by_id.values():
			if sig.return_type_id is None and sig.return_type is not None:
				sig.return_type_id = resolve_opaque_type(sig.return_type, shared_type_table, module_id=getattr(sig, "module", None))
			if sig.param_type_ids is None and sig.param_types is not None:
				sig.param_type_ids = [resolve_opaque_type(p, shared_type_table, module_id=getattr(sig, "module", None)) for p in sig.param_types]

	func_hirs_by_symbol: dict[str, H.HBlock] = {}
	signatures_by_symbol: dict[str, FnSignature] = {}
	for fid, block in func_hirs_by_id.items():
		sym = function_symbol(fid)
		func_hirs_by_symbol[sym] = block
	for fid, sig in signatures_by_id.items():
		sym = function_symbol(fid)
		signatures_by_symbol[sym] = replace(sig, name=sym)

	# Stage “checker”: obtain declared_can_throw from the checker stub so the
	# driver path mirrors the real compiler layering once a proper checker exists.
	checker = Checker(
		declared_can_throw=declared_can_throw,
		signatures=signatures_by_symbol,
		exception_catalog=exc_env,
		hir_blocks=func_hirs_by_symbol,
		type_table=shared_type_table,
	)
	# Important: the checker needs metadata for both:
	# - functions we are compiling (have HIR bodies), and
	# - functions we only know by signature (callees, intrinsics, externs).
	#
	# Several downstream phases (HIR→MIR lowering and SSA typing) consult the
	# checker's `FnInfo` map to decide whether a callee is can-throw.
	decl_names: set[str] = set(func_hirs_by_symbol.keys())
	decl_names.update(signatures_by_symbol.keys())
	checked = checker.check(sorted(decl_names))
	# Ensure declared_can_throw is a bool for downstream stages; guard against
	# accidental truthy objects sneaking in from legacy shims.
	for info in checked.fn_infos.values():
		if not isinstance(info.declared_can_throw, bool):
			info.declared_can_throw = bool(info.declared_can_throw)
	declared = {name: info.declared_can_throw for name, info in checked.fn_infos.items()}
	# Prefer the checker's table when the caller did not supply one so TypeIds
	# stay coherent across lowering/codegen.
	if shared_type_table is None and checked.type_table is not None:
		shared_type_table = checked.type_table
	mir_funcs: Dict[str, M.MirFunc] = {}

	# Normalize after typecheck so lowering sees canonical HIR and preserves any
	# checker-produced annotations needed by stage2 (e.g., match binder indices).
	normalized_hirs: Dict[str, H.HBlock] = {name: normalize_hir(hir_block) for name, hir_block in func_hirs_by_symbol.items()}

	for name, hir_norm in normalized_hirs.items():
		builder = MirBuilder(name=name)
		sig = signatures_by_symbol.get(name)
		param_types: dict[str, "TypeId"] = {}
		if sig is not None and sig.param_names is not None:
			builder.func.params = list(sig.param_names)
		if sig is not None and sig.param_names is not None and sig.param_type_ids is not None:
			param_types = {pname: pty for pname, pty in zip(sig.param_names, sig.param_type_ids)}
		HIRToMIR(
			builder,
			type_table=shared_type_table,
			exc_env=exc_env,
			param_types=param_types,
			signatures=signatures_by_symbol,
			can_throw_by_name=declared,
			return_type=sig.return_type_id if sig is not None else None,
		).lower_function_body(hir_norm)
		if sig is not None and sig.param_names is not None:
			builder.func.params = list(sig.param_names)
		mir_funcs[name] = builder.func
		if getattr(builder, "extra_funcs", None):
			for extra in builder.extra_funcs:
				mir_funcs[extra.name] = extra
				if extra.name not in checked.fn_infos and extra.name in signatures_by_symbol:
					info = FnInfo(
						name=extra.name,
						declared_can_throw=bool(getattr(signatures_by_symbol[extra.name], "declared_can_throw", False)),
						signature=signatures_by_symbol[extra.name],
					)
					checked.fn_infos[extra.name] = info
					declared[extra.name] = info.declared_can_throw

	# Stage3: summaries
	code_to_exc = {code: name for name, code in (exc_env or {}).items()}
	summaries = ThrowSummaryBuilder().build(mir_funcs, code_to_exc=code_to_exc)

	# Optional SSA/type-env for typed throw checks
	ssa_funcs = None
	type_env = checked.type_env
	if build_ssa:
		ssa_funcs = {name: MirToSSA().run(func) for name, func in mir_funcs.items()}
		if type_env is None:
			# First preference: checker-owned SSA typing using TypeIds + signatures.
			type_env = checker.build_type_env_from_ssa(ssa_funcs, signatures_by_symbol, can_throw_by_name=declared)
			checked.type_env = type_env
		if type_env is None and signatures_by_symbol:
			# Fallback: minimal checker TypeEnv that tags return SSA values with the
			# signature return TypeId. This keeps type-aware checks usable even when
			# the fuller SSA typing could not derive any facts.
			type_env = build_minimal_checker_type_env(checked, ssa_funcs, signatures_by_symbol, table=checked.type_table)
			checked.type_env = type_env

	# Stage4: throw checks
	run_throw_checks(
		funcs=mir_funcs,
		summaries=summaries,
		declared_can_throw=declared,
		type_env=type_env or checked.type_env,
		fn_infos=checked.fn_infos,
		ssa_funcs=ssa_funcs,
		diagnostics=checked.diagnostics,
	)

	if return_checked and return_ssa:
		return mir_funcs, checked, ssa_funcs
	if return_checked:
		return mir_funcs, checked
	return mir_funcs


def compile_to_llvm_ir_for_tests(
	func_hirs: Mapping[FunctionId | str, H.HBlock],
	signatures: Mapping[FunctionId | str, FnSignature],
	exc_env: Mapping[str, int] | None = None,
	entry: str = "main",
	type_table: "TypeTable | None" = None,
) -> tuple[str, CheckedProgram]:
	"""
	End-to-end helper: HIR -> MIR -> throw checks -> SSA -> LLVM IR for tests.

	This mirrors the stub driver pipeline and finishes by lowering SSA to LLVM IR.
	It is intentionally narrow: assumes a single Drift entry `drift_main` (or
	`entry`) returning `Int`, `String`, or `FnResult<Int, Error>` and uses the
	v1 ABI.
	Returns IR text and the CheckedProgram so callers can assert diagnostics.
	"""
	func_hirs_by_id, signatures_by_id, fn_ids_by_name = _normalize_func_maps(func_hirs, signatures)

	# Ensure prelude signatures are present for tests that bypass the CLI.
	shared_type_table = type_table or TypeTable()
	_inject_prelude(signatures_by_id, fn_ids_by_name, shared_type_table)

	func_hirs_by_symbol: dict[str, H.HBlock] = {}
	signatures_by_symbol: dict[str, FnSignature] = {}
	for fid, block in func_hirs_by_id.items():
		sym = function_symbol(fid)
		func_hirs_by_symbol[sym] = block
	for fid, sig in signatures_by_id.items():
		sym = function_symbol(fid)
		signatures_by_symbol[sym] = replace(sig, name=sym)

	# Mirror the real compiler behavior: any error diagnostics stop the pipeline
	# before MIR/SSA/LLVM lowering. This prevents stage2 assertions from surfacing
	# as user-facing failures in negative tests.
	precheck = Checker(
		declared_can_throw=None,
		signatures=signatures_by_symbol,
		exception_catalog=exc_env,
		hir_blocks=dict(func_hirs_by_symbol),
		type_table=shared_type_table,
	)
	decl_names: set[str] = set(func_hirs_by_symbol.keys())
	decl_names.update(signatures_by_symbol.keys())
	prechecked = precheck.check(sorted(decl_names))
	if any(d.severity == "error" for d in prechecked.diagnostics):
		return "", prechecked

	# First, run the normal pipeline to get MIR + FnInfos + SSA (and diagnostics).
	mir_funcs, checked, ssa_funcs = compile_stubbed_funcs(
		func_hirs=func_hirs_by_id,
		signatures=signatures_by_id,
		exc_env=exc_env,
		return_checked=True,
		build_ssa=True,
		return_ssa=True,
		type_table=shared_type_table,
	)
	if any(d.severity == "error" for d in checked.diagnostics):
		return "", checked

	# Lower module to LLVM IR and append the OS entry wrapper when needed.
	rename_map: dict[str, str] = {}
	argv_wrapper: str | None = None
	entry_info = checked.fn_infos.get(entry)
	# Detect main(argv: Array<String>) and emit a C-ABI wrapper that builds argv.
	if (
		entry == "main"
		and entry_info
		and entry_info.signature
		and entry_info.signature.param_type_ids
		and len(entry_info.signature.param_type_ids) == 1
		and checked.type_table is not None
	):
		param_ty = entry_info.signature.param_type_ids[0]
		td = checked.type_table.get(param_ty)
		if td.kind.name == "ARRAY" and td.param_types:
			elem_td = checked.type_table.get(td.param_types[0])
			if elem_td.name == "String":
				# Guard: require return Int and exactly one param of Array<String>.
				if entry_info.signature.return_type_id != checked.type_table.ensure_int():
					raise ValueError("main(argv: Array<String>) must return Int")
				rename_map["main"] = "drift_main"
				argv_wrapper = "drift_main"

	# Add prelude FnInfos so codegen can recognize console intrinsics by module/name.
	fn_infos = dict(checked.fn_infos)
	for name in ("print", "println", "eprintln"):
		ids = fn_ids_by_name.get(name, [])
		if ids:
			sym = function_symbol(ids[0])
			if sym not in fn_infos and sym in signatures_by_symbol:
				fn_infos[sym] = FnInfo(name=sym, declared_can_throw=False, signature=signatures_by_symbol[sym])

	module = lower_module_to_llvm(
		mir_funcs,
		ssa_funcs,
		fn_infos,
		type_table=checked.type_table,
		rename_map=rename_map,
		argv_wrapper=argv_wrapper,
	)
	# If the entry is already called "main" and has no argv wrapper, do not emit
	# a wrapper that would call itself; otherwise emit a thin OS wrapper that
	# calls the entry.
	if argv_wrapper is None and entry != "main":
		module.emit_entry_wrapper(entry)
	return module.render(), checked


def _fake_decls_from_hirs(hirs: Mapping[FunctionId, H.HBlock]) -> list[object]:
	"""
	Shim: build decl-like objects from HIR blocks so the type resolver can
	construct FnSignatures when real decls are not available.

	This exists only for the stub pipeline; a real front end will provide
	declarations with parsed types and throws clauses.
	"""
	def _scan_returns(block: H.HBlock) -> tuple[bool, bool]:
		"""Return (saw_value_return, saw_void_return)."""
		saw_val = False
		saw_void = False
		for stmt in block.statements:
			if isinstance(stmt, H.HReturn):
				if getattr(stmt, "value", None) is None:
					saw_void = True
				else:
					saw_val = True
			elif isinstance(stmt, H.HIf):
				t_val, t_void = _scan_returns(stmt.then_block)
				s_val = False
				s_void = False
				if stmt.else_block:
					s_val, s_void = _scan_returns(stmt.else_block)
				saw_val = saw_val or t_val or s_val
				saw_void = saw_void or t_void or s_void
			elif isinstance(stmt, H.HLoop):
				b_val, b_void = _scan_returns(stmt.body)
				saw_val = saw_val or b_val
				saw_void = saw_void or b_void
			elif isinstance(stmt, H.HTry):
				b_val, b_void = _scan_returns(stmt.body)
				saw_val = saw_val or b_val
				saw_void = saw_void or b_void
				for arm in stmt.catches:
					a_val, a_void = _scan_returns(arm.block)
					saw_val = saw_val or a_val
					saw_void = saw_void or a_void
		return saw_val, saw_void

	decls: list[FakeDecl] = []
	for fn_id, block in hirs.items():
		ret_ty = "Int"
		if isinstance(block, H.HBlock):
			val_ret, void_ret = _scan_returns(block)
			if void_ret and not val_ret:
				ret_ty = "Void"
		decls.append(FakeDecl(fn_id=fn_id, name=fn_id.name, params=[], return_type=ret_ty))
	return decls


__all__ = ["compile_stubbed_funcs", "compile_to_llvm_ir_for_tests"]


def _diag_to_json(diag: Diagnostic, phase: str, source: Path) -> dict:
	"""Render a Diagnostic to a structured JSON-friendly dict."""
	line = getattr(diag.span, "line", None) if diag.span is not None else None
	column = getattr(diag.span, "column", None) if diag.span is not None else None
	file = None
	if diag.span is not None:
		file = getattr(diag.span, "file", None)
	if file is None:
		file = str(source)
	phase = getattr(diag, "phase", None) or phase
	return {
		"phase": phase,
		"message": diag.message,
		"severity": diag.severity,
		"file": file,
		"line": line,
		"column": column,
	}


def main(argv: list[str] | None = None) -> int:
	"""
	Minimal CLI: parses a Drift file, type checks, then borrow checks. If any stage
	emits errors, compilation fails.

	With --json, prints structured diagnostics (phase/message/severity/file/line/column)
	and an exit_code; otherwise prints human-readable messages to stderr.
	"""
	parser = argparse.ArgumentParser(description="lang2 driftc stub")
	parser.add_argument("source", type=Path, nargs="+", help="Path(s) to Drift source file(s)")
	parser.add_argument(
		"-M",
		"--module-path",
		dest="module_paths",
		action="append",
		type=Path,
		help="Module root directory (repeatable); when provided, module ids are inferred from file paths under these roots",
	)
	parser.add_argument(
		"--package-root",
		dest="package_roots",
		action="append",
		type=Path,
		help="Package root directory (repeatable); used to satisfy imports from local package artifacts",
	)
	parser.add_argument(
		"--trust-store",
		type=Path,
		help="Path to project trust store JSON (default: ./drift/trust.json)",
	)
	parser.add_argument(
		"--no-user-trust-store",
		action="store_true",
		help="Disable user-level trust store fallback (~/.config/drift/trust.json)",
	)
	parser.add_argument(
		"--allow-unsigned-from",
		dest="allow_unsigned_from",
		action="append",
		type=Path,
		help="Allow unsigned packages from this directory (repeatable)",
	)
	parser.add_argument(
		"--require-signatures",
		action="store_true",
		help="Require signatures for all packages (including local build outputs)",
	)
	parser.add_argument("-o", "--output", type=Path, help="Path to output executable")
	parser.add_argument("--emit-ir", type=Path, help="Write LLVM IR to the given path")
	parser.add_argument("--emit-package", type=Path, help="Write an unsigned package artifact (.dmp) to the given path")
	parser.add_argument("--package-id", type=str, help="Package identity (required with --emit-package)")
	parser.add_argument("--package-version", type=str, help="Package version (SemVer; required with --emit-package)")
	parser.add_argument("--package-target", type=str, help="Target triple (required with --emit-package)")
	parser.add_argument("--package-build-epoch", type=str, default=None, help="Optional build epoch label (non-semantic)")
	parser.add_argument(
		"--json",
		action="store_true",
		help="Emit diagnostics as JSON (phase/message/severity/file/line/column)",
	)
	args = parser.parse_args(argv)

	source_paths: list[Path] = list(args.source)
	source_path = source_paths[0]
	# Treat the input set as a workspace, even for a single file, so import
	# resolution behavior is consistent across the CLI and the e2e harness:
	# if user code imports a missing module, we fail early with a parser-phase
	# diagnostic instead of silently compiling a single file in isolation.
	module_paths = list(args.module_paths or []) or None
	loaded_pkgs = []
	external_exports = None
	if args.package_roots:
		# Load trust store(s) for package signature verification.
		#
		# Pinned policy:
		# - project-local trust store is primary: ./drift/trust.json (or --trust-store)
		# - user-level trust store is an optional convenience layer
		# - `driftc` is the final gatekeeper: verification happens at use time
		project_trust_path = args.trust_store or (Path.cwd() / "drift" / "trust.json")
		project_trust = TrustStore(keys_by_kid={}, allowed_kids_by_namespace={}, revoked_kids=set())
		if project_trust_path.exists():
			project_trust = load_trust_store_json(project_trust_path)
		elif args.trust_store is not None:
			# Explicit trust store path is required to exist.
			msg = f"trust store not found: {project_trust_path}"
			if args.json:
				print(
					json.dumps(
						{
							"exit_code": 1,
							"diagnostics": [
								{
									"phase": "package",
									"message": msg,
									"severity": "error",
									"file": str(project_trust_path),
									"line": None,
									"column": None,
								}
							],
						}
					)
				)
			else:
				print(f"{project_trust_path}:?:?: error: {msg}", file=sys.stderr)
			return 1

		merged_trust = project_trust
		if not args.no_user_trust_store:
			user_path = Path.home() / ".config" / "drift" / "trust.json"
			if user_path.exists():
				user_trust = load_trust_store_json(user_path)
				merged_trust = merge_trust_stores(project_trust, user_trust)

		allow_unsigned_roots: list[Path] = []
		# Default local unsigned outputs directory (pinned).
		allow_unsigned_roots.append((Path.cwd() / "build" / "drift" / "localpkgs").resolve())
		for p in list(args.allow_unsigned_from or []):
			allow_unsigned_roots.append(p.resolve())

		policy = PackageTrustPolicy(
			trust_store=merged_trust,
			require_signatures=bool(args.require_signatures),
			allow_unsigned_roots=allow_unsigned_roots,
		)

		package_files = discover_package_files(list(args.package_roots))
		for pkg_path in package_files:
			# Integrity + trust verification happens here, before any package
			# metadata is used for import resolution.
			try:
				loaded_pkgs.append(load_package_v0_with_policy(pkg_path, policy=policy))
			except ValueError as err:
				msg = str(err)
				if args.json:
					print(
						json.dumps(
							{
								"exit_code": 1,
								"diagnostics": [
									{
										"phase": "package",
										"message": msg,
										"severity": "error",
										"file": str(pkg_path),
										"line": None,
										"column": None,
									}
								],
							}
						)
					)
				else:
					print(f"{pkg_path}:?:?: error: {msg}", file=sys.stderr)
				return 1

		# Determinism: package discovery order (filenames, rglob ordering, CLI
		# `--package-root` ordering) must not affect compilation results. Sort loaded
		# packages by the module ids they provide, which is a content-derived key and
		# independent of filesystem paths.
		loaded_pkgs.sort(key=lambda p: tuple(sorted(p.modules_by_id.keys())))

		# Enforce "single version per package id per build".
		pkg_id_map: dict[str, tuple[str, str, str, Path]] = {}  # package_id -> (version, target, sha256, path)
		for pkg in loaded_pkgs:
			man = pkg.manifest
			pkg_id = man.get("package_id")
			pkg_ver = man.get("package_version")
			pkg_target = man.get("target")
			if not isinstance(pkg_id, str) or not isinstance(pkg_ver, str) or not isinstance(pkg_target, str):
				msg = f"package '{pkg.path}' missing package identity fields"
				if args.json:
					print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "package", "message": msg, "severity": "error", "file": str(pkg.path), "line": None, "column": None}]}))
				else:
					print(f"{pkg.path}:?:?: error: {msg}", file=sys.stderr)
				return 1
			pkg_sha = sha256_hex(pkg.path.read_bytes())
			prev = pkg_id_map.get(pkg_id)
			if prev is None:
				pkg_id_map[pkg_id] = (pkg_ver, pkg_target, pkg_sha, pkg.path)
				continue
			prev_ver, prev_target, prev_sha, prev_path = prev
			if pkg_ver != prev_ver or pkg_target != prev_target:
				msg = (
					f"multiple versions/targets for package id '{pkg_id}' in build: "
					f"'{prev_ver}' ({prev_target}) from '{prev_path}' and '{pkg_ver}' ({pkg_target}) from '{pkg.path}'"
				)
				if args.json:
					print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "package", "message": msg, "severity": "error", "file": str(pkg.path), "line": None, "column": None}]}))
				else:
					print(f"{pkg.path}:?:?: error: {msg}", file=sys.stderr)
				return 1
			if pkg_sha != prev_sha and pkg.path != prev_path:
				msg = (
					f"duplicate package id '{pkg_id}' in build from different artifacts: "
					f"'{prev_path}' and '{pkg.path}'"
				)
				if args.json:
					print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "package", "message": msg, "severity": "error", "file": str(pkg.path), "line": None, "column": None}]}))
				else:
					print(f"{pkg.path}:?:?: error: {msg}", file=sys.stderr)
				return 1

		# Reject duplicate module ids across package files early. Unioning exports
		# is unsafe because it can mask collisions and make resolution nondeterministic.
		mod_to_pkg: dict[str, Path] = {}
		for pkg in loaded_pkgs:
			for mid in pkg.modules_by_id.keys():
				prev = mod_to_pkg.get(mid)
				if prev is None:
					mod_to_pkg[mid] = pkg.path
				elif prev != pkg.path:
					msg = f"module '{mid}' provided by multiple packages: '{prev}' and '{pkg.path}'"
					if args.json:
						print(
							json.dumps(
								{
									"exit_code": 1,
									"diagnostics": [
										{
											"phase": "package",
											"message": msg,
											"severity": "error",
											"file": str(source_path),
											"line": None,
											"column": None,
										}
									],
								}
							)
						)
					else:
						print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
					return 1
		external_exports = collect_external_exports(loaded_pkgs)

	func_hirs, signatures, fn_ids_by_name, type_table, exception_catalog, module_exports, module_deps, parse_diags = parse_drift_workspace_to_hir(
		source_paths,
		module_paths=module_paths,
		external_module_exports=external_exports,
	)
	_inject_prelude(signatures, fn_ids_by_name, type_table)
	func_hirs_by_id = func_hirs
	signatures_by_id = signatures
	external_signatures_by_name: dict[str, FnSignature] = {}

	if parse_diags:
		if args.json:
			payload = {
				"exit_code": 1,
				"diagnostics": [_diag_to_json(d, "parser", source_path) for d in parse_diags],
			}
			print(json.dumps(payload))
		else:
			for d in parse_diags:
				loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
				print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
		return 1

	# Prime builtins so TypeTable IDs are stable for package compatibility checks.
	# This must be done before comparing against package payload fingerprints.
	type_table.ensure_int()
	type_table.ensure_uint()
	type_table.ensure_bool()
	type_table.ensure_float()
	type_table.ensure_string()
	type_table.ensure_void()
	type_table.ensure_error()
	type_table.ensure_diagnostic_value()
	# Keep derived Optional<T> ids stable across builds (package embedding).
	type_table.new_optional(type_table.ensure_int())
	type_table.new_optional(type_table.ensure_bool())
	type_table.new_optional(type_table.ensure_string())

	# Verify package TypeTable compatibility before importing signatures/IR.
	# Build link-time TypeId maps for packages and import their type definitions
	# into the host TypeTable. This allows package consumption without requiring
	# identical TypeId assignment across independently-produced artifacts.
	pkg_typeid_maps: dict[Path, dict[int, int]] = {}
	if loaded_pkgs:
		pkg_paths: list[Path] = []
		pkg_tt_objs: list[dict[str, Any]] = []
		for pkg in loaded_pkgs:
			# MVP rule: all modules in a package must share the same encoded type table.
			pkg_tt_obj: dict[str, Any] | None = None
			for mid, mod in pkg.modules_by_id.items():
				payload = mod.payload
				if not isinstance(payload, dict):
					continue
				tt = payload.get("type_table")
				if not isinstance(tt, dict):
					msg = f"package '{pkg.path}' module '{mid}' is missing type_table"
					if args.json:
						print(
							json.dumps(
								{
									"exit_code": 1,
									"diagnostics": [
										{"phase": "package", "message": msg, "severity": "error", "file": str(pkg.path), "line": None, "column": None}
									],
								}
							)
						)
					else:
						print(f"{pkg.path}:?:?: error: {msg}", file=sys.stderr)
					return 1
				if pkg_tt_obj is None:
					pkg_tt_obj = tt
				else:
					if type_table_fingerprint(tt) != type_table_fingerprint(pkg_tt_obj):
						msg = f"package '{pkg.path}' contains inconsistent type_table across modules"
						if args.json:
							print(
								json.dumps(
									{
										"exit_code": 1,
										"diagnostics": [
											{"phase": "package", "message": msg, "severity": "error", "file": str(pkg.path), "line": None, "column": None}
										],
									}
								)
							)
						else:
							print(f"{pkg.path}:?:?: error: {msg}", file=sys.stderr)
						return 1
			if pkg_tt_obj is None:
				continue
			pkg_paths.append(pkg.path)
			pkg_tt_objs.append(pkg_tt_obj)

		try:
			maps = import_type_tables_and_build_typeid_maps(pkg_tt_objs, type_table)
		except ValueError as err:
			msg = f"failed to import package types: {err}"
			if args.json:
				print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "package", "message": msg, "severity": "error", "file": str(source_path), "line": None, "column": None}]}))
			else:
				print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
			return 1
		for path, tid_map in zip(pkg_paths, maps):
			pkg_typeid_maps[path] = tid_map

	# If package roots were provided, merge package signatures into the signature
	# environment so type checking can validate calls to imported functions.
	if loaded_pkgs:
		local_display_names = set(fn_ids_by_name.keys())
		for pkg in loaded_pkgs:
			for _mid, mod in pkg.modules_by_id.items():
				payload = mod.payload
				if not isinstance(payload, dict):
					continue
				sigs_obj = payload.get("signatures")
				if not isinstance(sigs_obj, dict):
					continue
				tid_map = pkg_typeid_maps.get(pkg.path, {})
				for sym, sd in sigs_obj.items():
					if not isinstance(sd, dict):
						continue
					name = str(sd.get("name") or sym)
					if "__impl" in name:
						msg = f"package signature references private symbol {name}; packages must expose only public entrypoints"
						if args.json:
							print(
								json.dumps(
									{
										"exit_code": 1,
										"diagnostics": [
											{
												"phase": "package",
												"message": msg,
												"severity": "error",
												"file": str(source_path),
												"line": None,
												"column": None,
											}
										],
									}
								)
							)
						else:
							print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
						return 1
					if name in local_display_names or name in external_signatures_by_name:
						continue
					module_name = sd.get("module")
					if module_name is None:
						if "::" in name:
							module_name = name.rsplit("::", 1)[0]
						elif "::" in sym:
							module_name = sym.rsplit("::", 1)[0]
						elif args.require_signatures:
							msg = f"package signature '{name}' missing module; signatures must include module or qualified name"
							if args.json:
								print(
									json.dumps(
										{
											"exit_code": 1,
											"diagnostics": [
												{
													"phase": "package",
													"message": msg,
													"severity": "error",
													"file": str(source_path),
													"line": None,
													"column": None,
												}
											],
										}
									)
								)
							else:
								print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
							return 1
					if module_name is not None and "::" not in name:
						name = f"{module_name}::{name}"
					param_type_ids = sd.get("param_type_ids")
					if isinstance(param_type_ids, list):
						param_type_ids = [tid_map.get(int(x), int(x)) for x in param_type_ids]
					ret_tid = sd.get("return_type_id")
					if isinstance(ret_tid, int):
						ret_tid = tid_map.get(ret_tid, ret_tid)
					impl_tid = sd.get("impl_target_type_id")
					if isinstance(impl_tid, int):
						impl_tid = tid_map.get(impl_tid, impl_tid)
					external_signatures_by_name[name] = FnSignature(
						name=name,
						module=module_name,
						method_name=sd.get("method_name"),
						param_names=sd.get("param_names"),
						param_type_ids=param_type_ids,
						return_type_id=ret_tid,
						is_method=bool(sd.get("is_method", False)),
						self_mode=sd.get("self_mode"),
						impl_target_type_id=impl_tid,
						is_exported_entrypoint=bool(sd.get("is_exported_entrypoint", False)),
					)

		# Import package constant tables into the host TypeTable so source code can
		# reference imported consts as typed literals.
		#
		# Const entries in package payloads use package-local TypeIds; remap them
		# through the link-time `tid_map` so the host TypeTable owns the canonical
		# ids used by the rest of the pipeline.
		for pkg in loaded_pkgs:
			tid_map = pkg_typeid_maps.get(pkg.path, {})
			for mid, mod in pkg.modules_by_id.items():
				payload = mod.payload
				if not isinstance(payload, dict):
					continue
				consts_obj = payload.get("consts")
				if not isinstance(consts_obj, dict):
					continue
				for cname, entry in consts_obj.items():
					if not isinstance(cname, str) or not cname:
						continue
					if not isinstance(entry, dict):
						continue
					raw_tid = entry.get("type_id")
					val = entry.get("value")
					if not isinstance(raw_tid, int):
						continue
					remapped_tid = tid_map.get(raw_tid, raw_tid)
					sym = f"{mid}::{cname}"
					prev = getattr(type_table, "consts", {}).get(sym)
					if prev is not None:
						if prev != (remapped_tid, val):
							msg = f"const '{sym}' provided by multiple sources with different values"
							if args.json:
								print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "package", "message": msg, "severity": "error", "file": str(pkg.path), "line": None, "column": None}]}))
							else:
								print(f"{pkg.path}:?:?: error: {msg}", file=sys.stderr)
							return 1
						continue
					type_table.define_const(module_id=mid, name=cname, type_id=remapped_tid, value=val)

	# Materialize const re-exports into the exporting module’s const table.
	#
	# Consts are compile-time values embedded into IR at each use site and also
	# recorded in module interfaces/packages. When a module re-exports an imported
	# const (`from a import ANSWER; export { ANSWER }`), downstream consumers must
	# be able to reference `b::ANSWER` *without* needing module `a` present at
	# compile time.
	#
	# Implementation strategy (MVP):
	# - export-resolution records `reexports.consts` mapping `{local: {module,name}}`
	#   for provenance.
	# - driftc copies the origin const's typed literal `(TypeId, value)` into the
	#   exporting module’s const table under `exporting_mid::local`.
	#
	# This step is performed after package const import because origin const values
	# may come from packages, and their TypeIds must be remapped into the host
	# TypeTable before we can copy them.
	for exporting_mid, exp in (module_exports or {}).items():
		if not isinstance(exp, dict):
			continue
		reexp = exp.get("reexports")
		if not isinstance(reexp, dict):
			continue
		consts_map = reexp.get("consts")
		if not isinstance(consts_map, dict):
			continue
		for local_name, target in consts_map.items():
			if not isinstance(local_name, str) or not local_name:
				continue
			if not isinstance(target, dict):
				continue
			origin_mid = target.get("module")
			origin_name = target.get("name")
			if not isinstance(origin_mid, str) or not origin_mid:
				continue
			if not isinstance(origin_name, str) or not origin_name:
				continue
			origin_sym = f"{origin_mid}::{origin_name}"
			dst_sym = f"{exporting_mid}::{local_name}"
			origin_entry = type_table.lookup_const(origin_sym)
			if origin_entry is None:
				msg = f"re-exported const '{dst_sym}' refers to missing const '{origin_sym}'"
				if args.json:
					print(
						json.dumps(
							{
								"exit_code": 1,
								"diagnostics": [
									{
										"phase": "typecheck",
										"message": msg,
										"severity": "error",
										"file": str(source_path),
										"line": None,
										"column": None,
									}
								],
							}
						)
					)
				else:
					print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
				return 1
			origin_tid, origin_val = origin_entry
			prev = type_table.lookup_const(dst_sym)
			if prev is not None:
				if prev != (origin_tid, origin_val):
					msg = f"const '{dst_sym}' defined with a different value than re-export target '{origin_sym}'"
					if args.json:
						print(
							json.dumps(
								{
									"exit_code": 1,
									"diagnostics": [
										{
											"phase": "typecheck",
											"message": msg,
											"severity": "error",
											"file": str(source_path),
											"line": None,
											"column": None,
										}
									],
								}
							)
						)
					else:
						print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
					return 1
				continue
			type_table.define_const(module_id=exporting_mid, name=local_name, type_id=origin_tid, value=origin_val)

	# Checker (stub) enforces language-level rules (e.g., Void returns) before the
	# lower-level TypeChecker/BorrowChecker run.
	# Normalize HIR before any further analysis so:
	# - sugar does not leak into later stages, and
	# - borrow materialization runs before borrow checking.
	normalized_hirs_by_id = {fn_id: normalize_hir(block) for fn_id, block in func_hirs_by_id.items()}
	func_hirs_by_symbol = {function_symbol(fn_id): block for fn_id, block in normalized_hirs_by_id.items()}
	signatures_by_symbol = {function_symbol(fn_id): sig for fn_id, sig in signatures_by_id.items()}
	signatures_for_checker = dict(signatures_by_symbol)
	signatures_for_checker.update(external_signatures_by_name)
	checker = Checker(
		declared_can_throw=None,
		signatures=signatures_for_checker,
		exception_catalog=exception_catalog,
		hir_blocks=func_hirs_by_symbol,
		type_table=type_table,
	)
	decl_names: set[str] = set(func_hirs_by_symbol.keys())
	decl_names.update(signatures_for_checker.keys())
	checked = checker.check(sorted(decl_names))
	if checked.type_table is not None:
		type_table = checked.type_table
	if checked.diagnostics:
		if args.json:
			payload = {
				"exit_code": 1,
				"diagnostics": [_diag_to_json(d, "typecheck", source_path) for d in checked.diagnostics],
			}
			print(json.dumps(payload))
		else:
			for d in checked.diagnostics:
				loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
				print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
		return 1

	# Type check each function with the shared TypeTable/signatures.
	type_checker = TypeChecker(type_table=type_table)
	callable_registry = CallableRegistry()
	next_callable_id = 1
	type_diags: list[Diagnostic] = []
	module_ids: dict[object, int] = {None: 0}

	display_name_by_id = {fn_id: _display_name_for_fn_id(fn_id) for fn_id in signatures_by_id.keys()}

	for fn_id, sig in signatures_by_id.items():
		if sig.param_type_ids is None or sig.return_type_id is None:
			continue
		param_types_tuple = tuple(sig.param_type_ids)
		module_id = module_ids.setdefault(sig.module, len(module_ids))
		if sig.is_method:
			if sig.impl_target_type_id is None or sig.self_mode is None:
				type_diags.append(
					Diagnostic(
						message=f"method '{display_name_by_id.get(fn_id, sig.name)}' missing receiver metadata (impl target/self_mode)",
						severity="error",
						span=getattr(sig, "loc", None),
					)
				)
				continue
			self_mode = {
				"value": SelfMode.SELF_BY_VALUE,
				"ref": SelfMode.SELF_BY_REF,
				"ref_mut": SelfMode.SELF_BY_REF_MUT,
			}.get(sig.self_mode)
			if self_mode is None:
				type_diags.append(
					Diagnostic(
						message=f"method '{display_name_by_id.get(fn_id, sig.name)}' has unsupported self_mode '{sig.self_mode}'",
						severity="error",
						span=getattr(sig, "loc", None),
					)
				)
				continue
			callable_registry.register_inherent_method(
				callable_id=next_callable_id,
				name=sig.method_name or sig.name,
				module_id=module_id,
				visibility=Visibility.public(),
				signature=CallableSignature(param_types=param_types_tuple, result_type=sig.return_type_id),
				fn_id=fn_id,
				impl_id=next_callable_id,
				impl_target_type_id=sig.impl_target_type_id,
				self_mode=self_mode,
				is_generic=bool(sig.type_params or getattr(sig, "impl_type_params", [])),
			)
			next_callable_id += 1
		else:
			callable_registry.register_free_function(
				callable_id=next_callable_id,
				# Workspace builds qualify call sites (`mod::fn`). Keep the callable
				# registry aligned with that identity to avoid string-rewrite
				# mismatches during resolution.
				name=display_name_by_id.get(fn_id, sig.name),
				module_id=module_id,
				visibility=Visibility.public(),
				signature=CallableSignature(param_types=param_types_tuple, result_type=sig.return_type_id),
				fn_id=fn_id,
				is_generic=bool(sig.type_params),
			)
			next_callable_id += 1

	for sig_name, sig in external_signatures_by_name.items():
		if sig.param_type_ids is None or sig.return_type_id is None:
			continue
		param_types_tuple = tuple(sig.param_type_ids)
		module_id = module_ids.setdefault(sig.module, len(module_ids))
		if sig.is_method:
			if sig.impl_target_type_id is None or sig.self_mode is None:
				type_diags.append(
					Diagnostic(
						message=f"method '{sig_name}' missing receiver metadata (impl target/self_mode)",
						severity="error",
						span=getattr(sig, "loc", None),
					)
				)
				continue
			self_mode = {
				"value": SelfMode.SELF_BY_VALUE,
				"ref": SelfMode.SELF_BY_REF,
				"ref_mut": SelfMode.SELF_BY_REF_MUT,
			}.get(sig.self_mode)
			if self_mode is None:
				type_diags.append(
					Diagnostic(
						message=f"method '{sig_name}' has unsupported self_mode '{sig.self_mode}'",
						severity="error",
						span=getattr(sig, "loc", None),
					)
				)
				continue
			callable_registry.register_inherent_method(
				callable_id=next_callable_id,
				name=sig.method_name or sig_name,
				module_id=module_id,
				visibility=Visibility.public(),
				signature=CallableSignature(param_types=param_types_tuple, result_type=sig.return_type_id),
				fn_id=None,
				impl_id=next_callable_id,
				impl_target_type_id=sig.impl_target_type_id,
				self_mode=self_mode,
				is_generic=bool(sig.type_params or getattr(sig, "impl_type_params", [])),
			)
			next_callable_id += 1
		else:
			callable_registry.register_free_function(
				callable_id=next_callable_id,
				name=sig_name,
				module_id=module_id,
				visibility=Visibility.public(),
				signature=CallableSignature(param_types=param_types_tuple, result_type=sig.return_type_id),
				fn_id=None,
				is_generic=bool(sig.type_params),
			)
			next_callable_id += 1

	# Build a name-keyed map for free-function signatures (fallback path only).
	call_sigs_by_name: dict[str, list[FnSignature]] = {}
	for fn_id, sig in signatures_by_id.items():
		if sig.is_method:
			continue
		name = display_name_by_id.get(fn_id, sig.name)
		call_sigs_by_name.setdefault(name, []).append(sig)
	for sig_name, sig in external_signatures_by_name.items():
		if sig.is_method:
			continue
		call_sigs_by_name.setdefault(sig_name, []).append(sig)

	def _collect_reexport_targets(mod: str) -> set[str]:
		if not isinstance(module_exports, dict):
			return set()
		exp = module_exports.get(mod) or {}
		reexp = exp.get("reexports") if isinstance(exp, dict) else None
		if not isinstance(reexp, dict):
			return set()
		targets: set[str] = set()
		type_reexp = reexp.get("types") if isinstance(reexp.get("types"), dict) else {}
		for kind in ("structs", "variants", "exceptions"):
			entries = type_reexp.get(kind) if isinstance(type_reexp, dict) else None
			if not isinstance(entries, dict):
				continue
			for info in entries.values():
				if isinstance(info, dict):
					tgt = info.get("module")
					if isinstance(tgt, str):
						targets.add(tgt)
		const_reexp = reexp.get("consts") if isinstance(reexp.get("consts"), dict) else {}
		if isinstance(const_reexp, dict):
			for info in const_reexp.values():
				if isinstance(info, dict):
					tgt = info.get("module")
					if isinstance(tgt, str):
						targets.add(tgt)
		return targets

	# Ensure module ids exist for any module mentioned in the workspace graph.
	if isinstance(module_deps, dict):
		all_mods = set(module_deps.keys())
		if isinstance(module_exports, dict):
			all_mods |= set(module_exports.keys())
		for mid in sorted(all_mods):
			module_ids.setdefault(mid, len(module_ids))

	visible_modules_by_name: dict[str, tuple[int, ...]] = {}
	if isinstance(module_deps, dict):
		for mod_name in module_deps.keys():
			visible: set[str] = {mod_name}
			visible |= set(module_deps.get(mod_name, set()))
			queue = list(visible)
			while queue:
				cur = queue.pop()
				for tgt in _collect_reexport_targets(cur):
					if tgt not in visible:
						visible.add(tgt)
						queue.append(tgt)
			visible_ids_list = []
			for m in sorted(visible):
				visible_ids_list.append(module_ids.setdefault(m, len(module_ids)))
			visible_ids = tuple(visible_ids_list)
			visible_modules_by_name[mod_name] = visible_ids

	typed_fns: dict[FunctionId, object] = {}
	for fn_id, hir_block in normalized_hirs_by_id.items():
		# Build param type map from signatures when available.
		param_types: dict[str, "TypeId"] = {}
		sig = signatures_by_id.get(fn_id)
		if sig and sig.param_names and sig.param_type_ids:
			param_types = {pname: pty for pname, pty in zip(sig.param_names, sig.param_type_ids) if pty is not None}
		fn_module_name = sig.module if sig is not None and sig.module is not None else "main"
		fn_module_id = module_ids.setdefault(fn_module_name, len(module_ids))
		visible_modules = visible_modules_by_name.get(fn_module_name, (fn_module_id,))
		result = type_checker.check_function(
			fn_id,
			hir_block,
			param_types=param_types,
			return_type=sig.return_type_id if sig is not None else None,
			call_signatures=call_sigs_by_name,
			callable_registry=callable_registry,
			visible_modules=visible_modules,
			current_module=fn_module_id,
			signatures_by_id=signatures_by_id,
		)
		type_diags.extend(result.diagnostics)
		typed_fns[fn_id] = result.typed_fn

	if type_diags:
		if args.json:
			payload = {
				"exit_code": 1,
				"diagnostics": [_diag_to_json(d, "typecheck", source_path) for d in type_diags],
			}
			print(json.dumps(payload))
			return 1
		else:
			for d in type_diags:
				loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
				print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
			return 1

	# Enforce non-escaping lambda rule after type resolution so method calls are visible.
	signatures_by_display = {display_name_by_id[fn_id]: sig for fn_id, sig in signatures_by_id.items()}
	signatures_for_validation = dict(signatures_by_display)
	signatures_for_validation.update(signatures_by_symbol)
	signatures_for_validation.update(external_signatures_by_name)
	lambda_diags: list[Diagnostic] = []
	for _fn_id, typed_fn in typed_fns.items():
		res = validate_lambdas_non_escaping(
			typed_fn.body,
			signatures=signatures_for_validation,
			call_resolutions=getattr(typed_fn, "call_resolutions", None),
		)
		lambda_diags.extend(res.diagnostics)
	if lambda_diags:
		if args.json:
			payload = {
				"exit_code": 1,
				"diagnostics": [_diag_to_json(d, "typecheck", source_path) for d in lambda_diags],
			}
			print(json.dumps(payload))
		else:
			for d in lambda_diags:
				loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
				print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
		return 1

	# Enforce trait requirements (struct + function requires) before borrow checking.
	trait_diags: list[Diagnostic] = []
	trait_worlds = getattr(type_table, "trait_worlds", {}) if type_table is not None else {}
	if isinstance(trait_worlds, dict) and trait_worlds:
		used_types = collect_used_type_keys(typed_fns, type_table, signatures_by_id)
		used_by_module: dict[str, set] = {}
		used_unknown: set = set()
		for ty in used_types:
			mod = getattr(ty, "module", None)
			if mod is None:
				used_unknown.add(ty)
				continue
			used_by_module.setdefault(mod, set()).add(ty)
		for module_name, world in trait_worlds.items():
			module_used = set(used_by_module.get(module_name, set()))
			module_used.update(used_unknown)
			res = enforce_struct_requires(world, module_used, module_name=module_name)
			trait_diags.extend(res.diagnostics)
		for fn_id, typed_fn in typed_fns.items():
			module_name = fn_id.module or "main"
			world = trait_worlds.get(module_name)
			if world is None:
				continue
			res = enforce_fn_requires(world, typed_fn, type_table, module_name=module_name, signatures=signatures_by_id)
			trait_diags.extend(res.diagnostics)
	if trait_diags:
		if args.json:
			payload = {
				"exit_code": 1,
				"diagnostics": [_diag_to_json(d, "typecheck", source_path) for d in trait_diags],
			}
			print(json.dumps(payload))
		else:
			for d in trait_diags:
				loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
				print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
		return 1

	# Borrow check each typed function (mandatory stage).
	borrow_diags: list[Diagnostic] = []
	signatures_for_hir = dict(external_signatures_by_name)
	signatures_for_hir.update(signatures_by_symbol)
	for _fn_id, typed_fn in typed_fns.items():
		bc = BorrowChecker.from_typed_fn(typed_fn, type_table=type_table, signatures=signatures_for_hir, enable_auto_borrow=True)
		borrow_diags.extend(bc.check_block(typed_fn.body))

	if borrow_diags:
		if args.json:
			payload = {
				"exit_code": 1,
				"diagnostics": [_diag_to_json(d, "borrowcheck", source_path) for d in borrow_diags],
			}
			print(json.dumps(payload))
			return 1
		else:
			for d in borrow_diags:
				loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
				print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
		return 1

	# Package emission mode (Milestone 4): produce an unsigned package artifact
	# containing provisional DMIR payloads for all modules in the workspace.
	if args.emit_package is not None:
		if not args.package_id or not args.package_version or not args.package_target:
			msg = "--emit-package requires --package-id, --package-version, and --package-target"
			if args.json:
				print(
					json.dumps(
						{
							"exit_code": 1,
							"diagnostics": [
								{
									"phase": "package",
									"message": msg,
									"severity": "error",
									"file": str(source_path),
									"line": None,
									"column": None,
								}
							],
						}
					)
				)
			else:
				print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
			return 1

		mir_funcs, checked_pkg = compile_stubbed_funcs(
			func_hirs=func_hirs_by_id,
			signatures=signatures_by_id,
			exc_env=exception_catalog,
			type_table=type_table,
			return_checked=True,
		)
		if any(d.severity == "error" for d in checked_pkg.diagnostics):
			if args.json:
				payload = {
					"exit_code": 1,
					"diagnostics": [_diag_to_json(d, "stage4", source_path) for d in checked_pkg.diagnostics],
				}
				print(json.dumps(payload))
			else:
				for d in checked_pkg.diagnostics:
					loc = f"{getattr(d.span, 'line', '?')}:{getattr(d.span, 'column', '?')}" if d.span else "?:?"
					print(f"{source_path}:{loc}: {d.severity}: {d.message}", file=sys.stderr)
			return 1

		# Group functions/signatures by module id.
		per_module_sigs: dict[str, dict[str, FnSignature]] = {}
		for name, sig in signatures_by_symbol.items():
			mid = getattr(sig, "module", None) or "main"
			per_module_sigs.setdefault(mid, {})[name] = sig

		per_module_mir: dict[str, dict[str, object]] = {}
		for name, fn in mir_funcs.items():
			sig = signatures_by_symbol.get(name)
			mid = getattr(sig, "module", None) if sig is not None else None
			mid = mid or "main"
			per_module_mir.setdefault(mid, {})[name] = fn

		blobs_by_sha: dict[str, bytes] = {}
		blob_types: dict[str, int] = {}
		blob_names: dict[str, str] = {}
		manifest_modules: list[dict[str, object]] = []
		manifest_blobs: dict[str, dict[str, object]] = {}

		all_module_ids: set[str] = set(per_module_sigs.keys()) | set(per_module_mir.keys())
		if isinstance(module_exports, dict):
			all_module_ids |= set(str(k) for k in module_exports.keys())
		for mid in sorted(all_module_ids):
			# MVP packaging: do not bundle the built-in prelude module. It is
			# supplied by the toolchain and will later be distributed as its own
			# package under the `std.*` namespace.
			if mid == "lang.core":
				continue

			# Export surface uses module-local names (unqualified). Global names
			# inside the compiler are qualified (`mid::name`).
			exported_values: list[str] = []
			for sym_name, sig in per_module_sigs.get(mid, {}).items():
				if not getattr(sig, "is_exported_entrypoint", False):
					continue
				if sig.is_method:
					continue
				prefix = f"{mid}::"
				exported_values.append(sym_name[len(prefix) :] if sym_name.startswith(prefix) else sym_name)
			exported_values.sort()

			exported_types_obj: object = {}
			reexports_obj: object = {}
			if isinstance(module_exports, dict):
				mexp = module_exports.get(mid, {})
				if isinstance(mexp, dict):
					exported_types_obj = mexp.get("types", {})
					reexports_obj = mexp.get("reexports", {})
			if not isinstance(exported_types_obj, dict):
				exported_types_obj = {}
			if not isinstance(reexports_obj, dict):
				reexports_obj = {}
			exported_types: dict[str, list[str]] = {
				"structs": list(exported_types_obj.get("structs", [])) if isinstance(exported_types_obj.get("structs"), list) else [],
				"variants": list(exported_types_obj.get("variants", [])) if isinstance(exported_types_obj.get("variants"), list) else [],
				"exceptions": list(exported_types_obj.get("exceptions", [])) if isinstance(exported_types_obj.get("exceptions"), list) else [],
			}
			exported_consts: list[str] = (
				list(module_exports.get(mid, {}).get("consts", [])) if isinstance(module_exports, dict) else []
			)

			payload_obj = encode_module_payload_v0(
				module_id=mid,
				type_table=checked_pkg.type_table or type_table,
				signatures=per_module_sigs.get(mid, {}),
				mir_funcs=per_module_mir.get(mid, {}),
				exported_values=exported_values,
				exported_types=exported_types,
				exported_consts=exported_consts,
				reexports=reexports_obj,
			)

			# Module interface (package interface table v0).
			#
			# This is the authoritative exported surface used by:
			# - the workspace loader for import validation, and
			# - driftc for ABI-boundary enforcement at call sites.
			#
			# Tightening rule: exported values must have corresponding signature
			# entries, and the interface must match the payload exports exactly.
			exported_syms = [f"{mid}::{v}" for v in exported_values]
			payload_sigs = payload_obj.get("signatures") if isinstance(payload_obj, dict) else None
			if not isinstance(payload_sigs, dict):
				payload_sigs = {}
			iface_sigs: dict[str, object] = {}
			for sym in exported_syms:
				sd = payload_sigs.get(sym)
				if not isinstance(sd, dict):
					msg = f"internal: missing signature metadata for exported value '{sym}' while emitting package"
					if args.json:
						print(
							json.dumps(
								{
									"exit_code": 1,
									"diagnostics": [
										{
											"phase": "package",
											"message": msg,
											"severity": "error",
											"file": str(source_path),
											"line": None,
											"column": None,
										}
									],
								}
							)
						)
					else:
						print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
					return 1
				iface_sigs[sym] = sd

			# Exported schemas (exceptions/variants) for the type namespace.
			#
			# These are used as load-time guardrails: exported type schemas must match
			# payload schemas exactly. For MVP, we include schemas only for exported
			# exceptions and variants; structs are validated via TypeTable linking.
			payload_tt = payload_obj.get("type_table") if isinstance(payload_obj, dict) else None
			if not isinstance(payload_tt, dict):
				payload_tt = {}

			iface_exc: dict[str, object] = {}
			payload_exc = payload_tt.get("exception_schemas")
			if isinstance(payload_exc, dict):
				for t in exported_types.get("exceptions", []):
					fqn = f"{mid}:{t}"
					raw = payload_exc.get(fqn)
					if isinstance(raw, list) and len(raw) == 2 and isinstance(raw[1], list):
						iface_exc[fqn] = list(raw[1])

			iface_var: dict[str, object] = {}
			payload_var = payload_tt.get("variant_schemas")
			if isinstance(payload_var, dict):
				for raw in payload_var.values():
					if not isinstance(raw, dict):
						continue
					if raw.get("module_id") != mid:
						continue
					name = raw.get("name")
					if not isinstance(name, str) or name not in exported_types.get("variants", []):
						continue
					iface_var[name] = raw

			iface_obj = {
				"format": "drift-module-interface",
				"version": 0,
				"module_id": mid,
				"exports": payload_obj.get(
					"exports",
					{
						"values": [],
						"types": {"structs": [], "variants": [], "exceptions": []},
						"consts": [],
					},
				),
				"reexports": payload_obj.get("reexports", {}) if isinstance(payload_obj, dict) else {},
				"signatures": iface_sigs,
				"exception_schemas": iface_exc,
				"variant_schemas": iface_var,
				"consts": payload_obj.get("consts", {}) if isinstance(payload_obj, dict) else {},
			}
			iface_bytes = canonical_json_bytes(iface_obj)
			iface_sha = sha256_hex(iface_bytes)
			blobs_by_sha[iface_sha] = iface_bytes
			blob_types[iface_sha] = 2
			blob_names[iface_sha] = f"iface:{mid}"
			manifest_blobs[f"sha256:{iface_sha}"] = {"type": "exports", "length": len(iface_bytes)}

			payload_bytes = canonical_json_bytes(payload_obj)
			payload_sha = sha256_hex(payload_bytes)
			blobs_by_sha[payload_sha] = payload_bytes
			blob_types[payload_sha] = 1
			blob_names[payload_sha] = f"dmir:{mid}"
			manifest_blobs[f"sha256:{payload_sha}"] = {"type": "dmir", "length": len(payload_bytes)}

			manifest_modules.append(
				{
					"module_id": mid,
					"exports": {"values": exported_values, "types": exported_types, "consts": exported_consts},
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			)

		manifest_obj: dict[str, object] = {
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": str(args.package_id),
			"package_version": str(args.package_version),
			"target": str(args.package_target),
			"build_epoch": str(args.package_build_epoch) if args.package_build_epoch else None,
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": manifest_modules,
			"blobs": manifest_blobs,
		}

		write_dmir_pkg_v0(
			args.emit_package,
			manifest_obj=manifest_obj,
			blobs=blobs_by_sha,
			blob_types=blob_types,
			blob_names=blob_names,
		)

		if args.json:
			print(json.dumps({"exit_code": 0, "diagnostics": []}))
		return 0

	# If no codegen requested, acknowledge success.
	if args.output is None and args.emit_ir is None:
		if args.json:
			print(json.dumps({"exit_code": 0, "diagnostics": []}))
		return 0

	# Require entry point main for codegen.
	if not fn_ids_by_name.get("main"):
		msg = "missing entry point 'main' for code generation"
		if args.json:
			print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "codegen", "message": msg, "severity": "error", "file": str(source_path), "line": None, "column": None}]}))
		else:
			print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
		return 1

	if loaded_pkgs:
		# Compile source functions through the normal pipeline to get MIR+SSA.
		src_mir, checked_src, ssa_src = compile_stubbed_funcs(
			func_hirs=func_hirs_by_id,
			signatures=signatures_by_id,
			exc_env=exception_catalog,
			return_checked=True,
			build_ssa=True,
			return_ssa=True,
			type_table=type_table,
		)
		ssa_src = ssa_src or {}

		# Decode package MIR payloads. We intentionally do not blindly embed all
		# loaded package modules; instead we include only the call-graph closure
		# reachable from the source module(s). This keeps builds predictable and
		# avoids unnecessary collisions/work.
		pkg_mir_all: dict[str, M.MirFunc] = {}
		pkg_sigs: dict[str, FnSignature] = {}
		for pkg in loaded_pkgs:
			tid_map = pkg_typeid_maps.get(pkg.path, {})
			for _mid, mod in pkg.modules_by_id.items():
				payload = mod.payload
				if not isinstance(payload, dict):
					continue
				if payload.get("payload_kind") != "provisional-dmir" or payload.get("payload_version") != 0:
					msg = f"unsupported package payload kind/version in {pkg.path}"
					if args.json:
						print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "package", "message": msg, "severity": "error", "file": str(source_path), "line": None, "column": None}]}))
					else:
						print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
					return 1
				mir_obj = payload.get("mir_funcs")
				if isinstance(mir_obj, dict):
					for name, fn in decode_mir_funcs(mir_obj).items():
						if isinstance(fn, M.MirFunc):
							_remap_mir_func_typeids(fn, tid_map)
							pkg_mir_all[name] = fn
				sigs_obj = payload.get("signatures")
				if isinstance(sigs_obj, dict):
					for name, sd in sigs_obj.items():
						if name in pkg_sigs:
							continue
						if not isinstance(sd, dict):
							continue
						param_type_ids = sd.get("param_type_ids")
						if isinstance(param_type_ids, list):
							param_type_ids = [tid_map.get(int(x), int(x)) for x in param_type_ids]
						ret_tid = sd.get("return_type_id")
						if isinstance(ret_tid, int):
							ret_tid = tid_map.get(ret_tid, ret_tid)
						impl_tid = sd.get("impl_target_type_id")
						if isinstance(impl_tid, int):
							impl_tid = tid_map.get(impl_tid, impl_tid)
						pkg_sigs[name] = FnSignature(
							name=str(sd.get("name") or name),
							module=sd.get("module"),
							method_name=sd.get("method_name"),
							param_names=sd.get("param_names"),
							param_type_ids=param_type_ids,
							return_type_id=ret_tid,
							is_method=bool(sd.get("is_method", False)),
							self_mode=sd.get("self_mode"),
							impl_target_type_id=impl_tid,
							is_exported_entrypoint=bool(sd.get("is_exported_entrypoint", False)),
						)

		# SSA for package functions (required for LLVM lowering v1).
		def _called_funcs_in_mir(fn: M.MirFunc) -> set[str]:
			calls: set[str] = set()
			for block in fn.blocks.values():
				for instr in block.instructions:
					if isinstance(instr, M.Call):
						calls.add(instr.fn)
			return calls

		# Roots: any call target from source MIR that is defined by a package.
		needed: set[str] = set()
		for fn in src_mir.values():
			for callee in _called_funcs_in_mir(fn):
				if callee in pkg_mir_all:
					needed.add(callee)

		# Expand to call-graph closure through package functions.
		queue = list(sorted(needed))
		while queue:
			cur = queue.pop()
			fn = pkg_mir_all.get(cur)
			if fn is None:
				continue
			for callee in _called_funcs_in_mir(fn):
				if callee in pkg_mir_all and callee not in needed:
					needed.add(callee)
					queue.append(callee)

		pkg_mir: dict[str, M.MirFunc] = {name: pkg_mir_all[name] for name in sorted(needed)}

		pkg_ssa: dict[str, MirToSSA.SsaFunc] = {}
		for name, fn in pkg_mir.items():
			pkg_ssa[name] = MirToSSA().run(fn)

		# Merge (source wins on symbol conflicts).
		mir_all = dict(pkg_mir)
		mir_all.update(src_mir)
		ssa_all = dict(pkg_ssa)
		ssa_all.update(ssa_src)

		# FnInfos: include source + package signatures so codegen can type calls.
		fn_infos = dict(checked_src.fn_infos)
		all_sig_env = dict(pkg_sigs)
		all_sig_env.update(signatures_by_symbol)
		for name, sig in all_sig_env.items():
			if name in fn_infos:
				continue
			fn_infos[name] = FnInfo(name=name, declared_can_throw=bool(getattr(sig, "declared_can_throw", False)), signature=sig)

		module = lower_module_to_llvm(
			mir_all,
			ssa_all,
			fn_infos,
			type_table=checked_src.type_table,
			rename_map={},
			argv_wrapper=None,
		)
		ir = module.render()
	else:
		ir, _checked = compile_to_llvm_ir_for_tests(
			func_hirs=func_hirs_by_id,
			signatures=signatures_by_id,
			exc_env=exception_catalog,
			entry="main",
			type_table=type_table,
		)

	# Emit IR if requested.
	if args.emit_ir is not None:
		args.emit_ir.parent.mkdir(parents=True, exist_ok=True)
		args.emit_ir.write_text(ir)

	# If only IR emission requested, we are done.
	if args.output is None:
		if args.json:
			print(json.dumps({"exit_code": 0, "diagnostics": []}))
		return 0

	clang = shutil.which("clang-15") or shutil.which("clang")
	if clang is None:
		msg = "clang not available for code generation"
		if args.json:
			print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "codegen", "message": msg, "severity": "error", "file": str(source_path), "line": None, "column": None}]}))
		else:
			print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
		return 1

	args.output.parent.mkdir(parents=True, exist_ok=True)
	ir_path = args.output.with_suffix(".ll")
	ir_path.write_text(ir)

	runtime_sources = [str(p) for p in get_runtime_sources(ROOT)]
	link_cmd = [
		clang,
		"-x",
		"ir",
		str(ir_path),
		"-x",
		"c",
		*runtime_sources,
		"-o",
		str(args.output),
	]
	link_res = subprocess.run(link_cmd, capture_output=True, text=True, cwd=ROOT)
	if link_res.returncode != 0:
		msg = f"clang failed: {link_res.stderr.strip()}"
		if args.json:
			print(json.dumps({"exit_code": 1, "diagnostics": [{"phase": "codegen", "message": msg, "severity": "error", "file": str(source_path), "line": None, "column": None}]}))
		else:
			print(f"{source_path}:?:?: error: {msg}", file=sys.stderr)
		return 1

	if args.json:
		print(json.dumps({"exit_code": 0, "diagnostics": []}))
	return 0


if __name__ == "__main__":
	sys.exit(main())

[==== File: staged/lang2/driftc/packages/provisional_dmir_v0.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
"""
Provisional DMIR payload (v0).

This is an intentionally unstable, compiler-internal IR encoding used for
package artifacts.

Goals:
- deterministic JSON encoding (stable keys, stable ordering),
- sufficiently rich to reconstruct the current stage2 MIR for all functions in a
  module (and the TypeTable required to lower MIR to LLVM later),
- explicit versioning so we can replace this with real DMIR without rewriting
  the package container format.
"""

from __future__ import annotations

import dataclasses
import struct
from enum import Enum
from typing import Any, Mapping

from lang2.driftc.checker import FnSignature
from lang2.driftc.core.generic_type_expr import GenericTypeExpr
from lang2.driftc.core.types_core import TypeDef, TypeId, TypeTable
from lang2.driftc.packages.dmir_pkg_v0 import canonical_json_bytes, sha256_hex


def _float64_bits_hex(value: float) -> str:
	"""Encode a Python float as IEEE754 bits for deterministic JSON."""
	bits = struct.unpack("<Q", struct.pack("<d", value))[0]
	return f"0x{bits:016x}"


def _to_jsonable(obj: Any) -> Any:
	"""
	Convert an arbitrary compiler object into JSONable structures.

	Rules:
	- dataclasses become dicts with a `_type` discriminator,
	- Enums are encoded by `name`,
	- floats are encoded by their IEEE754 bits (hex string),
	- dict keys are converted to strings (and callers must sort when serializing).
	"""
	if obj is None or isinstance(obj, (bool, int, str)):
		return obj
	if isinstance(obj, float):
		return {"_float64": _float64_bits_hex(obj)}
	if isinstance(obj, Enum):
		return {"_enum": type(obj).__name__, "name": obj.name}
	if dataclasses.is_dataclass(obj):
		out: dict[str, Any] = {"_type": type(obj).__name__}
		for f in dataclasses.fields(obj):
			out[f.name] = _to_jsonable(getattr(obj, f.name))
		return out
	if isinstance(obj, (list, tuple)):
		return [_to_jsonable(x) for x in obj]
	if isinstance(obj, dict):
		return {str(k): _to_jsonable(v) for k, v in obj.items()}
	return {"_unsupported": type(obj).__name__, "repr": repr(obj)}


def _float64_from_bits_hex(text: str) -> float:
	"""Decode a float encoded by `_float64_bits_hex`."""
	if text.startswith("0x"):
		text = text[2:]
	bits = int(text, 16)
	return struct.unpack("<d", struct.pack("<Q", bits))[0]


def build_dataclass_registry(*modules: Any) -> dict[str, type]:
	"""
	Build a dataclass name -> class registry.

	This is used to reconstruct stage2 MIR nodes and other internal dataclasses from
	the provisional JSON encoding.
	"""
	out: dict[str, type] = {}
	for mod in modules:
		for v in vars(mod).values():
			if dataclasses.is_dataclass(v):
				out[v.__name__] = v
	return out


def build_enum_registry(*modules: Any) -> dict[str, type[Enum]]:
	"""Build an Enum name -> class registry."""
	out: dict[str, type[Enum]] = {}
	for mod in modules:
		for v in vars(mod).values():
			if isinstance(v, type) and issubclass(v, Enum):
				out[v.__name__] = v
	return out


def from_jsonable(obj: Any, *, dataclasses_by_name: Mapping[str, type], enums_by_name: Mapping[str, type[Enum]]) -> Any:
	"""Reconstruct Python objects encoded by `_to_jsonable`."""
	if obj is None or isinstance(obj, (bool, int, str)):
		return obj
	if isinstance(obj, list):
		return [from_jsonable(x, dataclasses_by_name=dataclasses_by_name, enums_by_name=enums_by_name) for x in obj]
	if isinstance(obj, dict):
		if "_float64" in obj:
			return _float64_from_bits_hex(str(obj["_float64"]))
		if "_enum" in obj:
			enum_name = str(obj.get("_enum"))
			member_name = str(obj.get("name"))
			cls = enums_by_name.get(enum_name)
			if cls is None:
				raise ValueError(f"unknown enum '{enum_name}' in provisional payload")
			return cls[member_name]
		if "_type" in obj:
			type_name = str(obj.get("_type"))
			cls = dataclasses_by_name.get(type_name)
			if cls is None:
				raise ValueError(f"unknown dataclass '{type_name}' in provisional payload")
			kwargs: dict[str, Any] = {}
			for f in dataclasses.fields(cls):
				if f.name in obj:
					kwargs[f.name] = from_jsonable(obj[f.name], dataclasses_by_name=dataclasses_by_name, enums_by_name=enums_by_name)
			return cls(**kwargs)  # type: ignore[misc]
		return {str(k): from_jsonable(v, dataclasses_by_name=dataclasses_by_name, enums_by_name=enums_by_name) for k, v in obj.items()}
	return obj


def encode_type_table(table: TypeTable) -> dict[str, Any]:
	"""Encode the TypeTable deterministically."""

	def _def_to_obj(td: TypeDef) -> dict[str, Any]:
		return {
			"kind": td.kind.name,
			"name": td.name,
			"param_types": list(td.param_types),
			"module_id": td.module_id,
			"ref_mut": td.ref_mut,
			"field_names": list(td.field_names) if td.field_names is not None else None,
		}

	def _encode_generic_type_expr(expr: GenericTypeExpr) -> dict[str, Any]:
		return {
			"name": expr.name,
			"args": [_encode_generic_type_expr(a) for a in expr.args],
			"param_index": expr.param_index,
			"module_id": expr.module_id,
		}

	def _encode_variant_schema(schema: Any) -> dict[str, Any]:
		# `VariantSchema` / `VariantArmSchema` / `VariantFieldSchema` are dataclasses,
		# but we encode them manually so the payload stays stable even if we later
		# refactor internal Python class names.
		return {
			"module_id": schema.module_id,
			"name": schema.name,
			"type_params": list(schema.type_params),
			"arms": [
				{
					"name": arm.name,
					"fields": [{"name": f.name, "type_expr": _encode_generic_type_expr(f.type_expr)} for f in arm.fields],
				}
				for arm in schema.arms
			],
		}

	defs: dict[str, Any] = {}
	for tid in sorted(table._defs.keys()):  # type: ignore[attr-defined]
		defs[str(tid)] = _def_to_obj(table._defs[tid])  # type: ignore[attr-defined]
	variant_schemas: dict[str, Any] = {}
	for base_id in sorted(table.variant_schemas.keys()):
		variant_schemas[str(base_id)] = _encode_variant_schema(table.variant_schemas[base_id])
	return {
		"defs": defs,
		"struct_schemas": [
			{
				"module_id": key.module_id,
				"name": key.name,
				"fields": list(fields),
			}
			for key, (_n, fields) in sorted(
				table.struct_schemas.items(),
				key=lambda kv: ((kv[0].module_id or ""), kv[0].name),
			)
		],
		"exception_schemas": {k: v for k, v in sorted(table.exception_schemas.items())},
		"variant_schemas": variant_schemas,
	}


def type_table_fingerprint(table_obj: Mapping[str, Any]) -> str:
	"""
	Hash a TypeTable JSON object deterministically.

	This is a compatibility guardrail for package consumption: packages produced
	independently must have matching fingerprints, otherwise their TypeIds are not
	comparable and embedding IR would be unsafe.
	"""
	return sha256_hex(canonical_json_bytes(dict(table_obj)))


def encode_signatures(signatures: Mapping[str, FnSignature], *, module_id: str) -> dict[str, Any]:
	"""Encode module-local signatures (deterministic ordering)."""
	out: dict[str, Any] = {}
	for name in sorted(signatures.keys()):
		sig = signatures[name]
		if getattr(sig, "module", None) not in (module_id, None):
			continue
		sig_module = getattr(sig, "module", None) or module_id
		out[name] = {
			"name": sig.name,
			"module": sig_module,
			"is_method": sig.is_method,
			"method_name": getattr(sig, "method_name", None),
			"impl_target_type_id": getattr(sig, "impl_target_type_id", None),
			"self_mode": getattr(sig, "self_mode", None),
			"param_names": list(sig.param_names or []),
			"param_type_ids": list(sig.param_type_ids or []) if sig.param_type_ids is not None else None,
			"return_type_id": sig.return_type_id,
			"is_exported_entrypoint": bool(getattr(sig, "is_exported_entrypoint", False)),
		}
	return out


def encode_module_payload_v0(
	*,
	module_id: str,
	type_table: TypeTable,
	signatures: Mapping[str, FnSignature],
	mir_funcs: Mapping[str, Any],
	exported_values: list[str],
	exported_types: dict[str, list[str]],
	exported_consts: list[str] | None = None,
	reexports: dict[str, Any] | None = None,
) -> dict[str, Any]:
	"""Build the provisional payload object (not yet canonical-JSON encoded)."""
	tt_obj = encode_type_table(type_table)
	consts: list[str] = list(exported_consts or [])
	const_table: dict[str, Any] = {}
	for name in consts:
		sym = f"{module_id}::{name}"
		entry = type_table.lookup_const(sym)
		if entry is None:
			raise ValueError(f"internal: exported const '{sym}' missing from TypeTable const table")
		ty_id, val = entry
		if isinstance(val, bool):
			enc_val: Any = bool(val)
		elif isinstance(val, int):
			enc_val = int(val)
		elif isinstance(val, float):
			enc_val = float(val)
		elif isinstance(val, str):
			enc_val = str(val)
		else:
			raise ValueError(f"internal: unsupported const value type for '{sym}': {type(val).__name__}")
		const_table[name] = {"type_id": int(ty_id), "value": enc_val}
	types_obj = {
		"structs": list(exported_types.get("structs", [])),
		"variants": list(exported_types.get("variants", [])),
		"exceptions": list(exported_types.get("exceptions", [])),
	}
	reexports_obj = reexports if isinstance(reexports, dict) else {}
	return {
		"payload_kind": "provisional-dmir",
		"payload_version": 0,
		"unstable_format": True,
		"module_id": module_id,
		"exports": {"values": list(exported_values), "types": types_obj, "consts": consts},
		"reexports": _to_jsonable(reexports_obj),
		"consts": const_table,
		"type_table": tt_obj,
		"type_table_fingerprint": type_table_fingerprint(tt_obj),
		"signatures": encode_signatures(signatures, module_id=module_id),
		"mir_funcs": {name: _to_jsonable(mir_funcs[name]) for name in sorted(mir_funcs.keys())},
	}


def decode_mir_funcs(mir_funcs_obj: Mapping[str, Any]) -> dict[str, Any]:
	"""
	Decode `mir_funcs` as encoded by `encode_module_payload_v0`.

	This returns a dict of `name -> M.MirFunc` objects (stage2 dataclasses).
	"""
	from lang2.driftc.stage2 import mir_nodes as M  # local import to avoid heavy import at module init

	dc = build_dataclass_registry(M)
	enums = build_enum_registry(M)
	out: dict[str, Any] = {}
	for name, obj in mir_funcs_obj.items():
		out[str(name)] = from_jsonable(obj, dataclasses_by_name=dc, enums_by_name=enums)
	return out

[==== File: staged/lang2/driftc/parser/ast.py =====]
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Sequence


@dataclass
class StructField:
    name: str
    type_expr: "TypeExpr"


@dataclass
class StructDef:
    name: str
    fields: List[StructField]
    loc: "Located"
    is_pub: bool = False
    require: Optional["RequireClause"] = None
    type_params: List[str] = field(default_factory=list)
    type_param_locs: List["Located"] = field(default_factory=list)


@dataclass
class ExceptionArg:
    name: str
    type_expr: "TypeExpr"


@dataclass
class ExceptionDef:
    name: str
    args: List[ExceptionArg]
    loc: "Located"
    is_pub: bool = False
    domain: Optional[str] = None


@dataclass(frozen=True)
class Located:
    line: int
    column: int


@dataclass
class TypeExpr:
    name: str
    args: List["TypeExpr"] = field(default_factory=list)
    # Optional module alias qualifier for type names in surface annotations.
    #
    # Example:
    #   import lib as x
    #   val p: x.Point = ...
    #
    # The compiler resolves `module_alias` using per-file import bindings and
    # then rewrites the type reference to carry a canonical `module_id`.
    #
    # Note: once nominal type identity is module-scoped, the compiler must not
    # discard module qualification. Instead it records the resolved module id in
    # `module_id` so later phases can resolve `(module_id, name)` deterministically.
    module_alias: Optional[str] = None
    # Canonical resolved module id for this type reference (best-effort).
    #
    # For unqualified type names this is usually the current module id, but it
    # can also refer to an imported type (`from other.mod import Point`).
    #
    # Builtins use module_id=None.
    module_id: Optional[str] = None
    # Source location of the type expression (best-effort).
    #
    # This enables source-anchored diagnostics for type-level module qualifiers
    # (e.g. `x.Point`) and internal-only type usage rejections.
    loc: Optional[Located] = None


@dataclass
class Param:
	name: str
	type_expr: TypeExpr | None
	non_escaping: bool = False


@dataclass
class Block:
	statements: List["Stmt"]


class Stmt:
    loc: Located


@dataclass
class LetStmt(Stmt):
    loc: Located
    name: str
    type_expr: Optional[TypeExpr]
    value: "Expr"
    mutable: bool = False
    capture: bool = False
    capture_alias: Optional[str] = None


@dataclass
class AssignStmt(Stmt):
    loc: Located
    target: "Expr"
    value: "Expr"


@dataclass
class AugAssignStmt(Stmt):
	"""
	Augmented assignment statement.

	MVP supports:
	`+=`, `-=`, `*=`, `/=`, `%=`, `&=`, `|=`, `^=`, `<<=`, `>>=`.

	We keep augmented assignment distinct from plain assignment in the AST so
	lowering can preserve correct evaluation semantics for complex lvalues.
	In particular, `a[i] += 1` must evaluate `a` and `i` once (compute address,
	load, add, store), not duplicate the index expression by desugaring to
	`a[i] = a[i] + 1` too early.
	"""

	loc: Located
	target: "Expr"
	op: str
	value: "Expr"


@dataclass
class IfStmt(Stmt):
    loc: Located
    condition: "Expr"
    then_block: "Block"
    else_block: Optional["Block"] = None


@dataclass
class ReturnStmt(Stmt):
    loc: Located
    value: Optional["Expr"]


@dataclass
class RethrowStmt(Stmt):
    loc: Located


@dataclass
class RaiseStmt(Stmt):
    loc: Located
    value: "Expr"
    domain: Optional[str]


@dataclass
class ExprStmt(Stmt):
    loc: Located
    value: "Expr"


@dataclass
class FunctionDef:
	name: str
	orig_name: str
	type_params: List[str]
	params: Sequence[Param]
	return_type: TypeExpr
	body: Block
	loc: Located
	is_pub: bool = False
	type_param_locs: List[Located] = field(default_factory=list)
	require: Optional["RequireClause"] = None
	is_method: bool = False
	self_mode: str | None = None  # "value", "ref", "ref_mut"
	impl_target: TypeExpr | None = None


class Expr:
    loc: Located


@dataclass
class RequireClause:
    expr: "TraitExpr"
    loc: Located


class TraitExpr(Expr):
    """Boolean trait requirement expression (type-level)."""
    pass


@dataclass
class TraitIs(TraitExpr):
    loc: Located
    subject: str
    trait: TypeExpr


@dataclass
class TraitAnd(TraitExpr):
    loc: Located
    left: TraitExpr
    right: TraitExpr


@dataclass
class TraitOr(TraitExpr):
    loc: Located
    left: TraitExpr
    right: TraitExpr


@dataclass
class TraitNot(TraitExpr):
    loc: Located
    expr: TraitExpr


@dataclass
class Literal(Expr):
    loc: Located
    value: object


@dataclass
class Name(Expr):
    loc: Located
    ident: str


@dataclass
class Placeholder(Expr):
    loc: Located


@dataclass
class Attr(Expr):
    loc: Located
    value: Expr
    attr: str
    # Attribute operator used in source:
    # - "." for normal member access (`x.field`)
    # - "->" for member-through-reference access (`p->field`)
    #
    # The parser adapter lowers `->` into an explicit deref + normal member access
    # so later stages do not need to care about this flag (until we formalize
    # richer method receiver semantics).
    op: str = "."


@dataclass
class QualifiedMember(Expr):
	"""
	Type-level qualified member reference: `TypeRef::member`.

	This is intentionally a general expression node (not ctor-only). The typed
	checker determines which kinds of members are valid. In MVP, only variant
	constructor names are supported, and the qualified member must be called.

	`base_type` is a parser `TypeExpr` so later phases can resolve it into a
	concrete `TypeId` (including generic instantiation) without re-parsing.
	"""

	loc: Located
	base_type: TypeExpr
	member: str


@dataclass
class KwArg:
    """
    Keyword argument `name = value`.

    Note: we carry `loc` so later stages can emit precise diagnostics for
    unknown/duplicate keyword names without guessing from the value span.
    """
    name: str
    value: Expr
    loc: Located


@dataclass
class Call(Expr):
    loc: Located
    func: Expr
    args: List[Expr]
    kwargs: List[KwArg]
    type_args: List[TypeExpr] | None = None


@dataclass
class TypeApp(Expr):
    loc: Located
    func: Expr
    type_args: List[TypeExpr]


@dataclass
class Lambda(Expr):
    loc: Located
    params: List[Param]
    ret_type: TypeExpr | None
    body_expr: Expr | None
    body_block: Block | None


@dataclass
class Binary(Expr):
    loc: Located
    op: str
    left: Expr
    right: Expr


@dataclass
class Unary(Expr):
    loc: Located
    op: str
    operand: Expr


@dataclass
class Move(Expr):
    loc: Located
    value: Expr


@dataclass
class Index(Expr):
    loc: Located
    value: Expr
    index: Expr


@dataclass
class ArrayLiteral(Expr):
    loc: Located
    elements: List[Expr]


@dataclass
class FStringHole:
	"""
	Single `{expr[:spec]}` hole inside an f-string.

	Notes:
	- `expr` is a normal Drift expression AST.
	- `spec` is a compile-time substring (MVP: opaque text, no nested `{}`).
	- `loc` points at the start of the hole (the `{`).
	"""
	loc: Located
	expr: Expr
	spec: str = ""


@dataclass
class FString(Expr):
	"""
	f-string expression: `f"..."` with literal parts and `{...}` holes.

	Representation matches the lowering contract: `parts.len == holes.len + 1`.
	"""
	loc: Located
	parts: List[str]
	holes: List[FStringHole]


@dataclass
class Program:
	functions: List[FunctionDef] = field(default_factory=list)
	consts: List["ConstDef"] = field(default_factory=list)
	implements: List["ImplementDef"] = field(default_factory=list)
	traits: List["TraitDef"] = field(default_factory=list)
	# Module directives are tracked separately from general statements so later
	# compilation stages can ignore them without having to add stage0/HIR nodes
	# for import/export syntax.
	imports: List["ImportStmt"] = field(default_factory=list)
	exports: List["ExportStmt"] = field(default_factory=list)
	statements: List[Stmt] = field(default_factory=list)
	structs: List[StructDef] = field(default_factory=list)
	exceptions: List[ExceptionDef] = field(default_factory=list)
	variants: List["VariantDef"] = field(default_factory=list)
	module: Optional[str] = None
	# Location of the `module ...` declaration (when present).
	#
	# This is used to produce pinned diagnostics for module-id mismatch and other
	# module-directive validation rules. When absent, the file implicitly declares
	# its inferred module id (driver-level rule when module roots are provided).
	module_loc: Optional[Located] = None


@dataclass
class ConstDef:
	"""
	Top-level constant definition.

	Syntax:
	  const NAME: Type = expr;

	MVP constraints are enforced in the typed checker:
	- `expr` must be a compile-time literal (or unary +/- applied to a numeric literal),
	- the declared type must match exactly.
	"""

	loc: Located
	name: str
	type_expr: TypeExpr
	value: "Expr"
	is_pub: bool = False


@dataclass
class VariantField:
	"""
	Single constructor field in a `variant` arm.

	Example:
	  Some(value: T)

	`name` is the declared field name; patterns are positional-only in MVP, but
	we keep names for clarity and future evolution.
	"""

	name: str
	type_expr: TypeExpr


@dataclass
class VariantArm:
	"""Single arm (constructor) inside a `variant` definition."""

	name: str
	fields: List[VariantField]
	loc: Located


@dataclass
class VariantDef:
	"""
	Top-level variant (tagged union / sum type) definition.

	MVP supports generic type parameters, e.g.:
	  variant Optional<T> { Some(value: T), None }
	"""

	name: str
	type_params: List[str]
	arms: List[VariantArm]
	loc: Located
	is_pub: bool = False


@dataclass
class TraitMethodSig:
	name: str
	params: Sequence[Param]
	return_type: TypeExpr
	loc: Located


@dataclass
class TraitDef:
	name: str
	methods: List[TraitMethodSig]
	loc: Located
	is_pub: bool = False
	require: Optional["RequireClause"] = None


@dataclass
class ImplementDef:
    target: TypeExpr
    loc: Located
    is_pub: bool = False
    type_params: List[str] = field(default_factory=list)
    type_param_locs: List[Located] = field(default_factory=list)
    trait: Optional[TypeExpr] = None
    require: Optional["RequireClause"] = None
    methods: List[FunctionDef] = field(default_factory=list)


@dataclass
class ImportStmt(Stmt):
    loc: Located
    path: List[str]
    alias: Optional[str] = None


@dataclass
class ExportItem:
	loc: Located


@dataclass
class ExportName(ExportItem):
	name: str


@dataclass
class ExportModuleStar(ExportItem):
	module_path: List[str]


@dataclass
class ExportStmt(Stmt):
	"""
	Explicit export list for a module (MVP).

	Syntax:
	  export { foo, Bar, Baz }
	  export { other.module.* }
	"""

	loc: Located
	items: List[ExportItem]


@dataclass
class CatchClause:
    event: Optional[str]
    binder: Optional[str]
    block: Block
    event_code: Optional[int] = None
    arg_order: Optional[list[str]] = None


@dataclass
class TryStmt(Stmt):
    loc: Located
    body: Block
    catches: List[CatchClause]

@dataclass
class WhileStmt(Stmt):
    loc: Located
    condition: "Expr"
    body: Block


@dataclass
class ForStmt(Stmt):
    loc: Located
    var: str
    iter_expr: "Expr"
    body: Block

@dataclass
class BreakStmt(Stmt):
    loc: Located

@dataclass
class ContinueStmt(Stmt):
    loc: Located

@dataclass
class ThrowStmt(Stmt):
    loc: Located
    expr: "Expr"


@dataclass
class ExceptionCtor(Expr):
    """
    Semantic node representing an exception constructor application.

    This is *not* a general expression in the language: it only appears under a
    `throw` statement (see grammar). We still represent it as an Expr node so it
    can share parsing infrastructure with calls and literals.

    Args may be positional or keyword. Positional arguments must precede keyword
    arguments (the parser enforces this).
    """

    loc: Located
    name: str
    args: List[Expr]
    kwargs: List[KwArg]


@dataclass
class CatchExprArm:
    event: Optional[str]
    binder: Optional[str]
    block: Block


@dataclass
class TryCatchExpr(Expr):
    loc: Located
    attempt: Expr
    catch_arms: List[CatchExprArm]


@dataclass
class Ternary(Expr):
    loc: Located
    condition: Expr
    then_value: Expr
    else_value: Expr


@dataclass
class MatchArm:
	"""
	Single match arm.

	Patterns are:
	- constructor pattern: `Ctor(b1, b2, ...)`
	- zero-field constructor: `Ctor`
	- default: `default`

	Arm bodies are blocks; `value_block` is represented as a trailing ExprStmt.
	"""

	loc: Located
	ctor: Optional[str]  # None means default arm
	# Pattern argument form:
	# - "bare": `Ctor` (allowed only for zero-field constructors)
	# - "paren": `Ctor()` (tag-only match, ignores payload)
	# - "positional": `Ctor(a, b)` (binds fields by index, exact arity)
	# - "named": `Ctor(x = a, y = b)` (binds a subset of fields by name)
	pattern_arg_form: str
	binders: List[str]
	block: Block
	# Field names for named binders, parallel to `binders`. Only meaningful when
	# `pattern_arg_form == "named"`.
	binder_fields: Optional[List[str]] = None


@dataclass
class MatchExpr(Expr):
	"""Expression-form match."""

	loc: Located
	scrutinee: Expr
	arms: List[MatchArm]

[==== File: staged/lang2/driftc/parser/grammar.lark =====]
// Drift Language Grammar 1.0
// This file defines the surface syntax accepted by the Drift compiler frontend.

// Identifiers: allow leading underscore, but reserve double-underscore (compiler use).
// Give identifiers a lower priority than keyword terminals so the basic lexer
// prefers `and`, `or`, `val`, etc. as their own tokens instead of `NAME`.
NAME.0: /(?!type\b)(?!pub\b)[A-Za-z][A-Za-z0-9_]*/ | /_[A-Za-z0-9_]*(?<!__)/
%import common.SIGNED_INT
// Float literals: decimal only, digits on both sides of '.', optional exponent.
// Important: float literals never include a leading sign; unary '-' is parsed separately.
FLOAT.2: /[0-9]+\.[0-9]+([eE][+-]?[0-9]+)?/
// String literal with basic escapes plus \xHH hex byte escapes (UTF-8).
STRING: /"(?:[^"\\]|\\["\\ntbrf]|\\x[0-9A-Fa-f]{2})*"/
// f-string prefix: `f` immediately followed by a quote starts an f-string literal.
// This must not steal a standalone identifier named `f`, so we use a lookahead
// that only matches when the next character is the quote.
FSTRING_PREFIX.3: /f(?=\")/
%import common.WS_INLINE

%declare TERMINATOR

SEMI: ";"
NEWLINE: /(\r?\n[ \t]*)/

VAL: "val"
VAR: "var"
CONST: "const"
PUB.2: "pub"
MUT: "mut"
TYPE.2: "type"
CALL_TYPE_LT.2: /<\s*type\b/
THROW: "throw"
VARIANT: "variant"
MATCH: "match"
DEFAULT: "default"
NONESCAPING: "nonescaping"
# Note: `>>=` must be recognized before `>>` (longest match).
SHR_EQ: ">>="
SHR: ">>"
# Pipeline operator (functional-style staging).
PIPE_FWD: "|>"
# Reserved for future reverse pipeline; no semantics in v1.
PIPE_REV: "<|"
# Note: `<<=` / `<<` must be recognized before `<` / `<=`.
LSHIFT_EQ: "<<="
LSHIFT: "<<"
AND.2: "and"
OR.2: "or"
EQEQ: "=="
NOTEQ: "!="
IS: "is"
LTE: "<="
GTE: ">="
LT: "<"
GT: ">"

%declare TYPE_LT TYPE_GT QUAL_TYPE_LT QUAL_TYPE_GT
PLUS: "+"
PLUS_EQ: "+="
MINUS: "-"
MINUS_EQ: "-="
STAR_EQ: "*="
SLASH_EQ: "/="
PERCENT_EQ: "%="
AMP_EQ: "&="
BAR_EQ: "|="
CARET_EQ: "^="
STAR: "*"
SLASH: "/"
PERCENT: "%"
DOT: "."
DCOLON: "::"
COLON: ":"
COMMA: ","
AMP: "&"
BAR: "|"
EQUAL: "="
CARET: "^"
TILDE: "~"
RETURNS: "returns"
BANG: "!"
IF: "if"
ELSE: "else"
WHILE: "while"
MOVE: "move"
BREAK: "break"
CONTINUE: "continue"
RETHROW: "rethrow"
ARROW: "->"
FATARROW: "=>"
EXCEPTION: "exception"
TRAIT: "trait"
REQUIRE: "require"
TRY: "try"
CATCH: "catch"
EXPORT: "export"
QMARK: "?"
MODULE: "module"
LINE_COMMENT: /\/\/[^\n]*/
BLOCK_COMMENT: /\/\*([^*]|\*(?!\/))*\*\//

%ignore WS_INLINE
%ignore LINE_COMMENT
%ignore BLOCK_COMMENT

program: (module_decl | item | TERMINATOR)*

module_decl: MODULE module_path
module_path: NAME (DOT NAME)*
ident: NAME | MOVE

?item: pub_item
     | func_def
     | const_def
     | struct_def
     | exception_def
     | variant_def
     | trait_def
     | implement_def
     | import_stmt
     | export_stmt
     | stmt

pub_item: PUB (func_def | const_def | struct_def | exception_def | variant_def | trait_def | implement_def)

# Top-level constant definition.
#
# MVP constraints (enforced by the typed checker):
# - const initializers must be compile-time literals (or unary +/- of a numeric literal),
# - the declared type must match the initializer exactly,
# - consts are values (not types) and may be exported/imported like functions.
const_def: CONST NAME COLON type_expr EQUAL expr TERMINATOR

func_def: "fn" ident type_params? "(" [params] ")" return_sig require_clause? block
return_sig: RETURNS type_expr

params: param (COMMA param)*
param: NONESCAPING? ident COLON type_expr

# implement Trait for Type { fn ... }
implement_def: "implement" type_params? type_expr ("for" type_expr)? require_clause? implement_body
implement_body: "{" (implement_item | TERMINATOR)* "}"
implement_item: func_def   -> implement_func

trait_def: TRAIT NAME require_clause? trait_body
trait_body: "{" (trait_item | TERMINATOR)* "}"
trait_item: trait_method_sig TERMINATOR*
trait_method_sig: "fn" ident "(" [params] ")" return_sig

type_expr: ref_type
         | base_type

ref_type: AMP MUT? type_expr
// Base (nominal) type reference.
//
// MVP supports a single module-alias qualifier in type positions:
//   import lib as x
//   val p: x.Point = ...
//
// This is *not* a general module path type syntax. Only one dot is allowed
// (alias + name); direct `a.b.c` type paths are deferred.
base_type: NAME DOT NAME type_args? -> qualified_base_type
         | NAME type_args?          -> base_type
type_args: square_type_args
         | angle_type_args
square_type_args: "[" type_expr (COMMA type_expr)* "]"
angle_type_args: ("<" | TYPE_LT) type_expr (COMMA type_expr)* (">" | TYPE_GT)

block: "{" (stmt | TERMINATOR)* "}"

// A "value block" is a braced block that evaluates to a value.
// It allows zero or more statements, followed by a trailing expression that
// does not require a terminator (though one may be present).
//
// This is used by expression-form try/catch arms:
//   try foo() catch { log(e); 0 }
value_block: "{" (stmt | TERMINATOR)* expr "}"

stmt: if_stmt
    | try_stmt
    | simple_stmt TERMINATOR

simple_stmt: let_stmt
	           | return_stmt
	           | rethrow_stmt
	           | raise_stmt
	           | break_stmt
	           | continue_stmt
	           | while_stmt
	           | for_stmt
	           | aug_assign_stmt
	           | assign_stmt
	           | expr_stmt

// Disambiguation: `try ... catch ...` is both a statement form (`try_stmt`) and
// an expression form (`try_catch_expr`). In statement position we want the
// parser to prefer `try_stmt` so `try foo() catch { ... }` behaves like the
// statement construct (and does not get parsed as an expression statement).
try_stmt.2: TRY (block | expr) terminator_opt catch_clause (terminator_opt catch_clause)* terminator_opt
catch_clause: CATCH catch_pattern block
catch_pattern: event_fqn "(" ident ")"   -> catch_event
             | ident                     -> catch_all
             |                          -> catch_all_empty

// Assignments are restricted to lvalue-ish postfix expressions. This also
// prevents `try ... catch ...` from being parsed as the LHS of an assignment,
// which would otherwise interfere with statement-form `try`.
//
// Borrow MVP needs `*p = ...` to support `&mut` writes, so we also allow a
// dereference assignment target.
assign_stmt: assign_target EQUAL expr
aug_assign_stmt: assign_target (PLUS_EQ | MINUS_EQ | STAR_EQ | SLASH_EQ | PERCENT_EQ | AMP_EQ | BAR_EQ | CARET_EQ | LSHIFT_EQ | SHR_EQ) expr
assign_target: postfix
             | STAR assign_target -> deref

for_stmt: "for" ident "in" expr terminator_opt block

if_stmt: IF if_cond terminator_opt block else_clause? terminator_opt
if_cond: trait_expr | expr
else_clause: terminator_opt ELSE terminator_opt block
break_stmt: BREAK
continue_stmt: CONTINUE
while_stmt: WHILE expr terminator_opt block
terminator_opt: TERMINATOR*

let_stmt: binder binding_name type_spec? alias_clause? EQUAL expr
binder: VAL
      | VAR
binding_name: capture_marker? ident
capture_marker: CARET
alias_clause: "as" STRING
type_spec: COLON type_expr

require_clause: REQUIRE trait_expr (COMMA trait_expr)*

return_stmt: "return" expr?
rethrow_stmt: RETHROW

raise_stmt: ("raise" domain_clause? expr)
          | (THROW exception_ctor)
          | (THROW expr)

// Expression statements are restricted to postfix expressions (calls, member
// access, indexing, literals, names). This avoids ambiguity with statement-form
// `try` and keeps expression-form `try ... catch ...` as a value-producing
// expression (not a statement).
expr_stmt: postfix

// Module directives (tracked separately from normal statements).
import_stmt: "import" module_path import_alias?
import_alias: "as" NAME

export_stmt: EXPORT "{" export_items? "}"
export_items: export_item (COMMA export_item)*
export_item: NAME | module_path_star
module_path_star: NAME (DOT NAME)* DOT STAR

struct_def: "struct" NAME type_params? require_clause? struct_body
struct_body: tuple_struct | block_struct
tuple_struct: "(" struct_field_list? ")"
struct_field_list: struct_field (COMMA struct_field)*
struct_field: NAME COLON type_expr
block_struct: "{" TERMINATOR* (block_field TERMINATOR*)* "}"
block_field: struct_field [COMMA]

exception_def: EXCEPTION NAME "(" [exception_params] ")"
exception_params: exception_param (COMMA exception_param)*
exception_param: NAME COLON type_expr
                | "domain" EQUAL STRING -> exception_domain_param

domain_clause: "domain" NAME

// Variant (tagged union / sum type).
//
// MVP supports:
// - positional constructor field lists (`Some(value: T, other: U)`),
// - generic type parameters (`variant Optional<T> { ... }`).
variant_def: VARIANT NAME type_params? variant_body
type_params: ("<" | TYPE_LT) NAME (COMMA NAME)* (">" | TYPE_GT)
variant_body: "{" (variant_arm (COMMA | TERMINATOR)*)+ "}"
variant_arm: NAME variant_fields?
variant_fields: "(" [variant_field_list] ")"
variant_field_list: variant_field (COMMA variant_field)*
variant_field: NAME COLON type_expr

// `expr` is the full expression language (including `try ... catch ...`).
?expr: try_catch_expr
     | match_expr
     | ternary
     | pipeline

// `match` expression (expression-only in MVP). Each arm body is a block:
//   match x {
//     Some(v) => { v + 1 }
//     default => { 0 }
//   }
//
// Arm bodies use:
// - `value_block` when the arm produces a value, and
// - `block` when the match result is unused (statement position).
//
// Type checking enforces:
// - `default` (if present) is last and appears at most once,
// - duplicates are rejected,
// - without `default` matches must be exhaustive for known variants, and
// - when the match result is used, all arms must provide `value_block`s whose
//   result types match exactly.
match_expr: MATCH expr "{" (match_arm (TERMINATOR | COMMA)*)+ "}"
match_arm: match_pat FATARROW match_arm_body
match_pat: DEFAULT              -> match_default
         | NAME "(" ")"         -> match_ctor_paren
         | NAME "(" match_named_binders ")"  -> match_ctor_named
         | NAME "(" match_binders ")"  -> match_ctor
         | NAME                  -> match_ctor0
match_binders: NAME (COMMA NAME)*
match_named_binders: match_named_binder (COMMA match_named_binder)*
match_named_binder: NAME EQUAL NAME
match_arm_body: value_block
              | block

// Note: expression-form `try` (`try_catch_expr`) requires catch arms that
// produce a value (see `value_block`). Statement-form `try` (`try_stmt`) uses
// regular blocks.

try_catch_expr.1: TRY expr (CATCH catch_expr_arm)+
catch_expr_arm: event_fqn "(" ident ")" value_block   -> catch_expr_event
              | ident value_block                     -> catch_expr_binder
              | value_block                          -> catch_expr_block
ternary: pipeline QMARK expr COLON expr

?pipeline: logic_or pipeline_tail*
pipeline_tail: PIPE_FWD logic_or

?logic_or: logic_and logic_or_tail*
logic_or_tail: OR logic_and

?logic_and: bit_or logic_and_tail*
logic_and_tail: AND bit_or

?equality: comparison equality_tail*
equality_tail: (EQEQ | NOTEQ) comparison

?bit_or: bit_xor bit_or_tail*
bit_or_tail: BAR bit_xor

?bit_xor: bit_and bit_xor_tail*
bit_xor_tail: CARET bit_and

?bit_and: equality bit_and_tail*
bit_and_tail: AMP equality

?comparison: shift comparison_tail*
comparison_tail: (LT | LTE | GT | GTE) shift

?shift: sum shift_tail*
shift_tail: (LSHIFT | SHR) sum

?sum: term sum_tail*
sum_tail: (PLUS | MINUS) term

?term: factor term_tail*
term_tail: (STAR | SLASH | PERCENT) factor
?factor: postfix
       | STAR factor -> deref
       | MOVE factor -> move_op
       | "+" factor -> pos
       | "-" factor -> neg
       | "not" factor -> not_op
       | BANG factor -> not_op
       | TILDE factor -> bit_not
       | AMP MUT? factor -> borrow

postfix: primary postfix_suffix*
postfix_suffix: call_suffix
              | attr_suffix
              | arrow_suffix
              | index_suffix
              | type_app_suffix

call_suffix: call_type_args? "(" [call_args] ")"
call_type_args: CALL_TYPE_LT type_expr (COMMA type_expr)* ">"
type_app_suffix: call_type_args
attr_suffix: DOT NAME
arrow_suffix: ARROW NAME
index_suffix: "[" (leading_dot | expr) "]"

event_fqn: module_path COLON NAME

primary: literal
       | qualified_member
       | ident -> var
       | leading_dot
       | "(" expr ")"
       | lambda_expr
       | array_literal

lambda_expr: BAR lambda_params? BAR lambda_returns? FATARROW lambda_body
lambda_returns: RETURNS type_expr
lambda_params: lambda_param (COMMA lambda_param)*
lambda_param: NAME (COLON type_expr)?

trait_expr: trait_or
trait_or: trait_and (OR trait_and)*
trait_and: trait_not (AND trait_not)*
trait_not: "not" trait_not
         | trait_atom
trait_atom: trait_subject IS trait_name
          | "(" trait_expr ")"
trait_subject: NAME
trait_name: base_type
lambda_body: expr
           | value_block
           | block

// Type-level qualified member reference: `TypeRef::member`.
//
// This is intentionally narrow in MVP: the left-hand side is `base_type`
// (a nominal type reference with optional type arguments), not an arbitrary
// type expression.
//
// Semantics are determined by the typed checker. In MVP, only variant
// constructors are valid members, and the qualified member must be called.
// Qualified type member reference used for variant constructors (MVP).
//
// This is intentionally restricted to avoid ambiguity with existing expression
// syntax (notably module-alias member access and comparisons).
//
// Generic type arguments are supported in a narrowly disambiguated form:
// - `TypeName<T>::Ctor(...)`
// - `TypeName::Ctor<type T>(...)`
//
// The lexer/post-lexer only treats `<...>` as type arguments when the matching
// `>` is followed by an unambiguous commit token (`::` for the pre-`::` form).
// Call-site type args use a `type` marker: `f<type T>(...)`.
qualified_pre_type_args: QUAL_TYPE_LT type_expr (COMMA type_expr)* QUAL_TYPE_GT

qualified_member: NAME qualified_pre_type_args? DCOLON NAME

leading_dot: DOT NAME leading_suffix*
leading_suffix: call_suffix
              | attr_suffix
              | arrow_suffix
              | index_suffix
              | type_app_suffix

array_literal: "[" [expr (COMMA expr)*] "]"

// Exception constructor call syntax (used only in `throw`).
//
// Design choice:
// - Exceptions are thrown using constructor syntax `throw E(...)` (parens required),
//   not brace initializers. This matches user expectations and keeps `{...}` for
//   struct initializers and map literals.
// - Positional args are allowed and map to declared exception fields by
//   declaration order. Keyword args use `=` and follow the same rule as normal calls:
//   positional arguments must precede keyword arguments.
exception_ctor: NAME "(" [call_args] ")"

call_args: call_arg (COMMA call_arg)*
call_arg: NAME EQUAL expr -> kwarg
        | expr

literal: FLOAT        -> float_lit
       | SIGNED_INT   -> int_lit
       | STRING       -> str_lit
       | FSTRING_PREFIX STRING -> fstr_lit
       | "true"      -> true_lit
       | "false"     -> false_lit

[==== File: staged/lang2/driftc/parser/__init__.py =====]
"""
lang2 parser copy (self-contained, no runtime dependency on lang/).
Parses Drift source and adapts to lang2.driftc.stage0 AST + FnSignatures for the
lang2 pipeline.
"""

from __future__ import annotations

from pathlib import Path
from dataclasses import replace
from typing import Callable, Dict, Tuple, Optional, List

from lark.exceptions import UnexpectedInput

from . import parser as _parser
from . import ast as parser_ast
from lang2.driftc.stage0 import ast as s0
from lang2.driftc.stage1 import AstToHIR
from lang2.driftc import stage1 as H
from lang2.driftc.checker import FnSignature
from lang2.driftc.core.diagnostics import Diagnostic
from lang2.driftc.core.span import Span
from lang2.driftc.core.types_core import TypeKind
from lang2.driftc.core.event_codes import event_code, PAYLOAD_MASK
from lang2.driftc.core.function_id import FunctionId
from lang2.driftc.core.types_core import (
	TypeTable,
	StructFieldSchema,
	VariantArmSchema,
	VariantFieldSchema,
)
from lang2.driftc.core.type_resolve_common import resolve_opaque_type
from lang2.driftc.core.generic_type_expr import GenericTypeExpr


def _qualify_fn_name(module_id: str, name: str) -> str:
	# MVP: symbols in the default `main` module remain unqualified so single-module
	# programs keep legacy names. Other modules are qualified as `module::name`.
	if module_id in (None, "main"):
		return name
	return f"{module_id}::{name}"


def _validate_module_id(mid: str, *, span: Span) -> list[Diagnostic]:
	"""
	Validate a module id per the language spec (format + reserved prefixes).

	This is shared by:
	- single-module builds (`parse_drift_files_to_hir`), and
	- workspace builds (`parse_drift_workspace_to_hir`), including inferred ids
	  from `-M/--module-path`.
	"""
	if not isinstance(mid, str) or not mid:
		return [
			Diagnostic(
				message="invalid module id (empty)",
				severity="error",
				span=span,
			)
		]
	raw_len = len(mid.encode("utf-8"))
	if raw_len > 254:
		return [
			Diagnostic(
				message=f"invalid module id '{mid}': length {raw_len} exceeds 254 UTF-8 bytes",
				severity="error",
				span=span,
			)
		]
	# Reserved module namespaces. Only the dotted namespace prefixes are reserved
	# (e.g. `std.foo`), not the bare segment itself (e.g. `lib` is allowed).
	forbidden_prefixes = ("lang", "abi", "std", "core", "lib")
	for pfx in forbidden_prefixes:
		if mid.startswith(pfx + "."):
			return [
				Diagnostic(
					message=f"invalid module id '{mid}': reserved prefix '{pfx}' is not allowed",
					severity="error",
					span=span,
				)
			]
	if mid.startswith(".") or mid.endswith(".") or ".." in mid:
		return [
			Diagnostic(
				message=f"invalid module id '{mid}': dots must separate non-empty segments",
				severity="error",
				span=span,
			)
		]
	if mid.startswith("_") or mid.endswith("_") or "__" in mid:
		return [
			Diagnostic(
				message=f"invalid module id '{mid}': underscores must not be leading/trailing or consecutive",
				severity="error",
				span=span,
			)
		]
	segments = mid.split(".")
	for seg in segments:
		if not seg:
			return [
				Diagnostic(
					message=f"invalid module id '{mid}': empty segment",
					severity="error",
					span=span,
				)
			]
		if seg.startswith("_") or seg.endswith("_") or "__" in seg:
			return [
				Diagnostic(
					message=f"invalid module id '{mid}': segment '{seg}' has invalid underscore placement",
					severity="error",
					span=span,
				)
			]
		# MVP: segments must start with a lowercase letter to avoid ambiguous module
		# names and to keep directory→module inference predictable.
		if not ("a" <= seg[0] <= "z"):
			return [
				Diagnostic(
					message=f"invalid module id '{mid}': segment '{seg}' must start with a lowercase letter",
					severity="error",
					span=span,
				)
			]
		for ch in seg:
			if not (("a" <= ch <= "z") or ("0" <= ch <= "9") or ch == "_"):
				return [
					Diagnostic(
						message=f"invalid module id '{mid}': segment '{seg}' contains invalid character '{ch}'",
						severity="error",
						span=span,
					)
				]
	return []


def _format_span_short(span: Span) -> str:
	"""
	Format a span as `file:line:column` for use in `Diagnostic.notes`.

	Notes are currently plain strings (no secondary-span support), so we keep the
	format stable and human-oriented.
	"""
	f = span.file or "<unknown>"
	l = span.line if span.line is not None else "?"
	c = span.column if span.column is not None else "?"
	return f"{f}:{l}:{c}"


def _prime_builtins(table: TypeTable) -> None:
	"""
	Ensure builtin TypeIds exist and are seeded in a stable order.

	This is required for package embedding in Milestone 4: until TypeId remapping
	exists, independently-produced artifacts must agree on builtin ids.
	"""
	table.ensure_unknown()
	table.ensure_int()
	table.ensure_uint()
	table.ensure_bool()
	table.ensure_float()
	table.ensure_string()
	table.ensure_void()
	table.ensure_error()
	table.ensure_diagnostic_value()
	# Seed commonly used derived types so TypeIds are stable across builds.
	#
	# MVP: DV accessors return Optional<Int/Bool/String>, so we ensure those
	# instantiations exist even if a particular module doesn't use them directly.
	table.new_optional(table.ensure_int())
	table.new_optional(table.ensure_bool())
	table.new_optional(table.ensure_string())


def _type_expr_to_str(typ: parser_ast.TypeExpr) -> str:
	"""Render a TypeExpr into a string (e.g., Array<Int>, Result<Int, Error>)."""
	if not typ.args:
		return typ.name
	args = ", ".join(_type_expr_to_str(a) for a in typ.args)
	return f"{typ.name}<{args}>"


def _type_expr_key(typ: parser_ast.TypeExpr) -> tuple[object | None, str, tuple]:
	qual = getattr(typ, "module_id", None) or getattr(typ, "module_alias", None)
	return (qual, typ.name, tuple(_type_expr_key(a) for a in getattr(typ, "args", []) or []))


def _type_expr_key_str(typ: parser_ast.TypeExpr) -> str:
	qual = getattr(typ, "module_id", None) or getattr(typ, "module_alias", None)
	base = f"{qual}.{typ.name}" if qual else typ.name
	if not (getattr(typ, "args", []) or []):
		return base
	args = ", ".join(_type_expr_key_str(a) for a in getattr(typ, "args", []) or [])
	return f"{base}<{args}>"


def _impl_target_key(typ: parser_ast.TypeExpr, type_params: list[str]) -> tuple[object | None, str, tuple] | tuple[str, int]:
	"""Normalize impl target keys by treating type params as indexed placeholders."""
	if typ.name in type_params and not getattr(typ, "args", []):
		return ("param", type_params.index(typ.name))
	qual = getattr(typ, "module_id", None) or getattr(typ, "module_alias", None)
	return (qual, typ.name, tuple(_impl_target_key(a, type_params) for a in getattr(typ, "args", []) or []))


def _generic_type_expr_from_parser(
	typ: parser_ast.TypeExpr,
	*,
	type_params: list[str],
) -> GenericTypeExpr:
	"""
	Convert a parser `TypeExpr` into a generic-aware core `GenericTypeExpr`.

	This is used for schema-bearing declarations (variants) where field types may
	refer to generic parameters (e.g. `Some(value: T)`).
	"""
	if typ.name in type_params and not typ.args:
		return GenericTypeExpr.param(type_params.index(typ.name))
	return GenericTypeExpr.named(
		typ.name,
		[_generic_type_expr_from_parser(a, type_params=type_params) for a in getattr(typ, "args", [])],
		module_id=getattr(typ, "module_id", None),
	)


def _convert_expr(expr: parser_ast.Expr) -> s0.Expr:
	"""Convert parser AST expressions into lang2.driftc.stage0 AST expressions."""
	if isinstance(expr, parser_ast.Literal):
		return s0.Literal(value=expr.value, loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.Name):
		return s0.Name(ident=expr.ident, loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.TraitIs):
		return s0.TraitIs(
			subject=expr.subject,
			trait=expr.trait,
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.TraitAnd):
		return s0.TraitAnd(
			left=_convert_expr(expr.left),
			right=_convert_expr(expr.right),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.TraitOr):
		return s0.TraitOr(
			left=_convert_expr(expr.left),
			right=_convert_expr(expr.right),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.TraitNot):
		return s0.TraitNot(
			expr=_convert_expr(expr.expr),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.Lambda):
		params = [
			s0.Param(
				name=p.name,
				type_expr=p.type_expr,
				non_escaping=getattr(p, "non_escaping", False),
				loc=Span.from_loc(getattr(p, "loc", None)),
			)
			for p in expr.params
		]
		body_expr = _convert_expr(expr.body_expr) if expr.body_expr is not None else None
		body_block = s0.Block(statements=_convert_block(expr.body_block)) if expr.body_block is not None else None
		return s0.Lambda(
			params=params,
			ret_type=getattr(expr, "ret_type", None),
			body_expr=body_expr,
			body_block=body_block,
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.Call):
		return s0.Call(
			func=_convert_expr(expr.func),
			args=[_convert_expr(a) for a in expr.args],
			kwargs=[
				s0.KwArg(
					name=kw.name,
					value=_convert_expr(kw.value),
					loc=Span.from_loc(getattr(kw, "loc", None)),
				)
				for kw in getattr(expr, "kwargs", [])
			],
			type_args=getattr(expr, "type_args", None),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.TypeApp):
		return s0.TypeApp(
			func=_convert_expr(expr.func),
			type_args=list(expr.type_args),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.Attr):
		# Member-through-reference access (`p->field`) is normalized at the
		# parser→stage0 boundary by inserting an explicit deref.
		#
		# This keeps stage0/stage1 ASTs simple: later phases only need normal
		# member access plus unary deref (`*p`).
		base = _convert_expr(expr.value)
		if getattr(expr, "op", ".") == "->":
			base = s0.Unary(op="*", operand=base, loc=Span.from_loc(getattr(expr.value, "loc", None)))
		return s0.Attr(value=base, attr=expr.attr, loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.QualifiedMember):
		return s0.QualifiedMember(
			base_type_expr=expr.base_type,
			member=expr.member,
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.Index):
		return s0.Index(
			value=_convert_expr(expr.value),
			index=_convert_expr(expr.index),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.Binary):
		return s0.Binary(
			op=expr.op,
			left=_convert_expr(expr.left),
			right=_convert_expr(expr.right),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.Unary):
		return s0.Unary(op=expr.op, operand=_convert_expr(expr.operand), loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.ArrayLiteral):
		return s0.ArrayLiteral(elements=[_convert_expr(e) for e in expr.elements], loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.Move):
		return s0.Move(value=_convert_expr(expr.value), loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.Placeholder):
		return s0.Placeholder(loc=Span.from_loc(getattr(expr, "loc", None)))
	if isinstance(expr, parser_ast.Ternary):
		return s0.Ternary(
			cond=_convert_expr(expr.condition),
			then_expr=_convert_expr(expr.then_value),
			else_expr=_convert_expr(expr.else_value),
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.TryCatchExpr):
		catch_arms = [
			s0.CatchExprArm(
				event=arm.event,
				binder=arm.binder,
				block=_convert_block(arm.block),
				loc=Span.from_loc(getattr(arm, "loc", None)),
			)
			for arm in expr.catch_arms
		]
		return s0.TryCatchExpr(
			attempt=_convert_expr(expr.attempt),
			catch_arms=catch_arms,
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.MatchExpr):
		arms = [
			s0.MatchArm(
				ctor=arm.ctor,
				pattern_arg_form=getattr(arm, "pattern_arg_form", "positional"),
				binders=list(arm.binders),
				binder_fields=list(arm.binder_fields) if getattr(arm, "binder_fields", None) is not None else None,
				block=_convert_block(arm.block),
				loc=Span.from_loc(getattr(arm, "loc", None)),
			)
			for arm in expr.arms
		]
		return s0.MatchExpr(
			scrutinee=_convert_expr(expr.scrutinee),
			arms=arms,
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.ExceptionCtor):
		return s0.ExceptionCtor(
			name=expr.name,
			args=[_convert_expr(a) for a in expr.args],
			kwargs=[
				s0.KwArg(
					name=kw.name,
					value=_convert_expr(kw.value),
					loc=Span.from_loc(getattr(kw, "loc", None)),
				)
				for kw in expr.kwargs
			],
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	if isinstance(expr, parser_ast.FString):
		return s0.FString(
			parts=list(expr.parts),
			holes=[
				s0.FStringHole(
					expr=_convert_expr(h.expr),
					spec=h.spec,
					loc=Span.from_loc(getattr(h, "loc", None)),
				)
				for h in expr.holes
			],
			loc=Span.from_loc(getattr(expr, "loc", None)),
		)
	raise NotImplementedError(f"Unsupported expression in adapter: {expr!r}")


def _convert_return(stmt: parser_ast.ReturnStmt) -> s0.Stmt:
	return s0.ReturnStmt(value=_convert_expr(stmt.value) if stmt.value is not None else None, loc=Span.from_loc(stmt.loc))


def _convert_expr_stmt(stmt: parser_ast.ExprStmt) -> s0.Stmt:
	return s0.ExprStmt(expr=_convert_expr(stmt.value), loc=Span.from_loc(stmt.loc))


def _convert_let(stmt: parser_ast.LetStmt) -> s0.Stmt:
	return s0.LetStmt(
		name=stmt.name,
		value=_convert_expr(stmt.value),
		type_expr=getattr(stmt, "type_expr", None),
		mutable=bool(getattr(stmt, "mutable", False)),
		loc=Span.from_loc(stmt.loc),
	)


def _convert_assign(stmt: parser_ast.AssignStmt) -> s0.Stmt:
	return s0.AssignStmt(target=_convert_expr(stmt.target), value=_convert_expr(stmt.value), loc=Span.from_loc(stmt.loc))


def _convert_aug_assign(stmt: "parser_ast.AugAssignStmt") -> s0.Stmt:
	"""
	Convert an augmented assignment statement.

	MVP supports:
	`+=`, `-=`, `*=`, `/=`, `%=`, `&=`, `|=`, `^=`, `<<=`, `>>=`.

	We preserve this as a distinct stage0 statement so later lowering can
	implement correct read-modify-write semantics for complex lvalues.
	"""
	return s0.AugAssignStmt(
		target=_convert_expr(stmt.target),
		op=str(getattr(stmt, "op", "+=")),
		value=_convert_expr(stmt.value),
		loc=Span.from_loc(stmt.loc),
	)

def _convert_if(stmt: parser_ast.IfStmt) -> s0.Stmt:
	return s0.IfStmt(
		cond=_convert_expr(stmt.condition),
		then_block=_convert_block(stmt.then_block),
		else_block=_convert_block(stmt.else_block) if stmt.else_block else [],
		loc=Span.from_loc(stmt.loc),
	)


def _convert_break(stmt: parser_ast.BreakStmt) -> s0.Stmt:
	return s0.BreakStmt(loc=Span.from_loc(stmt.loc))


def _convert_continue(stmt: parser_ast.ContinueStmt) -> s0.Stmt:
	return s0.ContinueStmt(loc=Span.from_loc(stmt.loc))


def _convert_while(stmt: parser_ast.WhileStmt) -> s0.Stmt:
	return s0.WhileStmt(cond=_convert_expr(stmt.condition), body=_convert_block(stmt.body), loc=Span.from_loc(stmt.loc))


def _convert_for(stmt: parser_ast.ForStmt) -> s0.Stmt:
	return s0.ForStmt(iter_var=stmt.var, iterable=_convert_expr(stmt.iter_expr), body=_convert_block(stmt.body), loc=Span.from_loc(stmt.loc))


def _convert_throw(stmt: parser_ast.ThrowStmt) -> s0.Stmt:
	return s0.ThrowStmt(value=_convert_expr(stmt.expr), loc=Span.from_loc(stmt.loc))


def _convert_raise(stmt: parser_ast.RaiseStmt) -> s0.Stmt:
	# TODO: when rethrow semantics are defined, map RaiseStmt appropriately.
	# For now, treat parser RaiseStmt as a plain throw of the expression.
	expr = getattr(stmt, "expr", None) or getattr(stmt, "value")
	return s0.ThrowStmt(value=_convert_expr(expr), loc=Span.from_loc(stmt.loc))


def _convert_rethrow(stmt: parser_ast.RethrowStmt) -> s0.Stmt:
	return s0.RethrowStmt(loc=Span.from_loc(stmt.loc))


def _convert_try(stmt: parser_ast.TryStmt) -> s0.Stmt:
	catches = [
		s0.CatchExprArm(
			event=c.event,
			binder=c.binder,
			block=_convert_block(c.block),
			loc=Span.from_loc(getattr(c, "loc", None)),
		)
		for c in stmt.catches
	]
	return s0.TryStmt(body=_convert_block(stmt.body), catches=catches, loc=Span.from_loc(stmt.loc))


def _convert_import(stmt: parser_ast.ImportStmt) -> s0.Stmt:
	path = ".".join(stmt.path)
	return s0.ImportStmt(path=path, loc=Span.from_loc(stmt.loc))


_STMT_DISPATCH: dict[type[parser_ast.Stmt], Callable[[parser_ast.Stmt], s0.Stmt]] = {
	parser_ast.ReturnStmt: _convert_return,
	parser_ast.ExprStmt: _convert_expr_stmt,
	parser_ast.LetStmt: _convert_let,
	parser_ast.AssignStmt: _convert_assign,
	parser_ast.AugAssignStmt: _convert_aug_assign,
	parser_ast.IfStmt: _convert_if,
	parser_ast.BreakStmt: _convert_break,
	parser_ast.ContinueStmt: _convert_continue,
	parser_ast.WhileStmt: _convert_while,
	parser_ast.ForStmt: _convert_for,
	parser_ast.ThrowStmt: _convert_throw,
	parser_ast.RaiseStmt: _convert_raise,
	parser_ast.RethrowStmt: _convert_rethrow,
	parser_ast.TryStmt: _convert_try,
	parser_ast.ImportStmt: _convert_import,
}


def _convert_stmt(stmt: parser_ast.Stmt) -> s0.Stmt:
	"""Convert parser AST statements into lang2.driftc.stage0 AST statements."""
	fn = _STMT_DISPATCH.get(type(stmt))
	if fn is None:
		raise NotImplementedError(f"Unsupported statement in adapter: {stmt!r}")
	return fn(stmt)


def _convert_block(block: parser_ast.Block) -> list[s0.Stmt]:
	return [_convert_stmt(s) for s in block.statements]


class _FrontendParam:
	def __init__(
		self,
		name: str,
		type_expr: parser_ast.TypeExpr | None,
		loc: Optional[parser_ast.Located],
		*,
		non_escaping: bool = False,
	) -> None:
		self.name = name
		# Preserve the parsed type expression so the resolver can build real TypeIds.
		self.type = type_expr
		self.loc = loc
		self.non_escaping = non_escaping


class _FrontendDecl:
	def __init__(
		self,
		fn_id: FunctionId,
		name: str,
		method_name: Optional[str],
		type_params: list[str],
		type_param_locs: list[parser_ast.Located],
		params: list[_FrontendParam],
		return_type: parser_ast.TypeExpr,
		loc: Optional[parser_ast.Located],
		is_method: bool = False,
		self_mode: Optional[str] = None,
		impl_target: Optional[parser_ast.TypeExpr] = None,
		impl_type_params: list[str] | None = None,
		impl_type_param_locs: list[parser_ast.Located] | None = None,
		impl_owner: FunctionId | None = None,
		module: Optional[str] = None,
	) -> None:
		self.fn_id = fn_id
		self.name = name
		self.method_name = method_name
		self.type_params = type_params
		self.type_param_locs = type_param_locs
		self.params = params
		self.return_type = return_type
		self.throws = ()
		self.loc = loc
		self.is_extern = False
		self.is_intrinsic = False
		self.is_method = is_method
		self.self_mode = self_mode
		self.impl_target = impl_target
		self.impl_type_params = list(impl_type_params or [])
		self.impl_type_param_locs = list(impl_type_param_locs or [])
		self.impl_owner = impl_owner
		self.module = module


def _decl_from_parser_fn(
	fn: parser_ast.FunctionDef,
	*,
	fn_id: FunctionId,
	impl_type_params: list[str] | None = None,
	impl_type_param_locs: list[parser_ast.Located] | None = None,
	impl_owner: FunctionId | None = None,
) -> _FrontendDecl:
	params = [
		_FrontendParam(
			p.name,
			p.type_expr,
			getattr(p, "loc", None),
			non_escaping=getattr(p, "non_escaping", False),
		)
		for p in fn.params
	]
	return _FrontendDecl(
		fn_id,
		fn.name,
		fn.orig_name,
		fn.type_params,
		list(getattr(fn, "type_param_locs", []) or []),
		params,
		fn.return_type,
		getattr(fn, "loc", None),
		fn.is_method,
		fn.self_mode,
		fn.impl_target,
		impl_type_params,
		impl_type_param_locs,
		impl_owner,
	)


def _diagnostic(message: str, loc: object | None) -> Diagnostic:
	"""Helper to create a Diagnostic from a parser location."""
	return Diagnostic(message=message, severity="error", span=Span.from_loc(loc))


def _typeexpr_uses_internal_fnresult(typ: parser_ast.TypeExpr) -> bool:
	"""
	Return True if a surface type annotation mentions `FnResult` anywhere.

	`FnResult<T, Error>` is an internal ABI carrier used by lang2 for can-throw
	functions. It is not a surface type in the Drift language: user code should
	write `returns T` and use exceptions/try/catch for control flow.
	"""
	if typ.name == "FnResult":
		return True
	for arg in getattr(typ, "args", []) or []:
		if _typeexpr_uses_internal_fnresult(arg):
			return True
	return False


def _typeexpr_is_callable(typ: parser_ast.TypeExpr | None) -> bool:
	if typ is None:
		return False
	if typ.name in {"&", "&mut"} and getattr(typ, "args", None):
		return _typeexpr_is_callable(typ.args[0])
	return typ.name in {"Fn", "Callable"}


def _report_internal_fnresult_in_surface_type(
	*,
	kind: str,
	symbol: str,
	loc: object | None,
	diagnostics: list[Diagnostic],
) -> None:
	diagnostics.append(
		_diagnostic(
			f"{kind} '{symbol}' uses internal-only type 'FnResult' in a surface annotation; "
			"write `returns T` and use exceptions/try-catch instead",
			loc,
		)
	)


def _build_exception_catalog(exceptions: list[parser_ast.ExceptionDef], module_name: str | None, diagnostics: list[Diagnostic]) -> dict[str, int]:
	"""
	Assign deterministic event codes to exception declarations using the shared ABI hash.

	Collisions on the payload bits are reported as errors and the colliding
	exceptions are omitted from the catalog to avoid undefined dispatch.
	"""
	catalog: dict[str, int] = {}
	payload_seen: dict[int, str] = {}
	seen_names: set[str] = set()
	for exc in exceptions:
		if exc.name in seen_names:
			diagnostics.append(_diagnostic(f"duplicate exception '{exc.name}'", getattr(exc, "loc", None)))
			continue
		seen_names.add(exc.name)
		fqn = f"{module_name}:{exc.name}" if module_name else exc.name
		code = event_code(fqn)
		payload = code & PAYLOAD_MASK
		if payload in payload_seen and payload_seen[payload] != fqn:
			other = payload_seen[payload]
			diagnostics.append(
				_diagnostic(
					f"exception code collision between '{other}' and '{fqn}' (payload {payload})",
					getattr(exc, "loc", None),
				)
			)
			continue
		payload_seen[payload] = fqn
		catalog[fqn] = code
	return catalog


def _span_in_file(path: Path, loc: object | None) -> Span:
	"""
	Construct a Span that is anchored to a specific source file.

	The parser AST location objects do not carry a filename; for multi-file module
	builds we need the file to be explicit so diagnostics can point at the right
	origin.
	"""
	if loc is None:
		return Span(file=str(path))
	span = Span.from_loc(loc)
	if span.file is None:
		return Span(
			file=str(path),
			line=span.line,
			column=span.column,
			end_line=span.end_line,
			end_column=span.end_column,
			raw=span.raw,
		)
	return span


def _diag_duplicate(
	*,
	kind: str,
	name: str,
	first_path: Path,
	first_loc: object | None,
	second_path: Path,
	second_loc: object | None,
) -> list[Diagnostic]:
	"""
	Build a primary error + secondary note diagnostic for a cross-file duplicate.

	The error is pinned to the second definition; the note is pinned to the first.
	"""
	first_span = _span_in_file(first_path, first_loc)
	second_span = _span_in_file(second_path, second_loc)
	return [
		Diagnostic(
			message=f"duplicate {kind} definition for '{name}'",
			severity="error",
			span=second_span,
		),
		Diagnostic(
			message=f"previous definition of '{name}' is here",
			severity="note",
			span=first_span,
		),
	]


def parse_drift_files_to_hir(
	paths: list[Path],
) -> Tuple[Dict[FunctionId, H.HBlock], Dict[FunctionId, FnSignature], Dict[str, List[FunctionId]], "TypeTable", Dict[str, int], List[Diagnostic]]:
	"""
	Parse and lower a set of Drift source files into a single module unit.

	MVP (Milestone 1): accepts multiple files that all declare the same `module`
	id (or default to `main`). The module is lowered as if it were one merged file:
	- top-level declarations are combined,
	- cross-file collisions are diagnosed (with a pinned note pointing at the
	  first definition),
	- then the existing parser→stage0→HIR pipeline runs on the merged program.

	This does not implement cross-module imports yet; it only handles multiple
	files *within* the same module.
	"""
	diagnostics: list[Diagnostic] = []
	if not paths:
		return {}, {}, {}, TypeTable(), {}, [Diagnostic(message="no input files", severity="error")]

	programs: list[tuple[Path, parser_ast.Program]] = []
	for path in paths:
		source = path.read_text()
		try:
			prog = _parser.parse_program(source)
		except _parser.ModuleDeclError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc)))
			continue
		except _parser.QualifiedMemberParseError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc)))
			continue
		except _parser.FStringParseError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc)))
			continue
		except UnexpectedInput as err:
			span = Span(
				file=str(path),
				line=getattr(err, "line", None),
				column=getattr(err, "column", None),
				raw=err,
			)
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=span))
			continue
		programs.append((path, prog))

	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, diagnostics

	# Enforce single-module membership across the file set.
	def _effective_module_id(p: parser_ast.Program) -> str:
		return getattr(p, "module", None) or "main"

	module_id = _effective_module_id(programs[0][1])
	for path, prog in programs:
		mid = _effective_module_id(prog)
		decl_span = _span_in_file(path, getattr(prog, "module_loc", None))
		diagnostics.extend(_validate_module_id(mid, span=decl_span))
	for path, prog in programs[1:]:
		mid = _effective_module_id(prog)
		if mid != module_id:
			diagnostics.append(
				Diagnostic(
					message=f"module id mismatch: expected '{module_id}', found '{mid}'",
					severity="error",
					span=Span(file=str(path), line=1, column=1),
				)
			)
	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, diagnostics

	merged, _origins = _merge_module_files(module_id, programs, diagnostics)

	# Lower the merged program using the existing single-file pipeline.
	return _lower_parsed_program_to_hir(merged, diagnostics=diagnostics)


def _merge_module_files(
	module_id: str,
	files: list[tuple[Path, parser_ast.Program]],
	diagnostics: list[Diagnostic],
) -> tuple[parser_ast.Program, dict[FunctionId, Path]]:
	"""
	Merge a module's file set into a single parser AST `Program` (Milestone 1 rule set).

	This is the single source of truth for “multi-file module” merge behavior.
	Both `parse_drift_files_to_hir` (single-module build) and the workspace loader
	(Milestone 2) must call this helper to avoid drift.
	"""
	merged = parser_ast.Program(module=module_id)
	# Provenance map for module-local callable symbols (free functions and methods).
	#
	# Used by the workspace loader to implement per-file import environments:
	# we need to know which source file a given function body came from so we can
	# apply that file's imports while rewriting call sites.
	origin_by_fn_id: dict[FunctionId, Path] = {}

	first_fn_sig: dict[tuple, tuple[Path, object | None]] = {}
	name_ord: dict[str, int] = {}
	free_names: set[str] = set()
	for path, prog in files:
		for fn in getattr(prog, "functions", []) or []:
			sig_key = (
				fn.name,
				len(getattr(fn, "params", []) or []),
				tuple(_type_expr_key(p.type_expr) for p in getattr(fn, "params", []) or []),
			)
			if sig_key in first_fn_sig:
				first_path, first_loc = first_fn_sig[sig_key]
				diagnostics.extend(
					_diag_duplicate(
						kind="function",
						name=fn.name,
						first_path=first_path,
						first_loc=first_loc,
						second_path=path,
						second_loc=getattr(fn, "loc", None),
					)
				)
				continue
			first_fn_sig[sig_key] = (path, getattr(fn, "loc", None))
			free_names.add(fn.name)
			merged.functions.append(fn)
			ordinal = name_ord.get(fn.name, 0)
			name_ord[fn.name] = ordinal + 1
			fn_id = FunctionId(module=module_id, name=fn.name, ordinal=ordinal)
			origin_by_fn_id.setdefault(fn_id, path)

	first_const: dict[str, tuple[Path, object | None]] = {}
	for path, prog in files:
		for c in getattr(prog, "consts", []) or []:
			if c.name in first_const:
				first_path, first_loc = first_const[c.name]
				diagnostics.extend(
					_diag_duplicate(
						kind="const",
						name=c.name,
						first_path=first_path,
						first_loc=first_loc,
						second_path=path,
						second_loc=getattr(c, "loc", None),
					)
				)
				continue
			first_const[c.name] = (path, getattr(c, "loc", None))
			merged.consts.append(c)

	first_struct: dict[str, tuple[Path, object | None]] = {}
	for path, prog in files:
		for s in getattr(prog, "structs", []) or []:
			if s.name in first_struct:
				first_path, first_loc = first_struct[s.name]
				diagnostics.extend(
					_diag_duplicate(
						kind="struct",
						name=s.name,
						first_path=first_path,
						first_loc=first_loc,
						second_path=path,
						second_loc=getattr(s, "loc", None),
					)
				)
				continue
			first_struct[s.name] = (path, getattr(s, "loc", None))
			merged.structs.append(s)

	first_exc: dict[str, tuple[Path, object | None]] = {}
	for path, prog in files:
		for exc in getattr(prog, "exceptions", []) or []:
			if exc.name in first_exc:
				first_path, first_loc = first_exc[exc.name]
				diagnostics.extend(
					_diag_duplicate(
						kind="exception",
						name=exc.name,
						first_path=first_path,
						first_loc=first_loc,
						second_path=path,
						second_loc=getattr(exc, "loc", None),
					)
				)
				continue
			first_exc[exc.name] = (path, getattr(exc, "loc", None))
			merged.exceptions.append(exc)

	first_variant: dict[str, tuple[Path, object | None]] = {}
	for path, prog in files:
		for v in getattr(prog, "variants", []) or []:
			if v.name in first_variant:
				first_path, first_loc = first_variant[v.name]
				diagnostics.extend(
					_diag_duplicate(
						kind="variant",
						name=v.name,
						first_path=first_path,
						first_loc=first_loc,
						second_path=path,
						second_loc=getattr(v, "loc", None),
					)
				)
				continue
			first_variant[v.name] = (path, getattr(v, "loc", None))
			merged.variants.append(v)

	first_trait: dict[str, tuple[Path, object | None]] = {}
	for path, prog in files:
		for tr in getattr(prog, "traits", []) or []:
			if tr.name in first_trait:
				first_path, first_loc = first_trait[tr.name]
				diagnostics.extend(
					_diag_duplicate(
						kind="trait",
						name=tr.name,
						first_path=first_path,
						first_loc=first_loc,
						second_path=path,
						second_loc=getattr(tr, "loc", None),
					)
				)
				continue
			first_trait[tr.name] = (path, getattr(tr, "loc", None))
			merged.traits.append(tr)

	# Combine module directives (imports/exports).
	for _, prog in files:
		merged.imports.extend(getattr(prog, "imports", []) or [])
		merged.exports.extend(getattr(prog, "exports", []) or [])

	# Merge implement blocks by target repr and de-duplicate methods.
	impls_by_key: dict[tuple[tuple | None, tuple], parser_ast.ImplementDef] = {}
	first_method: dict[tuple[tuple | None, tuple, str], tuple[Path, object | None]] = {}
	for path, prog in files:
		for impl in getattr(prog, "implements", []) or []:
			impl_type_params = list(getattr(impl, "type_params", []) or [])
			target_key = _impl_target_key(impl.target, impl_type_params)
			target_str = _type_expr_key_str(impl.target)
			trait_key = _type_expr_key(impl.trait) if getattr(impl, "trait", None) is not None else None
			trait_str = _type_expr_key_str(impl.trait) if getattr(impl, "trait", None) is not None else None
			key = (trait_key, target_key)
			dst = impls_by_key.get(key)
			if dst is None:
				dst = parser_ast.ImplementDef(
					target=impl.target,
					trait=getattr(impl, "trait", None),
					require=getattr(impl, "require", None),
					loc=getattr(impl, "loc", None),
					is_pub=getattr(impl, "is_pub", False),
					type_params=list(getattr(impl, "type_params", []) or []),
					type_param_locs=list(getattr(impl, "type_param_locs", []) or []),
					methods=[],
				)
				impls_by_key[key] = dst
			elif list(getattr(dst, "type_params", []) or []) != impl_type_params:
				impl_label = f"{trait_str} for {target_str}" if trait_str else target_str
				diagnostics.append(
					Diagnostic(
						message=f"conflicting type parameter lists for implement block '{impl_label}'",
						severity="error",
						span=_span_in_file(path, getattr(impl, "loc", None)),
					)
				)
			elif getattr(dst, "require", None) != getattr(impl, "require", None):
				impl_label = f"{trait_str} for {target_str}" if trait_str else target_str
				diagnostics.append(
					Diagnostic(
						message=f"conflicting require clauses for implement block '{impl_label}'",
						severity="error",
						span=_span_in_file(path, getattr(impl, "loc", None)),
					)
				)
			for m in getattr(impl, "methods", []) or []:
				if m.name in free_names:
					first_path, _first_loc = first_fn[m.name]
					diagnostics.append(
						Diagnostic(
							message=f"method '{m.name}' conflicts with existing free function of the same name",
							severity="error",
							span=_span_in_file(path, getattr(m, "loc", None)),
							notes=[f"previous free function definition is in {first_path}"],
						)
					)
					continue
				method_key = (trait_key, target_key, m.name)
				if method_key in first_method:
					first_path, first_loc = first_method[method_key]
					impl_label = f"{trait_str} for {target_str}" if trait_str else target_str
					diagnostics.extend(
						_diag_duplicate(
							kind=f"method for type '{impl_label}'",
							name=m.name,
							first_path=first_path,
							first_loc=first_loc,
							second_path=path,
							second_loc=getattr(m, "loc", None),
						)
					)
					continue
				first_method[method_key] = (path, getattr(m, "loc", None))
				dst.methods.append(m)
				if trait_str:
					symbol_name = f"{target_str}::{trait_str}::{m.name}"
				else:
					symbol_name = f"{target_str}::{m.name}"
				ordinal = name_ord.get(symbol_name, 0)
				name_ord[symbol_name] = ordinal + 1
				fn_id = FunctionId(module=module_id, name=symbol_name, ordinal=ordinal)
				origin_by_fn_id.setdefault(fn_id, path)
	merged.implements = list(impls_by_key.values())
	return merged, origin_by_fn_id


def parse_drift_workspace_to_hir(
	paths: list[Path],
	*,
	module_paths: list[Path] | None = None,
	external_module_exports: dict[str, dict[str, object]] | None = None,
) -> Tuple[
	Dict[FunctionId, H.HBlock],
	Dict[FunctionId, FnSignature],
	Dict[str, List[FunctionId]],
	"TypeTable",
	Dict[str, int],
	Dict[str, Dict[str, object]],
	Dict[str, set[str]],
	List[Diagnostic],
]:
	"""
	Parse and lower a set of Drift source files that may belong to multiple modules.

	This is Milestone 2 (“module imports and cross-module resolution”) scaffolding:
	- input is an unordered set of files (typically all `*.drift` files in a build),
	- files are grouped by their declared `module <id>` (or default to `main`),
	- each module is merged from its file set (Milestone 1 behavior),
	- imports are resolved across modules (MVP: `from <module> import <symbol>`),
	- resulting HIR/signatures are returned as a single program unit suitable for
	  the existing HIR→MIR→SSA→LLVM pipeline.

		Important MVP constraints (pinned for clarity):
		- Imports are treated as **per-file** bindings:
		  - Duplicate identical imports in one file are idempotent (“no-op after first”).
		  - Conflicting aliases/bindings in one file are diagnosed as errors.
		  - Different files may import the same module/symbol freely; the module is still
		    parsed/merged/compiled once per build and referenced from all import sites.
		- Module-qualified access (`import m` then `m.foo()`) is supported for calling
		  exported free functions and for struct constructor calls (`m.Point(...)`).
		- Cross-module import validation supports both value and type namespaces
		  (types: structs, variants, exceptions).

	Returns:
	  (func_hirs, signatures, fn_ids_by_name, type_table, exception_catalog, module_exports, module_deps, diagnostics)
	"""
	diagnostics: list[Diagnostic] = []
	if not paths:
		return {}, {}, {}, TypeTable(), {}, {}, {}, [Diagnostic(message="no input files", severity="error")]

	def _effective_module_id(p: parser_ast.Program) -> str:
		return getattr(p, "module", None) or "main"

	# Parse all files first.
	parsed: list[tuple[Path, parser_ast.Program]] = []
	for path in paths:
		source = path.read_text()
		try:
			prog = _parser.parse_program(source)
		except _parser.ModuleDeclError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc)))
			continue
		except _parser.QualifiedMemberParseError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc)))
			continue
		except _parser.FStringParseError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc)))
			continue
		except UnexpectedInput as err:
			span = Span(
				file=str(path),
				line=getattr(err, "line", None),
				column=getattr(err, "column", None),
				raw=err,
			)
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=span))
			continue
		parsed.append((path, prog))

	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	def _infer_module_id_from_paths(path: Path) -> tuple[str, Path] | tuple[None, None]:
		"""
		Infer the module id for a file from the configured module roots.

		Rule (MVP, pinned in work-progress):
		- find the first module root that is a prefix of the file's absolute path,
		- module id is derived from the directory relative to the root:
		  - empty relative path => "main"
		  - otherwise path segments joined by '.' (platform-independent).
		"""
		if not module_paths:
			return None, None
		abs_path = path.resolve()
		candidates: list[tuple[Path, Path]] = []
		for root in module_paths:
			abs_root = root.resolve()
			try:
				rel_dir = abs_path.parent.relative_to(abs_root)
			except ValueError:
				continue
			candidates.append((abs_root, rel_dir))

		if not candidates:
			return None, None

		# Deterministic root selection: pick the most-specific root (longest prefix).
		# This prevents module identity from depending on CLI flag ordering when roots
		# overlap (e.g. `-M src` and `-M src/vendor`).
		candidates.sort(key=lambda r: len(r[0].parts), reverse=True)
		best_len = len(candidates[0][0].parts)
		best = [c for c in candidates if len(c[0].parts) == best_len]
		if len(best) != 1:
			# Ambiguous configuration: multiple roots at the same specificity match
			# the same file.
			return None, None

		abs_root, rel_dir = best[0]
		parts = list(rel_dir.parts)
		if not parts or parts == ["."]:
			return "main", abs_root
		for seg in parts:
			if seg in {".", ".."}:
				return None, None
			if not seg:
				return None, None
		return ".".join(parts), abs_root

	# Group by module id (declared or inferred).
	by_module: dict[str, list[tuple[Path, parser_ast.Program]]] = {}
	roots_by_module: dict[str, set[Path]] = {}
	# For pinned diagnostics, keep at least one representative file per (module, root).
	root_file_by_module: dict[str, dict[Path, Path]] = {}
	for path, prog in parsed:
		if module_paths:
			inferred, root = _infer_module_id_from_paths(path)
			if inferred is None or root is None:
				diagnostics.append(
					Diagnostic(
						message=f"file '{path}' is not under exactly one configured module root",
						severity="error",
						span=Span(file=str(path), line=1, column=1),
					)
				)
				continue
			# Validate inferred id before using it as a module-graph key.
			diagnostics.extend(_validate_module_id(inferred, span=Span(file=str(path), line=1, column=1)))
			declared = getattr(prog, "module", None)
			if declared is not None:
				decl_span = _span_in_file(path, getattr(prog, "module_loc", None))
				diagnostics.extend(_validate_module_id(declared, span=decl_span))
				if any(d.severity == "error" for d in diagnostics):
					continue
				if declared != inferred:
					notes = [f"inferred module id is '{inferred}' from root '{root}'"]
					diagnostics.append(
						Diagnostic(
							message=f"module id mismatch: expected '{inferred}', found '{declared}'",
							severity="error",
							span=decl_span,
							notes=notes,
						)
					)
					continue
			# Treat missing module header as implicit declaration of the inferred id.
			if declared is None:
				prog = replace(prog, module=inferred)
			by_module.setdefault(inferred, []).append((path, prog))
			roots_by_module.setdefault(inferred, set()).add(root)
			root_file_by_module.setdefault(inferred, {}).setdefault(root, path)
		else:
			mid = _effective_module_id(prog)
			decl_span = _span_in_file(path, getattr(prog, "module_loc", None))
			diagnostics.extend(_validate_module_id(mid, span=decl_span))
			by_module.setdefault(mid, []).append((path, prog))

	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	# When module roots are used, reject ambiguous module ids coming from
	# multiple roots (prevents accidental shadowing/selection by search order).
	if module_paths:
		for mid, roots in roots_by_module.items():
			if len(roots) > 1:
				root_list = ", ".join(str(r) for r in sorted(roots))
				span_file = None
				# Anchor the diagnostic to a concrete file under one of the roots.
				for r in sorted(roots):
					span_file = root_file_by_module.get(mid, {}).get(r)
					if span_file is not None:
						break
				span = Span(file=str(span_file), line=1, column=1) if span_file else Span()
				diagnostics.append(
					Diagnostic(
						message=f"multiple module roots provide module '{mid}' ({root_list})",
						severity="error",
						span=span,
					)
				)
	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics
	# Merge each module (Milestone 1 rules) and retain callable provenance per file.
	merged_programs: dict[str, parser_ast.Program] = {}
	origin_by_module: dict[str, dict[FunctionId, Path]] = {}
	for mid, files in by_module.items():
		merged, origins = _merge_module_files(mid, files, diagnostics)
		merged_programs[mid] = merged
		origin_by_module[mid] = origins

	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	def _build_export_interface(
		*,
		module_id: str,
		merged_prog: parser_ast.Program,
		module_files: list[tuple[Path, parser_ast.Program]],
	) -> tuple[
		dict[str, tuple[str, str]],
		dict[str, set[str]],
		set[str],
		dict[str, Span],
		dict[str, tuple[str, str]],
		dict[str, tuple[tuple[str, str], tuple[str, str], Span, Span]],
	]:
		"""
		Build the exported interface for a module.

		MVP visibility model:
		- items are private by default,
		- `export { Name, ... }` lists the exported names,
		- `export { other.module.* }` re-exports the other module's export set,
		- `export` cannot elevate visibility; only `pub` items may be exported.
		- both values and types may be exported, but in separate namespaces:
		  - values: free functions
		  - types: structs, variants, exceptions

		Because `export { ... }` syntax is unqualified, exporting a name that exists
		in both namespaces is ambiguous. Until we add explicit qualifiers, that is
		a compile-time error.

		Because `exports.types` is kind-separated (structs/variants/exceptions), an
		exported name that resolves to multiple type kinds is also a compile-time
		error.

		Spans are anchored to the source file that contained the `export { ... }`
		statement so diagnostics remain useful in multi-file modules.
		"""
		module_fn_names: set[str] = {fn.name for fn in getattr(merged_prog, "functions", []) or []}
		module_const_names: set[str] = {c.name for c in getattr(merged_prog, "consts", []) or []}
		module_struct_names: set[str] = {s.name for s in getattr(merged_prog, "structs", []) or []}
		module_variant_names: set[str] = {v.name for v in getattr(merged_prog, "variants", []) or []}
		module_exception_names: set[str] = {e.name for e in getattr(merged_prog, "exceptions", []) or []}
		module_pub_fn_names: set[str] = {fn.name for fn in getattr(merged_prog, "functions", []) or [] if getattr(fn, "is_pub", False)}
		module_pub_const_names: set[str] = {c.name for c in getattr(merged_prog, "consts", []) or [] if getattr(c, "is_pub", False)}
		module_pub_struct_names: set[str] = {s.name for s in getattr(merged_prog, "structs", []) or [] if getattr(s, "is_pub", False)}
		module_pub_variant_names: set[str] = {v.name for v in getattr(merged_prog, "variants", []) or [] if getattr(v, "is_pub", False)}
		module_pub_exception_names: set[str] = {e.name for e in getattr(merged_prog, "exceptions", []) or [] if getattr(e, "is_pub", False)}

		raw_export_entries: list[tuple[str, Span]] = []
		star_export_entries: list[tuple[str, Span]] = []
		for path, parsed_prog in module_files:
			for ex in getattr(parsed_prog, "exports", []) or []:
				for item in getattr(ex, "items", []) or []:
					item_span = _span_in_file(path, getattr(item, "loc", None))
					if isinstance(item, parser_ast.ExportName):
						raw_export_entries.append((item.name, item_span))
					elif isinstance(item, parser_ast.ExportModuleStar):
						mod = ".".join(getattr(item, "module_path", []) or [])
						if mod:
							star_export_entries.append((mod, item_span))

		# MVP rule: exporting the same name multiple times within a module is a
		# deterministic user error (even if it would be a no-op). We treat it as a
		# duplicate declaration so the module interface remains crisp and tooling
		# never has to guess which export site is authoritative.
		seen_export_names: dict[str, Span] = {}
		seen_star_modules: dict[str, Span] = {}

		# Exported values map exported local name -> underlying (module_id, symbol).
		#
		# Export entries always name symbols in the *current* module interface
		# (e.g., `a::foo`). Re-exports preserve the origin module in the map so
		# consumers always bind to the defining symbol.
		exported_values: dict[str, tuple[str, str]] = {}
		exported_types: dict[str, set[str]] = {"structs": set(), "variants": set(), "exceptions": set()}
		exported_consts: set[str] = set()
		star_reexports: dict[str, Span] = {}
		for mod, ex_span in star_export_entries:
			prev = seen_star_modules.get(mod)
			if prev is None:
				seen_star_modules[mod] = ex_span
				star_reexports[mod] = ex_span
			else:
				diagnostics.append(
					Diagnostic(
						message=f"duplicate export of module '{mod}.*' in module '{module_id}'",
						severity="error",
						span=ex_span,
						notes=[f"first export was here: {_format_span_short(prev)}"],
					)
				)

		for n, ex_span in raw_export_entries:
			first_span = seen_export_names.get(n)
			if first_span is None:
				seen_export_names[n] = ex_span
			else:
				diagnostics.append(
					Diagnostic(
						message=f"duplicate export of symbol '{n}' in module '{module_id}'",
						severity="error",
						span=ex_span,
						notes=[f"first export was here: {_format_span_short(first_span)}"],
					)
				)
				continue

			in_values = n in module_fn_names
			in_consts = n in module_const_names
			in_struct = n in module_struct_names
			in_variant = n in module_variant_names
			in_exc = n in module_exception_names
			type_hits = int(in_struct) + int(in_variant) + int(in_exc)
			in_types = type_hits > 0

			if (in_values and in_consts) or (in_values and in_types) or (in_consts and in_types):
				diagnostics.append(
					Diagnostic(
						message=f"exported name '{n}' is ambiguous (defined as multiple kinds in module '{module_id}')",
						severity="error",
						span=ex_span,
					)
				)
				continue
			if type_hits > 1:
				diagnostics.append(
					Diagnostic(
						message=f"exported type name '{n}' is ambiguous (defined as multiple type kinds in module '{module_id}')",
						severity="error",
						span=ex_span,
					)
				)
				continue

			if not in_values and not in_consts and not in_types:
				diagnostics.append(
					Diagnostic(
						message=f"module '{module_id}' exports unknown symbol '{n}'",
						severity="error",
						span=ex_span,
					)
				)
				continue

			if in_values:
				if n not in module_pub_fn_names:
					diagnostics.append(
						Diagnostic(
							message=f"cannot export '{n}': symbol is not public (mark it 'pub')",
							severity="error",
							span=ex_span,
						)
					)
					continue
				exported_values[n] = (module_id, n)
			if in_consts:
				if n not in module_pub_const_names:
					diagnostics.append(
						Diagnostic(
							message=f"cannot export '{n}': symbol is not public (mark it 'pub')",
							severity="error",
							span=ex_span,
						)
					)
					continue
				exported_consts.add(n)
			if in_struct:
				if n not in module_pub_struct_names:
					diagnostics.append(
						Diagnostic(
							message=f"cannot export '{n}': symbol is not public (mark it 'pub')",
							severity="error",
							span=ex_span,
						)
					)
					continue
				exported_types["structs"].add(n)
			if in_variant:
				if n not in module_pub_variant_names:
					diagnostics.append(
						Diagnostic(
							message=f"cannot export '{n}': symbol is not public (mark it 'pub')",
							severity="error",
							span=ex_span,
						)
					)
					continue
				exported_types["variants"].add(n)
			if in_exc:
				if n not in module_pub_exception_names:
					diagnostics.append(
						Diagnostic(
							message=f"cannot export '{n}': symbol is not public (mark it 'pub')",
							severity="error",
							span=ex_span,
						)
					)
					continue
				exported_types["exceptions"].add(n)

		return (
			exported_values,
			exported_types,
			exported_consts,
			star_reexports,
		)

	# Note: module-scoped nominal type identity is implemented in lang2.
	# Multiple modules may define types with the same short name without
	# colliding; identity is `(module_id, name, kind)`.

	# Export sets (private by default, explicit exports required).
	#
	# MVP supports exporting/importing both value-level and type-level symbols,
	# but keeps them in separate namespaces:
	# - values: currently just free functions
	# - types: structs, variants, exceptions
	#
	# Export lists are unqualified identifiers, so to avoid ambiguity we reject
	# any module that defines the same name in both namespaces (until the language
	# adds explicit `export type ...` / `export fn ...` syntax).
	exports_values_by_module: dict[str, dict[str, tuple[str, str]]] = {}
	exports_types_by_module: dict[str, dict[str, set[str]]] = {}
	exports_consts_by_module: dict[str, set[str]] = {}
	star_reexports_by_module: dict[str, dict[str, Span]] = {}
	exported_const_origins_by_module: dict[str, dict[str, tuple[str, str]]] = {}
	exported_type_origins_by_module: dict[str, dict[str, dict[str, tuple[str, str]]]] = {}
	# Re-export target maps (for types/consts). Values are materialized as
	# trampolines, so consumers always reference the exporting module id.
	reexported_type_targets_by_module: dict[str, dict[str, dict[str, tuple[str, str]]]] = {}
	reexported_const_targets_by_module: dict[str, dict[str, tuple[str, str]]] = {}
	for mid, prog in merged_programs.items():
		(
			exported_values,
			exported_types,
			exported_consts,
			star_reexports,
		) = _build_export_interface(
			module_id=mid,
			merged_prog=prog,
			module_files=by_module.get(mid, []),
		)
		exports_values_by_module[mid] = exported_values
		exports_types_by_module[mid] = exported_types
		exports_consts_by_module[mid] = exported_consts
		star_reexports_by_module[mid] = star_reexports
		exported_const_origins_by_module[mid] = {n: (mid, n) for n in exported_consts}
		exported_type_origins_by_module[mid] = {
			"structs": {n: (mid, n) for n in exported_types.get("structs") or set()},
			"variants": {n: (mid, n) for n in exported_types.get("variants") or set()},
			"exceptions": {n: (mid, n) for n in exported_types.get("exceptions") or set()},
		}
		reexported_type_targets_by_module[mid] = {"structs": {}, "variants": {}, "exceptions": {}}
		reexported_const_targets_by_module[mid] = {}

	# Resolve star re-exports across modules deterministically.
	def _export_origin_lookup(
		mod: str,
	) -> tuple[dict[str, tuple[str, str]], dict[str, tuple[str, str]], dict[str, dict[str, tuple[str, str]]]]:
		"""Return (exported_values, exported_consts, exported_types_by_kind) with origin targets."""
		if mod in exports_values_by_module or mod in exports_types_by_module or mod in exports_consts_by_module:
			return (
				exports_values_by_module.get(mod) or {},
				exported_const_origins_by_module.get(mod) or {},
				exported_type_origins_by_module.get(mod) or {"structs": {}, "variants": {}, "exceptions": {}},
			)
		if external_module_exports is not None and mod in external_module_exports:
			ext = external_module_exports.get(mod) or {}
			values_obj = {n: (mod, n) for n in sorted(ext.get("values") or set())}
			consts_obj = {n: (mod, n) for n in sorted(ext.get("consts") or set())}
			types_obj: dict[str, dict[str, tuple[str, str]]] = {"structs": {}, "variants": {}, "exceptions": {}}
			ext_types = ext.get("types")
			if isinstance(ext_types, dict):
				for kind in ("structs", "variants", "exceptions"):
					for name in sorted(ext_types.get(kind) or set()):
						types_obj[kind][name] = (mod, name)
			ext_reexp = ext.get("reexports")
			if isinstance(ext_reexp, dict):
				ext_reexp_types = ext_reexp.get("types")
				ext_reexp_consts = ext_reexp.get("consts")
				if isinstance(ext_reexp_consts, dict):
					for name, v in ext_reexp_consts.items():
						if isinstance(v, dict):
							tm = v.get("module")
							tn = v.get("name")
							if isinstance(tm, str) and isinstance(tn, str):
								consts_obj[name] = (tm, tn)
				if isinstance(ext_reexp_types, dict):
					for kind in ("structs", "variants", "exceptions"):
						km = ext_reexp_types.get(kind)
						if isinstance(km, dict):
							for name, v in km.items():
								if isinstance(v, dict):
									tm = v.get("module")
									tn = v.get("name")
									if isinstance(tm, str) and isinstance(tn, str):
										types_obj[kind][name] = (tm, tn)
			return values_obj, consts_obj, types_obj
		return {}, {}, {"structs": {}, "variants": {}, "exceptions": {}}

	# We iterate until no progress so multi-hop star re-exports resolve deterministically.
	for _ in range(len(merged_programs) + 1):
		progress = False
		for mid, stars in star_reexports_by_module.items():
			for target_mod, ex_span in stars.items():
				if target_mod not in merged_programs and (external_module_exports is None or target_mod not in external_module_exports):
					diagnostics.append(
						Diagnostic(
							message=f"module '{mid}' re-exports unknown module '{target_mod}'",
							severity="error",
							span=ex_span,
						)
					)
					continue
				vals, consts, types_obj = _export_origin_lookup(target_mod)
				for name, origin in vals.items():
					prev = exports_values_by_module[mid].get(name)
					if prev is None:
						exports_values_by_module[mid][name] = origin
						progress = True
					elif prev != origin:
						diagnostics.append(
							Diagnostic(
								message=(
									f"exported name '{name}' is ambiguous due to re-exports "
									f"('{prev[0]}' vs '{origin[0]}') in module '{mid}'"
								),
								severity="error",
								span=ex_span,
							)
						)
				for name, origin in consts.items():
					prev = exported_const_origins_by_module[mid].get(name)
					if prev is None:
						exports_consts_by_module[mid].add(name)
						exported_const_origins_by_module[mid][name] = origin
						if origin[0] != mid:
							reexported_const_targets_by_module[mid][name] = origin
						progress = True
					elif prev != origin:
						diagnostics.append(
							Diagnostic(
								message=(
									f"exported const '{name}' is ambiguous due to re-exports "
									f"('{prev[0]}' vs '{origin[0]}') in module '{mid}'"
								),
								severity="error",
								span=ex_span,
							)
						)
				for kind, origins in types_obj.items():
					for name, origin in origins.items():
						prev = exported_type_origins_by_module[mid][kind].get(name)
						if prev is None:
							exports_types_by_module[mid][kind].add(name)
							exported_type_origins_by_module[mid][kind][name] = origin
							if origin[0] != mid:
								reexported_type_targets_by_module[mid][kind][name] = origin
							progress = True
						elif prev != origin:
							diagnostics.append(
								Diagnostic(
									message=(
										f"exported type '{name}' is ambiguous due to re-exports "
										f"('{prev[0]}' vs '{origin[0]}') in module '{mid}'"
									),
									severity="error",
									span=ex_span,
								)
							)
		if not progress:
			break

	def _union_exported_types(types_obj: dict[str, set[str]] | None) -> set[str]:
		if not types_obj:
			return set()
		out: set[str] = set()
		for vs in types_obj.values():
			out |= set(vs)
		return out

	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	# Export interface summary (used by package emission and future tooling).
	module_exports: dict[str, dict[str, list[str]]] = {}
	for mid in merged_programs.keys():
		vals = exports_values_by_module.get(mid, {})
		types = exports_types_by_module.get(mid, {"structs": set(), "variants": set(), "exceptions": set()})
		consts = exports_consts_by_module.get(mid, set())
		reexp_types = reexported_type_targets_by_module.get(mid, {"structs": {}, "variants": {}, "exceptions": {}})
		reexp_consts = reexported_const_targets_by_module.get(mid, {})
		module_exports[mid] = {
			"values": sorted(list(vals.keys())),
			"types": {
				"structs": sorted(list(types.get("structs", set()))),
				"variants": sorted(list(types.get("variants", set()))),
				"exceptions": sorted(list(types.get("exceptions", set()))),
			},
			"consts": sorted(list(consts)),
			"reexports": {
				"types": {
					"structs": {n: {"module": m, "name": s} for n, (m, s) in sorted(reexp_types.get("structs", {}).items())},
					"variants": {n: {"module": m, "name": s} for n, (m, s) in sorted(reexp_types.get("variants", {}).items())},
					"exceptions": {n: {"module": m, "name": s} for n, (m, s) in sorted(reexp_types.get("exceptions", {}).items())},
				},
				"consts": {n: {"module": m, "name": s} for n, (m, s) in sorted(reexp_consts.items())},
			},
		}

	# Resolve imports and build a dependency graph.
	#
	# MVP rule: import bindings are per-file. Module dependencies, however, are
	# computed at module granularity (a module depends on another module if any
	# of its files import it).
	#
	# Keep per-edge provenance so cycle diagnostics can be source-anchored.
	# Each edge is (to_module, span).
	dep_edges: dict[str, list[tuple[str, Span]]] = {mid: [] for mid in merged_programs}
	module_aliases_by_file: dict[Path, dict[str, str]] = {}
	for mid, files in by_module.items():
		for path, prog in files:
			file_module_aliases: dict[str, str] = {}

			for imp in getattr(prog, "imports", []) or []:
				mod = ".".join(getattr(imp, "path", []) or [])
				if not mod:
					continue
				span = _span_in_file(path, getattr(imp, "loc", None))
				dep_edges[mid].append((mod, span))
				if mod not in merged_programs and (external_module_exports is None or mod not in external_module_exports):
					diagnostics.append(Diagnostic(message=f"imported module '{mod}' not found", severity="error", span=span))
					continue
				alias = getattr(imp, "alias", None) or (getattr(imp, "path", []) or [mod])[-1]
				prev = file_module_aliases.get(alias)
				if prev is None:
					file_module_aliases[alias] = mod
				elif prev != mod:
					diagnostics.append(
						Diagnostic(
							message=f"import alias '{alias}' conflicts: cannot import both '{prev}' and '{mod}' as '{alias}'",
							severity="error",
							span=span,
						)
					)

			# Record module aliases for later module-qualified access resolution.
			# This is per-file by design (imports are file-scoped in MVP).
			module_aliases_by_file[path] = dict(file_module_aliases)

	# Collapse edge lists into a simple adjacency set for cycle detection.
	# Include external modules so visibility rules can see package imports.
	deps: dict[str, set[str]] = {
		mid: {to for (to, _sp) in edges if to in merged_programs or (external_module_exports and to in external_module_exports)}
		for mid, edges in dep_edges.items()
	}

	# Resolve module-qualified type references using per-file module aliases and
	# module export interfaces.
	#
	# After successful resolution we record the canonical `module_id` on the type
	# expression (and rewrite imported aliases to their original symbol name). This
	# preserves module-scoped nominal identity end-to-end.
	def _exported_types_for_module(mod: str) -> set[str]:
		if mod in exports_types_by_module:
			return _union_exported_types(exports_types_by_module.get(mod))
		if external_module_exports is not None and mod in external_module_exports:
			ext = external_module_exports.get(mod) or {}
			ext_types = ext.get("types")
			if isinstance(ext_types, dict):
				return set(ext_types.get("structs") or set()) | set(ext_types.get("variants") or set()) | set(
					ext_types.get("exceptions") or set()
				)
			return set()
		return set()

	def _resolve_type_expr_in_file(
		path: Path,
		file_aliases: dict[str, str],
		te: parser_ast.TypeExpr | None,
	) -> None:
		if te is None:
			return
		if getattr(te, "module_alias", None):
			alias = te.module_alias
			mod = file_aliases.get(alias or "")
			span = _span_in_file(path, getattr(te, "loc", None))
			if mod is None:
				diagnostics.append(
					Diagnostic(
						message=f"unknown module alias '{alias}' in type reference '{alias}.{te.name}'",
						severity="error",
						span=span,
					)
				)
			else:
				types = _exported_types_for_module(mod)
				if te.name not in types:
					available = ", ".join(sorted(types))
					notes = (
						[f"available exported types: {available}"]
						if available
						else [f"module '{mod}' exports no types (private by default)"]
					)
					diagnostics.append(
						Diagnostic(
							message=f"module '{mod}' does not export type '{te.name}'",
							severity="error",
							span=span,
							notes=notes,
						)
					)
				else:
					# Record the canonical module id for later lowering.
					#
					# If `mod` re-exports this type, resolve it to the defining module
					# identity (no type duplication across module interfaces).
					def_mod, def_name = (mod, te.name)
					reexp = reexported_type_targets_by_module.get(mod)
					if reexp is not None:
						for kind in ("structs", "variants", "exceptions"):
							if te.name in (exports_types_by_module.get(mod) or {}).get(kind, set()):
								def_mod, def_name = reexp.get(kind, {}).get(te.name, (mod, te.name))
								break
					elif external_module_exports is not None and mod in external_module_exports:
						ext = external_module_exports.get(mod) or {}
						ext_reexp = ext.get("reexports")
						ext_types = ext.get("types")
						if isinstance(ext_reexp, dict) and isinstance(ext_types, dict):
							ext_reexp_types = ext_reexp.get("types")
							if isinstance(ext_reexp_types, dict):
								for kind in ("structs", "variants", "exceptions"):
									kind_set = set(ext_types.get(kind) or set())
									if te.name in kind_set:
										tgt = ext_reexp_types.get(kind, {}).get(te.name) if isinstance(ext_reexp_types.get(kind), dict) else None
										if isinstance(tgt, dict):
											tm = tgt.get("module")
											tn = tgt.get("name")
											if isinstance(tm, str) and isinstance(tn, str):
												def_mod, def_name = (tm, tn)
										break
					te.module_id = def_mod
					te.name = def_name
					te.module_alias = None
		for a in getattr(te, "args", []) or []:
			_resolve_type_expr_in_file(path, file_aliases, a)

	def _resolve_types_in_block(path: Path, file_aliases: dict[str, str], blk: parser_ast.Block) -> None:
		for st in getattr(blk, "statements", []) or []:
			# Resolve any type-level references embedded in expressions (e.g.,
			# `TypeRef::Ctor(...)` where `TypeRef` may include a module alias).
			def _resolve_types_in_expr(expr: parser_ast.Expr) -> None:
				if isinstance(expr, parser_ast.QualifiedMember):
					_resolve_type_expr_in_file(path, file_aliases, expr.base_type)
					return
				if isinstance(expr, parser_ast.Call):
					_resolve_types_in_expr(expr.func)
					for a in getattr(expr, "args", []) or []:
						_resolve_types_in_expr(a)
					for kw in getattr(expr, "kwargs", []) or []:
						_resolve_types_in_expr(kw.value)
					return
				if isinstance(expr, parser_ast.Attr):
					_resolve_types_in_expr(expr.value)
					return
				if isinstance(expr, parser_ast.Index):
					_resolve_types_in_expr(expr.value)
					_resolve_types_in_expr(expr.index)
					return
				if isinstance(expr, parser_ast.Unary):
					_resolve_types_in_expr(expr.operand)
					return
				if isinstance(expr, parser_ast.Binary):
					_resolve_types_in_expr(expr.left)
					_resolve_types_in_expr(expr.right)
					return
				if isinstance(expr, parser_ast.Move):
					_resolve_types_in_expr(expr.value)
					return
				if isinstance(expr, parser_ast.Ternary):
					_resolve_types_in_expr(expr.condition)
					_resolve_types_in_expr(expr.then_value)
					_resolve_types_in_expr(expr.else_value)
					return
				if isinstance(expr, parser_ast.ArrayLiteral):
					for e in getattr(expr, "elements", []) or []:
						_resolve_types_in_expr(e)
					return
				if isinstance(expr, parser_ast.TryCatchExpr):
					_resolve_types_in_expr(expr.attempt)
					for arm in getattr(expr, "catch_arms", []) or []:
						_resolve_types_in_block(path, file_aliases, arm.block)
					return
				if isinstance(expr, parser_ast.MatchExpr):
					_resolve_types_in_expr(expr.scrutinee)
					for arm in getattr(expr, "arms", []) or []:
						_resolve_types_in_block(path, file_aliases, arm.block)
					return
				if isinstance(expr, parser_ast.ExceptionCtor):
					for a in getattr(expr, "args", []) or []:
						_resolve_types_in_expr(a)
					for kw in getattr(expr, "kwargs", []) or []:
						_resolve_types_in_expr(kw.value)
					return
				if isinstance(expr, parser_ast.FString):
					for h in getattr(expr, "holes", []) or []:
						_resolve_types_in_expr(h.expr)
					return
				# literals/names/placeholders are leaf nodes

			if isinstance(st, parser_ast.LetStmt) and getattr(st, "type_expr", None) is not None:
				_resolve_type_expr_in_file(path, file_aliases, st.type_expr)
			if isinstance(st, parser_ast.LetStmt):
				_resolve_types_in_expr(st.value)
			if isinstance(st, parser_ast.AssignStmt):
				_resolve_types_in_expr(st.target)
				_resolve_types_in_expr(st.value)
			if isinstance(st, parser_ast.AugAssignStmt):
				_resolve_types_in_expr(st.target)
				_resolve_types_in_expr(st.value)
			if isinstance(st, parser_ast.ReturnStmt) and st.value is not None:
				_resolve_types_in_expr(st.value)
			if isinstance(st, parser_ast.ExprStmt):
				_resolve_types_in_expr(st.value)
			if isinstance(st, parser_ast.IfStmt):
				_resolve_types_in_expr(st.condition)
				_resolve_types_in_block(path, file_aliases, st.then_block)
				if st.else_block is not None:
					_resolve_types_in_block(path, file_aliases, st.else_block)
			if isinstance(st, parser_ast.TryStmt):
				if isinstance(getattr(st, "attempt", None), parser_ast.Expr):
					_resolve_types_in_expr(st.attempt)
				_resolve_types_in_block(path, file_aliases, st.body)
				for c in getattr(st, "catches", []) or []:
					_resolve_types_in_block(path, file_aliases, c.block)
			if isinstance(st, parser_ast.WhileStmt):
				_resolve_types_in_expr(st.condition)
				_resolve_types_in_block(path, file_aliases, st.body)
			if isinstance(st, parser_ast.ForStmt):
				_resolve_types_in_expr(st.iter_expr)
				_resolve_types_in_block(path, file_aliases, st.body)
			if isinstance(st, parser_ast.ThrowStmt):
				_resolve_types_in_expr(st.expr)

	for mid, files in by_module.items():
		for path, prog in files:
			file_aliases = module_aliases_by_file.get(path, {})
			# Top-level declarations.
			for fn in getattr(prog, "functions", []) or []:
				for p in getattr(fn, "params", []) or []:
					_resolve_type_expr_in_file(path, file_aliases, p.type_expr)
				_resolve_type_expr_in_file(path, file_aliases, getattr(fn, "return_type", None))
				_resolve_types_in_block(path, file_aliases, fn.body)
			for impl in getattr(prog, "implements", []) or []:
				_resolve_type_expr_in_file(path, file_aliases, impl.target)
				for mfn in getattr(impl, "methods", []) or []:
					for p in getattr(mfn, "params", []) or []:
						_resolve_type_expr_in_file(path, file_aliases, p.type_expr)
					_resolve_type_expr_in_file(path, file_aliases, getattr(mfn, "return_type", None))
					_resolve_types_in_block(path, file_aliases, mfn.body)
			for s in getattr(prog, "structs", []) or []:
				for f in getattr(s, "fields", []) or []:
					_resolve_type_expr_in_file(path, file_aliases, f.type_expr)
			for e in getattr(prog, "exceptions", []) or []:
				for a in getattr(e, "args", []) or []:
					_resolve_type_expr_in_file(path, file_aliases, a.type_expr)
			for v in getattr(prog, "variants", []) or []:
				for arm in getattr(v, "arms", []) or []:
					for f in getattr(arm, "fields", []) or []:
						_resolve_type_expr_in_file(path, file_aliases, f.type_expr)

	# Cycle detection (MVP: reject import cycles).
	def _find_cycle() -> list[str] | None:
		vis: set[str] = set()
		stack: list[str] = []
		onstack: set[str] = set()

		def dfs(n: str) -> list[str] | None:
			vis.add(n)
			stack.append(n)
			onstack.add(n)
			for m in deps.get(n, set()):
				if m not in merged_programs:
					continue
				if m not in vis:
					c = dfs(m)
					if c is not None:
						return c
				elif m in onstack:
					try:
						i = stack.index(m)
					except ValueError:
						i = 0
					return stack[i:] + [m]
			stack.pop()
			onstack.remove(n)
			return None

		for n in merged_programs:
			if n not in vis:
				c = dfs(n)
				if c is not None:
					return c
		return None

	cycle = _find_cycle()
	if cycle is not None:
		# Anchor the diagnostic to one concrete import site in the cycle.
		# (We choose the first edge in the reported cycle.)
		primary_span: Span | None = None
		notes: list[str] = []
		for i in range(len(cycle) - 1):
			a = cycle[i]
			b = cycle[i + 1]
			for to, sp in dep_edges.get(a, []):
				if to == b:
					if i == 0:
						primary_span = sp
					notes.append(f"{a} imports {b}")
					break
		if primary_span is None:
			# Fallback: pick any import edge span from any node in the cycle.
			for node in cycle:
				for _to, sp in dep_edges.get(node, []):
					primary_span = sp
					break
				if primary_span is not None:
					break
		diagnostics.append(
			Diagnostic(
				message=f"import cycle detected: {' -> '.join(cycle)}",
				severity="error",
				span=primary_span or Span(),
				notes=notes,
			)
		)

	if any(d.severity == "error" for d in diagnostics):
		return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	# Lower modules using a shared TypeTable so TypeIds remain comparable across the workspace.
	shared_type_table = TypeTable()
	_prime_builtins(shared_type_table)
	# Pre-declare all nominal type names across the workspace before lowering any
	# individual module.
	#
	# This prevents cross-module type references (e.g. `import lib as x; val p: x.Point`)
	# from accidentally minting placeholder scalar TypeIds via `ensure_named` when
	# the defining module hasn't been lowered yet. `declare_struct`/`declare_variant`
	# are idempotent when the kind already matches, so later per-module lowering
	# can safely re-run its local declaration passes.
	for _mid, _prog in merged_programs.items():
		for _s in getattr(_prog, "structs", []) or []:
			try:
				struct_id = shared_type_table.declare_struct(
					_mid,
					_s.name,
					[f.name for f in getattr(_s, "fields", []) or []],
					list(getattr(_s, "type_params", []) or []),
				)
				field_templates = [
					StructFieldSchema(
						name=_f.name,
						type_expr=_generic_type_expr_from_parser(
							_f.type_expr, type_params=list(getattr(_s, "type_params", []) or [])
						),
					)
					for _f in getattr(_s, "fields", []) or []
				]
				shared_type_table.define_struct_schema_fields(struct_id, field_templates)
			except ValueError as err:
				diagnostics.append(Diagnostic(message=str(err), severity="error", span=Span.from_loc(getattr(_s, "loc", None))))
		for _v in getattr(_prog, "variants", []) or []:
			arms: list[VariantArmSchema] = []
			for _arm in getattr(_v, "arms", []) or []:
				fields = [
					VariantFieldSchema(
						name=_f.name,
						type_expr=_generic_type_expr_from_parser(
							_f.type_expr, type_params=list(getattr(_v, "type_params", []) or [])
						),
					)
					for _f in getattr(_arm, "fields", []) or []
				]
				arms.append(VariantArmSchema(name=_arm.name, fields=fields))
			try:
				shared_type_table.declare_variant(
					_mid,
					_v.name,
					list(getattr(_v, "type_params", []) or []),
					arms,
				)
			except ValueError as err:
				diagnostics.append(Diagnostic(message=str(err), severity="error", span=Span.from_loc(getattr(_v, "loc", None))))

		if any(d.severity == "error" for d in diagnostics):
			return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	def _qualify_fn_name(module_id: str, name: str) -> str:
		# MVP: symbols in the default `main` module remain unqualified so
		# single-module programs keep legacy names.
		if module_id in (None, "main"):
			return name
		return f"{module_id}::{name}"

	def _qualify_symbol(module_id: str, sym: str, *, local_free_fns: set[str]) -> str:
		"""
		Qualify a module-local callable symbol for inclusion in a multi-module build.

		MVP intent: avoid collisions when multiple modules are lowered into a single
		LLVM module by making callable symbols module-scoped:
		- free functions: `foo` → `mod::foo` (except the entry `main`)
		- methods: `Type::method` → `mod::Type::method`

		This does not define a long-term symbol identity model; it's a pragmatic
		naming layer until the resolver carries explicit module/type identities.
		"""
		if sym in local_free_fns:
			return _qualify_fn_name(module_id, sym)
		if "::" in sym and not sym.startswith(f"{module_id}::"):
			return f"{module_id}::{sym}"
		return sym

	all_func_hirs: dict[FunctionId, H.HBlock] = {}
	all_sigs: dict[FunctionId, FnSignature] = {}
	fn_ids_by_name: dict[str, list[FunctionId]] = {}
	exc_catalog: dict[str, int] = {}
	fn_owner_module: dict[FunctionId, str] = {}
	fn_symbol_by_id: dict[FunctionId, str] = {}

	# Lower each module and qualify its callable symbols.
	for mid, prog in merged_programs.items():
		func_hirs, sigs, ids_by_name, _table, excs, diags = _lower_parsed_program_to_hir(
			prog,
			diagnostics=[],
			type_table=shared_type_table,
		)
		diagnostics.extend(diags)
		exc_catalog.update(excs)

		local_free_fns = {fn.name for fn in getattr(prog, "functions", []) or []}
		exported_values = exports_values_by_module.get(mid, {})

		# Qualify and copy function bodies/signatures.
		for fn_id, block in func_hirs.items():
			local_name = fn_id.name
			global_name = _qualify_symbol(mid, local_name, local_free_fns=local_free_fns)
			all_func_hirs[fn_id] = block
			fn_owner_module[fn_id] = mid
			fn_symbol_by_id[fn_id] = global_name
			fn_ids_by_name.setdefault(global_name, []).append(fn_id)

		for fn_id, sig in sigs.items():
			local_name = fn_id.name
			global_name = _qualify_symbol(mid, local_name, local_free_fns=local_free_fns)
			# Mark module-interface entry points early so downstream phases can
			# enforce visibility and (later) ABI-boundary rules consistently.
			is_exported = (local_name in local_free_fns) and (local_name in exported_values) and (local_name != "main")
			all_sigs[fn_id] = replace(sig, name=global_name, is_exported_entrypoint=is_exported)

	# Materialize re-exported functions as trampoline entry points.
	#
	# Even though `export { foo }` can refer to an imported binding (re-export),
	# module interfaces must remain self-contained: importing a symbol from a module
	# binds to `module::symbol`, not to some hidden downstream module.
	#
	# MVP implementation strategy:
	# - if module `a` exports `foo` that maps to an underlying target `(b, foo)`,
	#   synthesize `a::foo` as a trivial trampoline calling `b::foo`.
	# - this ensures exported entrypoints exist in the exporting module and keeps
	#   future package/interface metadata straightforward.
	for mid, exported_values in exports_values_by_module.items():
		for export_name, (target_mod, target_sym) in exported_values.items():
			if export_name == "main":
				continue
			if (target_mod, target_sym) == (mid, export_name):
				continue
			trampoline_name = _qualify_fn_name(mid, export_name)
			if fn_ids_by_name.get(trampoline_name):
				continue
			target_name = _qualify_fn_name(target_mod, target_sym)
			target_ids = fn_ids_by_name.get(target_name) or []
			if not target_ids:
				diagnostics.append(
					Diagnostic(
						message=f"internal: missing signature for re-export target '{target_mod}::{target_sym}'",
						severity="error",
						span=Span(),
					)
				)
				continue
			if len(target_ids) > 1:
				diagnostics.append(
					Diagnostic(
						message=f"ambiguous re-export target '{target_mod}::{target_sym}' (overloaded)",
						severity="error",
						span=Span(),
					)
				)
				continue
			target_id = target_ids[0]
			target_sig = all_sigs.get(target_id)
			if target_sig is None:
				diagnostics.append(
					Diagnostic(
						message=f"internal: missing signature for re-export target '{target_mod}::{target_sym}'",
						severity="error",
						span=Span(),
					)
				)
				continue
			ordinal = len(fn_ids_by_name.get(trampoline_name, []))
			trampoline_id = FunctionId(module=mid, name=export_name, ordinal=ordinal)
			all_sigs[trampoline_id] = replace(target_sig, name=trampoline_name, is_exported_entrypoint=True)
			fn_owner_module[trampoline_id] = mid
			fn_symbol_by_id[trampoline_id] = trampoline_name
			fn_ids_by_name.setdefault(trampoline_name, []).append(trampoline_id)

			# Build a minimal HIR body that forwards to the underlying target.
			arg_exprs: list[H.HExpr] = []
			for p in getattr(target_sig, "param_names", None) or []:
				if p:
					arg_exprs.append(H.HVar(name=p))
			callee = H.HVar(name=target_name)
			call_expr = H.HCall(fn=callee, args=arg_exprs)
			if target_sig.return_type_id is not None and shared_type_table.is_void(target_sig.return_type_id):
				all_func_hirs[trampoline_id] = H.HBlock(
					statements=[
						H.HExprStmt(expr=call_expr),
						H.HReturn(value=None),
					]
				)
			else:
				all_func_hirs[trampoline_id] = H.HBlock(statements=[H.HReturn(value=call_expr)])

		if any(d.severity == "error" for d in diagnostics):
			return {}, {}, {}, TypeTable(), {}, {}, {}, diagnostics

	# Materialize const re-exports into the exporting module’s const table when
	# the origin const value is already available in the shared TypeTable.
	#
	# This covers the source-only workspace case (all modules provided as source).
	# When the origin const is provided by a package, the value is imported later
	# in the driver pipeline (after package TypeId remapping); in that case we
	# leave the const unresolved here and let `driftc` materialize it once the
	# origin const becomes available.
	for exporting_mid, targets in reexported_const_targets_by_module.items():
		for local_name, (origin_mid, origin_name) in targets.items():
			origin_sym = f"{origin_mid}::{origin_name}"
			dst_sym = f"{exporting_mid}::{local_name}"
			origin_entry = shared_type_table.lookup_const(origin_sym)
			if origin_entry is None:
				continue
			origin_tid, origin_val = origin_entry
			prev = shared_type_table.lookup_const(dst_sym)
			if prev is not None:
				if prev != (origin_tid, origin_val):
					diagnostics.append(
						Diagnostic(
							message=f"const '{dst_sym}' defined with a different value than re-export target '{origin_sym}'",
							severity="error",
							span=Span(),
						)
					)
				continue
			shared_type_table.define_const(module_id=exporting_mid, name=local_name, type_id=origin_tid, value=origin_val)

	# Rewrite call sites: HCall(fn=HVar(name="foo")) -> HVar(name="m::foo") for imported/local functions.
	local_maps: dict[str, dict[str, str]] = {
		mid: {fn.name: _qualify_fn_name(mid, fn.name) for fn in getattr(prog, "functions", []) or []}
		for mid, prog in merged_programs.items()
	}

	def _rewrite_calls_in_block(
		block: H.HBlock,
		*,
		module_id: str,
		fn_id: FunctionId,
		fn_symbol: str,
		origin_file: Path | None,
	) -> None:
		local_map = local_maps.get(module_id, {})
		file_module_aliases = module_aliases_by_file.get(origin_file or Path(), {})
		# Call-site rewriting must be scope-correct: a local binding shadows only
		# within its lexical block, not across the whole function.
		#
		# This is still a limited MVP resolver (it only rewrites direct calls
		# represented as `HCall(HVar("foo"))`), but it avoids silent miscompiles by:
		# - never rewriting names that are currently bound (params, lets, binders),
		# - applying bindings as statements are traversed (let-binding is visible
		#   only *after* its initializer).
		param_names: list[str] = []
		sig = all_sigs.get(fn_id)
		if sig is not None and getattr(sig, "param_names", None):
			param_names = [p for p in sig.param_names if p]

		def rewrite_name(name: str, *, bound: set[str]) -> str:
			if name in bound:
				return name
			if name in local_map:
				return local_map[name]
			return name

		def rewrite_const_name(name: str, *, bound: set[str]) -> str:
			if name in bound:
				return name
			return name

		def exported_value_names(mod: str) -> set[str]:
			if mod in exports_values_by_module:
				return set((exports_values_by_module.get(mod) or {}).keys())
			if external_module_exports is not None and mod in external_module_exports:
				ext = external_module_exports.get(mod) or {}
				return set(ext.get("values") or set())
			return set()

		def exported_type_names(mod: str) -> set[str]:
			if mod in exports_types_by_module:
				return _union_exported_types(exports_types_by_module.get(mod))
			if external_module_exports is not None and mod in external_module_exports:
				ext = external_module_exports.get(mod) or {}
				ext_types = ext.get("types")
				if isinstance(ext_types, dict):
					return (
						set(ext_types.get("structs") or set())
						| set(ext_types.get("variants") or set())
						| set(ext_types.get("exceptions") or set())
					)
				return set()
			return set()

		def exported_const_names(mod: str) -> set[str]:
			if mod in exports_consts_by_module:
				return set(exports_consts_by_module.get(mod) or set())
			if external_module_exports is not None and mod in external_module_exports:
				ext = external_module_exports.get(mod) or {}
				return set(ext.get("consts") or set())
			return set()

		def exported_struct_names(mod: str) -> set[str]:
			if mod in exports_types_by_module:
				return set((exports_types_by_module.get(mod) or {}).get("structs") or set())
			if external_module_exports is not None and mod in external_module_exports:
				ext = external_module_exports.get(mod) or {}
				ext_types = ext.get("types")
				if isinstance(ext_types, dict):
					return set(ext_types.get("structs") or set())
				return set()
			return set()

		def _rewrite_module_qualified_call(
			*,
			receiver: H.HExpr,
			member: str,
			args: list[H.HExpr],
			kwargs: list[H.HKwArg],
			type_args: list[object] | None,
		) -> H.HExpr | None:
			"""
			Rewrite a syntactic member call `x.member(...)` when `x` is a module alias.

			MVP surface rule (pinned):
			  import lib as x
			  x.foo(1, 2)   // call exported function foo from module lib
			  x.Point(...)  // call struct constructor Point from module lib

			We do *not* create a runtime module object. Instead, we resolve the
			member at compile time and rewrite the callee to a fully-qualified
			callable symbol (`lib::foo`) or an unqualified struct constructor (`Point`).

			Note on representation: in stage1 HIR, a `.`-call like `x.foo(...)` is
			represented as `HMethodCall(receiver=x, method_name=\"foo\", ...)` (method
			sugar). We reuse that syntactic form for module-qualified access and
			rewrite it here into a plain `HCall` once we confirm `x` is a module alias.
			"""
			if not isinstance(receiver, H.HVar):
				return None
			if receiver.binding_id is not None:
				# Local/param shadowing wins: `x.foo` refers to the local `x`, not a module.
				return None
			alias = receiver.name
			mod = file_module_aliases.get(alias)
			if mod is None:
				return None
			vals = exported_value_names(mod)
			types = exported_type_names(mod)
			structs = exported_struct_names(mod)
			if member in vals:
				return H.HCall(
					fn=H.HVar(name=_qualify_fn_name(mod, member)),
					args=args,
					kwargs=kwargs,
					type_args=type_args,
				)
			if member in structs:
				# Constructor call through a module alias. MVP supports only struct ctors.
				def_mod, def_name = reexported_type_targets_by_module.get(mod, {}).get("structs", {}).get(member, (mod, member))
				struct_id = shared_type_table.get_nominal(kind=TypeKind.STRUCT, module_id=def_mod, name=def_name)
				if struct_id is None:
					diagnostics.append(
						Diagnostic(
							message=f"module-qualified constructor call '{alias}.{member}(...)' is only supported for structs in MVP",
							severity="error",
							span=getattr(receiver, "loc", Span()),
						)
					)
					return None
				# Rewrite to an internal fully-qualified constructor name so later
				# phases can resolve it deterministically even when multiple modules
				# define the same short type name.
				return H.HCall(
					fn=H.HVar(name=f"{def_mod}::{def_name}"),
					args=args,
					kwargs=kwargs,
					type_args=type_args,
				)
			available = ", ".join(sorted(vals | types))
			notes = (
				[f"available exports: {available}"]
				if available
				else [f"module '{mod}' exports nothing (private by default)"]
			)
			diagnostics.append(
				Diagnostic(
					message=f"module '{mod}' does not export symbol '{member}'",
					severity="error",
					span=getattr(receiver, "loc", Span()),
					notes=notes,
				)
			)
			return None

		def walk_block(b: H.HBlock, *, bound: set[str]) -> None:
			scope_bound = set(bound)
			for st in b.statements:
				walk_stmt(st, bound=scope_bound)
				if isinstance(st, H.HLet):
					scope_bound.add(st.name)

		def walk_expr(expr: H.HExpr, *, bound: set[str]) -> H.HExpr:
			# Module-qualified access: the surface syntax is `x.foo(...)`. Stage1
			# initially represents this as `HMethodCall`, so we rewrite that form
			# when `x` resolves to a module alias in the current file.
			if isinstance(expr, H.HMethodCall):
				expr.receiver = walk_expr(expr.receiver, bound=bound)
				expr.args = [walk_expr(a, bound=bound) for a in expr.args]
				for kw in getattr(expr, "kwargs", []) or []:
					if getattr(kw, "value", None) is not None:
						kw.value = walk_expr(kw.value, bound=bound)
				rewritten = _rewrite_module_qualified_call(
					receiver=expr.receiver,
					member=expr.method_name,
					args=expr.args,
					kwargs=getattr(expr, "kwargs", []) or [],
					type_args=getattr(expr, "type_args", None),
				)
				if rewritten is not None:
					return rewritten
				return expr

			if isinstance(expr, H.HCall):
				expr.fn = walk_expr(expr.fn, bound=bound)
				expr.args = [walk_expr(a, bound=bound) for a in expr.args]
				for kw in getattr(expr, "kwargs", []) or []:
					if getattr(kw, "value", None) is not None:
						kw.value = walk_expr(kw.value, bound=bound)
				# Resolve direct calls that name a local/imported/free function.
				if isinstance(expr.fn, H.HVar):
					expr.fn.name = rewrite_name(expr.fn.name, bound=bound)
				elif isinstance(expr.fn, H.HField) and isinstance(expr.fn.subject, H.HVar):
					# Handle the (rarer) explicit field-call form: `(x.foo)(...)`.
					q = _rewrite_module_qualified_call(
						receiver=expr.fn.subject,
						member=expr.fn.name,
						args=expr.args,
						kwargs=getattr(expr, "kwargs", []) or [],
						type_args=getattr(expr, "type_args", None),
					)
					if isinstance(q, H.HCall):
						# Preserve the rewritten call and ignore the original callee expression.
						return q
				return expr

			if isinstance(expr, H.HVar):
				expr.name = rewrite_const_name(expr.name, bound=bound)
				return expr

			if isinstance(expr, H.HField) and isinstance(expr.subject, H.HVar) and expr.subject.binding_id is None:
				mod = file_module_aliases.get(expr.subject.name)
				if mod is not None:
					if expr.name in exported_value_names(mod):
						return H.HVar(name=_qualify_fn_name(mod, expr.name))
					if expr.name in exported_const_names(mod):
						# Module-qualified const access always targets the module’s own
						# const table. Const re-exports are materialized by copying the
						# literal value into the exporting module, so consumers do not
						# need to reference the origin module.
						return H.HVar(name=f"{mod}::{expr.name}")
					available = ", ".join(sorted(exported_value_names(mod) | exported_const_names(mod) | exported_type_names(mod)))
					notes = (
						[f"available exports: {available}"]
						if available
						else [f"module '{mod}' exports nothing (private by default)"]
					)
					diagnostics.append(
						Diagnostic(
							message=f"module '{mod}' does not export symbol '{expr.name}'",
							severity="error",
							span=getattr(expr.subject, "loc", Span()),
							notes=notes,
						)
					)
					# Note: module-qualified type names are handled in type positions
					# via TypeExpr.module_id. Expression-position `x.Point` without
					# call is not a supported surface construct in MVP.
				return expr

			# Generic recursion for other expression shapes.
			for k, child in list(getattr(expr, "__dict__", {}).items()):
				if isinstance(child, H.HExpr):
					setattr(expr, k, walk_expr(child, bound=bound))
				elif isinstance(child, H.HBlock):
					walk_block(child, bound=bound)
				elif isinstance(child, list):
					new_list = []
					for it in child:
						if isinstance(it, H.HExpr):
							new_list.append(walk_expr(it, bound=bound))
						elif isinstance(it, H.HBlock):
							walk_block(it, bound=bound)
							new_list.append(it)
						# Expression-form arms (match/try) live under expression nodes and
						# must be handled here so binders introduce lexical scopes.
						elif hasattr(H, "HMatchArm") and isinstance(it, getattr(H, "HMatchArm")):
							arm_bound = set(bound)
							for bname in getattr(it, "binders", []) or []:
								arm_bound.add(bname)
							walk_block(it.block, bound=arm_bound)
							if getattr(it, "result", None) is not None:
								it.result = walk_expr(it.result, bound=arm_bound)
							new_list.append(it)
						elif hasattr(H, "HTryExprArm") and isinstance(it, getattr(H, "HTryExprArm")):
							arm_bound = set(bound)
							if getattr(it, "binder", None):
								arm_bound.add(it.binder)
							walk_block(it.block, bound=arm_bound)
							if getattr(it, "result", None) is not None:
								it.result = walk_expr(it.result, bound=arm_bound)
							new_list.append(it)
						else:
							new_list.append(it)
					setattr(expr, k, new_list)
			return expr

		def walk_stmt(stmt: H.HStmt, *, bound: set[str]) -> None:
			if isinstance(stmt, H.HTry):
				walk_block(stmt.body, bound=bound)
				for arm in stmt.catches:
					arm_bound = set(bound)
					if arm.binder:
						arm_bound.add(arm.binder)
					walk_block(arm.block, bound=arm_bound)
				return
			for k, child in list(getattr(stmt, "__dict__", {}).items()):
				if isinstance(child, H.HExpr):
					setattr(stmt, k, walk_expr(child, bound=bound))
				elif isinstance(child, H.HBlock):
					walk_block(child, bound=bound)
				elif isinstance(child, list):
					new_list = []
					for it in child:
						if isinstance(it, H.HStmt):
							walk_stmt(it, bound=bound)
							new_list.append(it)
						elif isinstance(it, H.HExpr):
							new_list.append(walk_expr(it, bound=bound))
						elif isinstance(it, H.HBlock):
							walk_block(it, bound=bound)
							new_list.append(it)
						elif hasattr(H, "HCatchArm") and isinstance(it, getattr(H, "HCatchArm")):
							arm_bound = set(bound)
							if getattr(it, "binder", None):
								arm_bound.add(it.binder)
							walk_block(it.block, bound=arm_bound)
							new_list.append(it)
						elif hasattr(H, "HMatchArm") and isinstance(it, getattr(H, "HMatchArm")):
							arm_bound = set(bound)
							for bname in getattr(it, "binders", []) or []:
								arm_bound.add(bname)
							walk_block(it.block, bound=arm_bound)
							if getattr(it, "result", None) is not None:
								it.result = walk_expr(it.result, bound=arm_bound)
							new_list.append(it)
						elif hasattr(H, "HTryExprArm") and isinstance(it, getattr(H, "HTryExprArm")):
							arm_bound = set(bound)
							if getattr(it, "binder", None):
								arm_bound.add(it.binder)
							walk_block(it.block, bound=arm_bound)
							if getattr(it, "result", None) is not None:
								it.result = walk_expr(it.result, bound=arm_bound)
							new_list.append(it)
						else:
							new_list.append(it)
					setattr(stmt, k, new_list)

		initial_bound = set(param_names)
		walk_block(block, bound=initial_bound)

	# Apply rewrite to each function body using its origin file’s import environment.
	fn_origin_file: dict[str, Path] = {}
	for _mid, origins in origin_by_module.items():
		for fn_id, src_path in origins.items():
			fn_symbol = fn_symbol_by_id.get(fn_id, _qualify_fn_name(fn_id.module, fn_id.name))
			fn_origin_file[fn_symbol] = src_path

	for fn_id, block in all_func_hirs.items():
		fn_symbol = fn_symbol_by_id.get(fn_id, _qualify_fn_name(fn_id.module, fn_id.name))
		_rewrite_calls_in_block(
			block,
			module_id=fn_owner_module.get(fn_id, "main"),
			fn_id=fn_id,
			fn_symbol=fn_symbol,
			origin_file=fn_origin_file.get(fn_symbol),
		)

	# Cross-module exception code collision detection: event codes are derived
	# from the canonical event FQN (`module:Event`). Collisions are extremely
	# unlikely, but if they happen we must diagnose them deterministically.
	payload_seen: dict[int, str] = {}
	for fqn, code in exc_catalog.items():
		payload = code & PAYLOAD_MASK
		other = payload_seen.get(payload)
		if other is not None and other != fqn:
			diagnostics.append(Diagnostic(message=f"exception code collision between '{other}' and '{fqn}' (payload {payload})", severity="error", span=Span()))
		else:
			payload_seen[payload] = fqn

	return all_func_hirs, all_sigs, fn_ids_by_name, shared_type_table, exc_catalog, module_exports, deps, diagnostics


def _lower_parsed_program_to_hir(
	prog: parser_ast.Program,
	*,
	diagnostics: list[Diagnostic] | None = None,
	type_table: TypeTable | None = None,
) -> Tuple[Dict[FunctionId, H.HBlock], Dict[FunctionId, FnSignature], Dict[str, List[FunctionId]], "TypeTable", Dict[str, int], List[Diagnostic]]:
	"""
	Lower an already-parsed `Program` to HIR/signatures/type table.

	This is shared by both single-file and multi-file entry points.
	"""
	diagnostics = list(diagnostics or [])
	module_name = getattr(prog, "module", None)
	module_id = module_name or "main"
	func_hirs: Dict[FunctionId, H.HBlock] = {}
	fn_ids_by_name: Dict[str, List[FunctionId]] = {}
	decls: list[_FrontendDecl] = []
	signatures: Dict[FunctionId, FnSignature] = {}
	lowerer = AstToHIR()
	lowerer._module_name = module_id
	from lang2.driftc.traits.world import build_trait_world
	# Track method keys to prevent duplicate method bodies within the same impl.
	method_keys: set[tuple[tuple | None, tuple, str]] = set()  # (trait_key, impl_target_key, method_name)
	module_function_names: set[str] = {fn.name for fn in getattr(prog, "functions", []) or []}
	exception_schemas: dict[str, tuple[str, list[str]]] = {}
	struct_defs = list(getattr(prog, "structs", []) or [])
	variant_defs = list(getattr(prog, "variants", []) or [])
	exception_catalog: dict[str, int] = _build_exception_catalog(prog.exceptions, module_name, diagnostics)
	for exc in prog.exceptions:
		fqn = f"{module_name}:{exc.name}" if module_name else exc.name
		field_names = [arg.name for arg in getattr(exc, "args", [])]
		exception_schemas[fqn] = (fqn, field_names)
	# Build a TypeTable early so we can register user-defined type names (structs)
	# before resolving function signatures. This prevents `resolve_opaque_type`
	# from minting unrelated placeholder TypeIds for struct names.
	type_table = type_table or TypeTable()
	_prime_builtins(type_table)
	# Build a per-module TraitWorld and stash it on the shared TypeTable so later
	# phases can enforce requirements without re-parsing sources.
	world = build_trait_world(prog, diagnostics=diagnostics)
	trait_worlds = getattr(type_table, "trait_worlds", None)
	if not isinstance(trait_worlds, dict):
		trait_worlds = {}
	trait_worlds[module_id] = world
	type_table.trait_worlds = trait_worlds

	# Register module-local compile-time constants.
	#
	# MVP: const initializers are restricted to literal values (or unary +/- applied
	# to a numeric literal). We evaluate them here so later phases can
	# treat const references as typed literals without requiring whole-program
	# evaluation infrastructure.
	def _eval_const_value(expr: parser_ast.Expr) -> object | None:
		if isinstance(expr, parser_ast.Literal):
			return expr.value
		if isinstance(expr, parser_ast.Unary) and getattr(expr, "op", None) in ("-", "+"):
			inner = getattr(expr, "operand", None)
			if isinstance(inner, parser_ast.Literal) and isinstance(inner.value, (int, float)):
				if getattr(expr, "op", None) == "-":
					return -inner.value
				return inner.value
		return None

	for c in getattr(prog, "consts", []) or []:
		decl_ty = resolve_opaque_type(c.type_expr, type_table, module_id=module_id)
		val = _eval_const_value(c.value)
		if val is None:
			diagnostics.append(
				Diagnostic(
					phase="typecheck",
					message=(
						f"const '{c.name}' initializer must be a compile-time literal in MVP "
						"(Int/Uint/Bool/String/Float, optionally with unary '+' or '-')"
					),
					severity="error",
					span=Span.from_loc(getattr(c, "loc", None)),
				)
			)
			continue
		# Enforce that the declared type matches the literal kind exactly.
		#
		# Consts are intentionally strict: they form part of the module interface,
		# and packages must be able to embed them deterministically without
		# re-running the evaluator.
		ok = False
		if decl_ty == type_table.ensure_int() and isinstance(val, int):
			ok = True
		elif decl_ty == type_table.ensure_uint() and isinstance(val, int) and val >= 0:
			ok = True
		elif decl_ty == type_table.ensure_bool() and isinstance(val, bool):
			ok = True
		elif decl_ty == type_table.ensure_string() and isinstance(val, str):
			ok = True
		elif decl_ty == type_table.ensure_float() and isinstance(val, float):
			ok = True
		if not ok:
			diagnostics.append(
				Diagnostic(
					phase="typecheck",
					message=f"const '{c.name}' declared type does not match initializer value",
					severity="error",
					span=Span.from_loc(getattr(c, "loc", None)),
				)
			)
			continue
		type_table.define_const(module_id=module_id, name=c.name, type_id=decl_ty, value=val)
	# Prelude: `Optional<T>` is required for iterator-style `for` desugaring and
	# other control-flow sugar. Until modules are supported, the compiler injects
	# a canonical `Optional<T>` variant base into every compilation unit unless
	# user code declares its own `variant Optional<...>`.
	#
	# MVP contract:
	#   variant Optional<T> { Some(value: T), None }
	if not any(getattr(v, "name", None) == "Optional" for v in variant_defs) and type_table.get_variant_base(
		module_id="lang.core", name="Optional"
	) is None:
		type_table.declare_variant(
			"lang.core",
			"Optional",
			["T"],
			[
				VariantArmSchema(
					name="Some",
					fields=[VariantFieldSchema(name="value", type_expr=GenericTypeExpr.param(0))],
				),
				VariantArmSchema(name="None", fields=[]),
			],
		)
	# Declare all struct names first (placeholder field types) to support recursion.
	for s in struct_defs:
		field_names = [f.name for f in getattr(s, "fields", [])]
		try:
			type_table.declare_struct(
				module_id,
				s.name,
				field_names,
				list(getattr(s, "type_params", []) or []),
			)
		except ValueError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=Span.from_loc(getattr(s, "loc", None))))
	# Declare all variant names/schemas next so type resolution can instantiate
	# variants (e.g., Optional<Int>) while resolving later annotations/fields.
	for v in variant_defs:
		arms: list[VariantArmSchema] = []
		for arm in getattr(v, "arms", []) or []:
			fields = [
				VariantFieldSchema(
					name=f.name,
					type_expr=_generic_type_expr_from_parser(f.type_expr, type_params=list(getattr(v, "type_params", []) or [])),
				)
				for f in getattr(arm, "fields", []) or []
			]
			arms.append(VariantArmSchema(name=arm.name, fields=fields))
		try:
			type_table.declare_variant(
				module_id,
				v.name,
				list(getattr(v, "type_params", []) or []),
				arms,
			)
		except ValueError as err:
			diagnostics.append(Diagnostic(message=str(err), severity="error", span=Span.from_loc(getattr(v, "loc", None))))
	# Fill field TypeIds in a second pass now that all names exist.
	for s in struct_defs:
		struct_id = type_table.require_nominal(kind=TypeKind.STRUCT, module_id=module_id, name=s.name)
		type_params = list(getattr(s, "type_params", []) or [])
		field_types = []
		field_templates = []
		for f in getattr(s, "fields", []):
			field_templates.append(
				StructFieldSchema(
					name=f.name,
					type_expr=_generic_type_expr_from_parser(f.type_expr, type_params=type_params),
				)
			)
			if type_params:
				continue
			ft = resolve_opaque_type(f.type_expr, type_table, module_id=module_id)
			# MVP escape policy: references cannot be stored in long-lived memory.
			# Struct fields are long-lived by construction, so `struct S(r: &T)` is
			# rejected early (before lowering/typecheck) with a source-anchored
			# diagnostic.
			try:
				td = type_table.get(ft)
			except Exception:
				td = None
			if td is not None and td.kind is TypeKind.REF:
				diagnostics.append(
					Diagnostic(
						message=f"struct '{s.name}' field '{f.name}' cannot have a reference type in MVP",
						severity="error",
						span=Span.from_loc(getattr(f.type_expr, "loc", getattr(f, "loc", None))),
					)
				)
			field_types.append(ft)
		type_table.define_struct_schema_fields(struct_id, field_templates)
		if not type_params:
			type_table.define_struct_fields(struct_id, field_types)
	# After all variant schemas are known and structs are declared, finalize
	# non-generic variants so their concrete arm types are available.
	type_table.finalize_variants()
	seen_sig: dict[tuple, object | None] = {}
	name_ord: dict[str, int] = {}
	for fn in prog.functions:
		sig_key = (
			module_id,
			fn.name,
			len(getattr(fn, "params", []) or []),
			tuple(_type_expr_key(p.type_expr) for p in getattr(fn, "params", []) or []),
		)
		if sig_key in seen_sig:
			diagnostics.append(
				Diagnostic(
					message=f"duplicate function signature for '{fn.name}'",
					severity="error",
					span=Span.from_loc(getattr(fn, "loc", None)),
				)
			)
			continue
		seen_sig[sig_key] = getattr(fn, "loc", None)
		ordinal = name_ord.get(fn.name, 0)
		name_ord[fn.name] = ordinal + 1
		fn_id = FunctionId(module=module_id, name=fn.name, ordinal=ordinal)
		qualified_name = _qualify_fn_name(module_id, fn.name)
		fn_ids_by_name.setdefault(qualified_name, []).append(fn_id)
		decl_decl = _decl_from_parser_fn(fn, fn_id=fn_id)
		decl_decl.module = module_id
		# Reject FnResult in surface type annotations (return or parameter types).
		# FnResult is an internal ABI carrier in lang2, not a user-facing type.
		if _typeexpr_uses_internal_fnresult(decl_decl.return_type):
			_report_internal_fnresult_in_surface_type(
				kind="function",
				symbol=fn.name,
				loc=getattr(fn.return_type, "loc", getattr(fn, "loc", None)),
				diagnostics=diagnostics,
			)
		for p in getattr(fn, "params", []) or []:
			if _typeexpr_uses_internal_fnresult(p.type_expr):
				_report_internal_fnresult_in_surface_type(
					kind="parameter",
					symbol=f"{fn.name}({p.name})",
					loc=getattr(p.type_expr, "loc", getattr(p, "loc", None)),
					diagnostics=diagnostics,
				)
			if getattr(p, "non_escaping", False) and not _typeexpr_is_callable(p.type_expr):
				diagnostics.append(
					_diagnostic(
						f"nonescaping parameter '{fn.name}({p.name})' must have a callable type",
						getattr(p.type_expr, "loc", getattr(p, "loc", None)),
					)
				)
		decls.append(decl_decl)
		stmt_block = _convert_block(fn.body)
		param_names = [p.name for p in getattr(fn, "params", []) or []]
		hir_block = lowerer.lower_function_block(stmt_block, param_names=param_names)
		func_hirs[fn_id] = hir_block
	# Methods inside implement blocks.
	for impl_index, impl in enumerate(getattr(prog, "implements", [])):
		# Reject reference-qualified impl headers in v1 (must be nominal types).
		if getattr(impl.target, "name", None) in {"&", "&mut"}:
			diagnostics.append(
				Diagnostic(
					message="implement header must use a nominal type, not a reference type",
					severity="error",
					span=Span.from_loc(getattr(impl, "loc", None)),
				)
			)
			continue
		impl_type_params = list(getattr(impl, "type_params", []) or [])
		impl_type_param_locs = list(getattr(impl, "type_param_locs", []) or [])
		impl_target_str = _type_expr_key_str(impl.target)
		impl_trait_str = _type_expr_key_str(impl.trait) if getattr(impl, "trait", None) is not None else None
		impl_owner = FunctionId(
			module="lang.__internal",
			name=f"__impl_{module_id}::{impl_trait_str or 'inherent'}::{impl_target_str}",
			ordinal=impl_index,
		)
		for fn in impl.methods:
			# Note: receiver shape/name/type are semantic rules enforced by the
			# typecheck phase. The parser adapter stays structural-only here so
			# related errors consistently report as typecheck diagnostics.
			receiver_ty = fn.params[0].type_expr if fn.params else None
			self_mode: str | None = None
			if receiver_ty is not None:
				self_mode = "value"
				if receiver_ty.name == "&":
					self_mode = "ref"
				elif receiver_ty.name == "&mut":
					self_mode = "ref_mut"

			trait_key = _type_expr_key(impl.trait) if getattr(impl, "trait", None) is not None else None
			trait_str = _type_expr_key_str(impl.trait) if getattr(impl, "trait", None) is not None else None
			# Compute the canonical symbol for this method early so any diagnostics
			# (including type-annotation validation) can reference it.
			target_key = _impl_target_key(impl.target, impl_type_params)
			target_str = _type_expr_key_str(impl.target)
			if trait_str:
				symbol_name = f"{target_str}::{trait_str}::{fn.name}"
			else:
				symbol_name = f"{target_str}::{fn.name}"

			params = [
				_FrontendParam(
					p.name,
					p.type_expr,
					getattr(p, "loc", None),
					non_escaping=getattr(p, "non_escaping", False),
				)
				for p in fn.params
			]
			# Reject FnResult in method surface type annotations too.
			if _typeexpr_uses_internal_fnresult(fn.return_type):
				_report_internal_fnresult_in_surface_type(
					kind="method",
					symbol=symbol_name,
					loc=getattr(fn.return_type, "loc", getattr(fn, "loc", None)),
					diagnostics=diagnostics,
				)
			for p in getattr(fn, "params", []) or []:
				if _typeexpr_uses_internal_fnresult(p.type_expr):
					_report_internal_fnresult_in_surface_type(
						kind="parameter",
						symbol=f"{symbol_name}({p.name})",
						loc=getattr(p.type_expr, "loc", getattr(p, "loc", None)),
						diagnostics=diagnostics,
					)
				if getattr(p, "non_escaping", False) and not _typeexpr_is_callable(p.type_expr):
					diagnostics.append(
						_diagnostic(
							f"nonescaping parameter '{symbol_name}({p.name})' must have a callable type",
							getattr(p.type_expr, "loc", getattr(p, "loc", None)),
						)
					)
			if fn.name in module_function_names:
				diagnostics.append(
					Diagnostic(
						message=f"method '{fn.name}' conflicts with existing free function of the same name",
						severity="error",
						span=Span.from_loc(getattr(fn, "loc", None)),
					)
				)
				continue
			key = (trait_key, target_key, fn.name)
			if key in method_keys:
				impl_label = f"{trait_str} for {target_str}" if trait_str else target_str
				diagnostics.append(
					Diagnostic(
						message=f"duplicate method definition '{fn.name}' for type '{impl_label}'",
						severity="error",
						span=Span.from_loc(getattr(fn, "loc", None)),
					)
				)
				continue
			method_keys.add(key)
			ordinal = name_ord.get(symbol_name, 0)
			name_ord[symbol_name] = ordinal + 1
			fn_id = FunctionId(module=module_id, name=symbol_name, ordinal=ordinal)
			fn_ids_by_name.setdefault(symbol_name, []).append(fn_id)
			decls.append(
				_FrontendDecl(
					fn_id,
					symbol_name,
					fn.orig_name,
					fn.type_params,
					list(getattr(fn, "type_param_locs", []) or []),
					params,
					fn.return_type,
					getattr(fn, "loc", None),
					is_method=True,
					self_mode=self_mode,
					impl_target=impl.target,
					impl_type_params=impl_type_params,
					impl_type_param_locs=impl_type_param_locs,
					impl_owner=impl_owner,
					module=module_id,
				)
			)
			stmt_block = _convert_block(fn.body)
			# Enable implicit `self` member lookup for method bodies (spec §3.9).
			# Unknown identifiers may resolve to fields/methods on `self` after
			# locals and module-scope items are considered.
			#
			# We only need names here; semantic validation happens in the typed checker.
			# Collect receiver field names for implicit `self` member lookup.
			#
			# IMPORTANT: structs are module-scoped. We must resolve the impl target
			# in the current module context, not by bare name.
			field_names: set[str] = set()
			try:
				origin_mod = getattr(impl.target, "module_id", None) or module_name or "main"
				struct_id = type_table.get_struct_base(module_id=origin_mod, name=impl.target.name)
				if struct_id is not None:
					td = type_table.get(struct_id)
					if td.field_names is not None:
						field_names = set(td.field_names)
			except Exception:
				field_names = set()
			method_names: set[str] = {m.name for m in getattr(impl, "methods", []) or []}
			param_names = [p.name for p in getattr(fn, "params", []) or []]
			if fn.params and self_mode is not None:
				lowerer._push_implicit_self(
					self_name=str(getattr(fn.params[0], "name", "self")),
					self_mode=self_mode,
					field_names=field_names,
					method_names=method_names,
					module_function_names=module_function_names,
				)
				try:
					hir_block = lowerer.lower_function_block(stmt_block, param_names=param_names)
				finally:
					lowerer._pop_implicit_self()
			else:
				hir_block = lowerer.lower_function_block(stmt_block, param_names=param_names)
			func_hirs[fn_id] = hir_block
	# Build signatures with resolved TypeIds from parser decls.
	from lang2.driftc.type_resolver import resolve_program_signatures

	type_table, sigs = resolve_program_signatures(decls, table=type_table)
	signatures.update(sigs)
	# Resolve function require subjects (T -> TypeParamId) now that signatures exist.
	from lang2.driftc.traits.world import resolve_fn_require_subjects

	resolve_fn_require_subjects(world, signatures)
	# Thread exception schemas through the shared type table for downstream validators.
	#
	# In a multi-module build, this function may be called repeatedly with a
	# shared TypeTable; preserve previously registered schemas and extend them.
	prev_schemas = getattr(type_table, "exception_schemas", None)
	if not isinstance(prev_schemas, dict):
		prev_schemas = {}
	prev_schemas.update(exception_schemas)
	type_table.exception_schemas = prev_schemas
	return func_hirs, signatures, fn_ids_by_name, type_table, exception_catalog, diagnostics


def parse_drift_to_hir(path: Path) -> Tuple[Dict[FunctionId, H.HBlock], Dict[FunctionId, FnSignature], Dict[str, List[FunctionId]], "TypeTable", Dict[str, int], List[Diagnostic]]:
	"""
	Parse a Drift source file into lang2 HIR blocks + FnSignatures + TypeTable.

	Collects parser/adapter diagnostics (e.g., duplicate functions) instead of
	throwing, so callers can report them alongside later pipeline checks.
	"""
	source = path.read_text()
	try:
		prog = _parser.parse_program(source)
	except _parser.FStringParseError as err:
		return {}, {}, {}, TypeTable(), {}, [Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc))]
	except _parser.QualifiedMemberParseError as err:
		return {}, {}, {}, TypeTable(), {}, [Diagnostic(message=str(err), severity="error", span=_span_in_file(path, err.loc))]
	except UnexpectedInput as err:
		span = Span(
			file=str(path),
			line=getattr(err, "line", None),
			column=getattr(err, "column", None),
			raw=err,
		)
		return {}, {}, {}, TypeTable(), {}, [Diagnostic(message=str(err), severity="error", span=span)]
	return _lower_parsed_program_to_hir(prog, diagnostics=[])


__all__ = ["parse_drift_to_hir", "parse_drift_files_to_hir", "parse_drift_workspace_to_hir"]

[==== File: staged/lang2/driftc/parser/parser.py =====]
from __future__ import annotations

import ast
import codecs
from pathlib import Path
from typing import List, Optional

from lark import Lark, Token, Tree

from .ast import (
    ArrayLiteral,
    AssignStmt,
    AugAssignStmt,
    Attr,
    QualifiedMember,
    Binary,
    Block,
    Call,
    ConstDef,
    CatchClause,
    ExceptionArg,
    ExceptionDef,
    Expr,
    ExprStmt,
    ForStmt,
    FunctionDef,
    IfStmt,
    ImportStmt,
    ExportItem,
    ExportModuleStar,
    ExportName,
    ExportStmt,
    ImplementDef,
    Index,
    KwArg,
    LetStmt,
    Literal,
    Lambda,
    Located,
    Move,
    Name,
    Placeholder,
    Param,
    Program,
    RaiseStmt,
    RethrowStmt,
    ReturnStmt,
    StructDef,
    StructField,
    TraitDef,
    TraitMethodSig,
    RequireClause,
    TraitExpr,
    TraitIs,
    TraitAnd,
    TraitOr,
    TraitNot,
    TypeExpr,
    Ternary,
    TryCatchExpr,
    CatchExprArm,
    ExceptionCtor,
    TypeApp,
    TryStmt,
    WhileStmt,
    BreakStmt,
    ContinueStmt,
    Unary,
    FString,
    FStringHole,
    VariantDef,
    VariantArm,
    VariantField,
    MatchExpr,
    MatchArm,
)

_GRAMMAR_PATH = Path(__file__).with_name("grammar.lark")
_GRAMMAR_SRC = _GRAMMAR_PATH.read_text()


def _decode_string_token(tok: Token) -> str:
	"""
	Decode STRING tokens, including \\xHH hex byte escapes. We first interpret
	Python-style escapes (unicode_escape), then reinterpret the resulting code
	points as raw bytes (latin-1) and decode as UTF-8 to recover the intended
	byte sequence.
	"""
	content = tok.value[1:-1]  # strip quotes
	unescaped = codecs.decode(content, "unicode_escape")
	raw_bytes = unescaped.encode("latin-1")
	return raw_bytes.decode("utf-8")


def _decode_string_fragment(raw: str) -> str:
	"""
	Decode the contents of a string *fragment* using the same escape rules as STRING.

	This is used by the f-string parser to decode text parts that are split by holes.
	The input must be the raw source substring *inside* the quotes (i.e., it may
	contain backslash escapes like `\\n` or `\\xNN`).
	"""
	unescaped = codecs.decode(raw, "unicode_escape")
	raw_bytes = unescaped.encode("latin-1")
	return raw_bytes.decode("utf-8")


def _unwrap_ident(node: object) -> Token:
	"""
	Extract an identifier token from a grammar `ident` node.

	The grammar defines `ident: NAME | MOVE` so callers may receive:
	- a `Token` (`NAME` or `MOVE`) directly, or
	- a `Tree('ident', [Token(...)])` wrapper.
	"""
	if isinstance(node, Token):
		if node.type in {"NAME", "MOVE"}:
			return node
		raise TypeError(f"Expected identifier token, got {node.type}")
	if isinstance(node, Tree) and _name(node) == "ident":
		tok = next((c for c in node.children if isinstance(c, Token)), None)
		if tok is None:
			raise TypeError("ident node missing token child")
		if tok.type not in {"NAME", "MOVE"}:
			raise TypeError(f"Expected NAME/MOVE token in ident, got {tok.type}")
		return tok
	raise TypeError(f"Expected ident node, got {type(node)}")


_EXPR_PARSER = Lark(
	_GRAMMAR_SRC,
	parser="lalr",
	lexer="basic",
	start="expr",
	propagate_positions=True,
	maybe_placeholders=False,
)


def _parse_expr_fragment(source: str) -> Expr:
	"""
	Parse a Drift expression from a source fragment.

	This helper is used for f-string holes. The grammar is shared with the main
	parser, but the start rule is `expr` and no newline-terminator insertion is
	performed (holes never contain newlines because string literals don't).
	"""
	tree = _EXPR_PARSER.parse(source)
	return _build_expr(tree)


class FStringParseError(ValueError):
	"""
	Error raised while parsing an f-string literal.

	This is intentionally a `ValueError` subclass so existing parser plumbing can
continue to treat it as a parse-time failure, but it carries a best-effort
location (`loc`) so callers can convert it into a structured diagnostic instead
of crashing the compiler.
	"""

	def __init__(self, message: str, *, loc: "Located") -> None:
		super().__init__(message)
		self.loc = loc


def _parse_fstring(loc: Located, raw_string_token: Token) -> FString:
	"""
	Parse a raw STRING token (including braces) into an f-string AST.

	The main grammar tokenizes the interior of the f-string as a normal STRING token
	so we can reuse the existing string escape rules. This function is responsible
	for:
	- splitting text vs `{...}` holes,
	- supporting brace escaping via `{{` / `}}`,
	- extracting the hole expression and optional `:spec` substring, and
	- producing accurate-ish hole locations (line/column within the string).

	MVP limitations:
	- spec strings are opaque and must not contain `{` or `}`.
	- errors are reported as `FStringParseError` and converted into diagnostics.
	"""
	raw = raw_string_token.value[1:-1]  # strip quotes, keep escapes

	parts: list[str] = []
	holes: list[FStringHole] = []
	text_buf: list[str] = []

	base_line = getattr(raw_string_token, "line", loc.line)
	base_col = getattr(raw_string_token, "column", loc.column)

	def _flush_text() -> None:
		fragment_raw = "".join(text_buf)
		text_buf.clear()
		parts.append(_decode_string_fragment(fragment_raw))

	def _hole_loc(offset: int) -> Located:
		return Located(line=base_line, column=base_col + offset)

	def _unescape_hole_source(src: str) -> str:
		out: list[str] = []
		j = 0
		while j < len(src):
			c = src[j]
			if c == "\\" and j + 1 < len(src):
				nxt = src[j + 1]
				if nxt in ("\\", "\""):
					out.append(nxt)
					j += 2
					continue
			out.append(c)
			j += 1
		return "".join(out)

	i = 0
	while i < len(raw):
		ch = raw[i]
		if raw.startswith("{{", i):
			text_buf.append("{")
			i += 2
			continue
		if raw.startswith("}}", i):
			text_buf.append("}")
			i += 2
			continue
		if ch == "}":
			raise FStringParseError("E-FSTR-UNBALANCED-BRACE: unescaped '}' in f-string", loc=_hole_loc(i))
		if ch != "{":
			text_buf.append(ch)
			i += 1
			continue

		hole_start = i
		_flush_text()
		i += 1  # consume '{'
		if i < len(raw) and raw[i] == "}":
			raise FStringParseError("E-FSTR-EMPTY-HOLE: '{}' is not a valid f-string hole", loc=_hole_loc(hole_start))

		paren_depth = 0
		bracket_depth = 0
		brace_depth = 0
		in_string = False
		expr_buf: list[str] = []
		spec_buf: list[str] | None = None

		while i < len(raw):
			c = raw[i]
			if in_string:
				expr_buf.append(c)
				if c == "\\":
					i += 1
					if i < len(raw):
						expr_buf.append(raw[i])
					i += 1
					continue
				if c == "\"":
					in_string = False
				i += 1
				continue

			if (
				spec_buf is None
				and c == ":"
				and paren_depth == 0
				and bracket_depth == 0
				and brace_depth == 0
			):
				candidate_expr = "".join(expr_buf).strip()
				try:
					_parse_expr_fragment(candidate_expr)
				except Exception:
					expr_buf.append(c)
					i += 1
					continue
				spec_buf = []
				i += 1
				continue

			if c == "\"":
				in_string = True
				expr_buf.append(c)
				i += 1
				continue

			if c == "(":
				paren_depth += 1
			elif c == ")" and paren_depth:
				paren_depth -= 1
			elif c == "[":
				bracket_depth += 1
			elif c == "]" and bracket_depth:
				bracket_depth -= 1
			elif c == "{":
				brace_depth += 1
			elif c == "}":
				if brace_depth:
					brace_depth -= 1
				elif paren_depth == 0 and bracket_depth == 0:
					i += 1
					break
				else:
					raise FStringParseError("E-FSTR-UNBALANCED-BRACE: '}' in f-string hole", loc=_hole_loc(i))

			if spec_buf is None:
				expr_buf.append(c)
			else:
				if c in "{}":
					raise FStringParseError("E-FSTR-NESTED: nested braces are not allowed in :spec (MVP)", loc=_hole_loc(i))
				spec_buf.append(c)
			i += 1
		else:
			raise FStringParseError("E-FSTR-UNBALANCED-BRACE: unterminated '{' in f-string", loc=_hole_loc(hole_start))

		expr_src = _unescape_hole_source("".join(expr_buf).strip())
		if not expr_src:
			raise FStringParseError("E-FSTR-EMPTY-HOLE: hole must contain an expression", loc=_hole_loc(hole_start))

		expr_ast = _parse_expr_fragment(expr_src)
		spec = "".join(spec_buf).strip() if spec_buf is not None else ""
		holes.append(FStringHole(loc=_hole_loc(hole_start), expr=expr_ast, spec=spec))

	_flush_text()
	if len(parts) != len(holes) + 1:
		raise AssertionError("f-string parser bug: parts/holes shape mismatch")
	return FString(loc=loc, parts=parts, holes=holes)


class TerminatorInserter:
    always_accept = ("NEWLINE", "SEMI")

    TERMINABLE = {
        "NAME",
        "SIGNED_INT",
        # Float literals in lang2 MVP use the `FLOAT` token (dot required).
        "FLOAT",
        # Legacy/compat token name; keep for safety if the grammar changes.
        "SIGNED_FLOAT",
        "STRING",
        "TRUE",
        "FALSE",
        "RPAR",
        "RSQB",
        "RBRACE",
        "RETURN",
        "THROW",
        "RETHROW",
        "BREAK",
        "CONTINUE",
    }

    SUPPRESS = {
        "DOT",
        "ARROW",
        "PLUS",
        "PLUS_EQ",
        "MINUS",
        "MINUS_EQ",
        "STAR_EQ",
        "SLASH_EQ",
        "PERCENT_EQ",
        "AMP_EQ",
        "BAR_EQ",
        "CARET_EQ",
        "LSHIFT_EQ",
        "SHR_EQ",
        "STAR",
        "SLASH",
        "PERCENT",
        "BAR",
        "CARET",
        "TILDE",
        "LSHIFT",
        "SHR",
        "PIPE_FWD",
        "PIPE_REV",
        "AND",
        "OR",
        "EQEQ",
        "NOTEQ",
        "LTE",
        "GTE",
        "LT",
        "GT",
        "COLON",
        "COMMA",
        "EQUAL",
        "IF",
        "ELSE",
    }

    def __init__(self) -> None:
        self._reset()

    def _reset(self) -> None:
        self.paren_depth = 0
        self.bracket_depth = 0
        self.can_terminate = False

    def process(self, stream):
        """
        Insert `TERMINATOR` tokens for statement boundaries.

        We treat explicit `;` as an unconditional terminator, and we treat
        newline as a terminator *only* when the previous token can terminate a
        statement and we're not inside parentheses/brackets.

        One additional rule keeps `try ... catch ...` readable across lines:

            try foo()
                catch { ... }

        The newline between the attempt and the `catch` keyword must not create
        a statement terminator. To support that without complicating the
        grammar, we suppress newline-terminator insertion when the next
        significant token is `CATCH`.
        """

        self._reset()
        pending_newline: Token | None = None

        for token in stream:
            ttype = token.type

            if ttype == "NEWLINE":
                # Defer the decision until we see the next non-newline token so
                # we can suppress terminators before `catch`.
                pending_newline = token
                continue

            if ttype == "SEMI":
                # An explicit semicolon always terminates the current statement.
                pending_newline = None
                yield Token.new_borrow_pos("TERMINATOR", token.value, token)
                self.can_terminate = False
                continue

            if pending_newline is not None:
                if self._should_emit_terminator() and ttype != "CATCH":
                    yield Token.new_borrow_pos("TERMINATOR", pending_newline.value, pending_newline)
                    self.can_terminate = False
                pending_newline = None

            yield token
            self._update_depth(ttype)
            self.can_terminate = self._is_terminable(ttype)

        if pending_newline is not None:
            if self._should_emit_terminator():
                yield Token.new_borrow_pos("TERMINATOR", pending_newline.value, pending_newline)
                self.can_terminate = False

    def _update_depth(self, ttype: str) -> None:
        if ttype == "LPAR":
            self.paren_depth += 1
        elif ttype == "RPAR" and self.paren_depth:
            self.paren_depth -= 1
        elif ttype == "LSQB":
            self.bracket_depth += 1
        elif ttype == "RSQB" and self.bracket_depth:
            self.bracket_depth -= 1

    def _is_terminable(self, ttype: str) -> bool:
        if ttype in self.SUPPRESS:
            return False
        return ttype in self.TERMINABLE

    def _should_emit_terminator(self) -> bool:
        return self.paren_depth == 0 and self.bracket_depth == 0 and self.can_terminate


class QualifiedTypeArgInserter:
    """
    Disambiguate `<...>` as type arguments in expression position for qualified members.

    We want to support:
      - `TypeName<T>::Ctor(...)`
      - `TypeName::Ctor<type T>(...)`

    …without making `<` ambiguous with the `<` comparison operator (e.g. `i < 3`).

    This post-lexer rewrites generic-angle spans for two deterministic cases:
      - split `>>` inside `<type ...>` call spans
      - pre-`::` type args: `Type<T>::Ctor` -> `QUAL_TYPE_LT/QUAL_TYPE_GT`
    """

    def process(self, stream):
        recent: list[Token] = []
        call_angle_depth = 0
        type_mode = False
        type_mode_next = False
        type_angle_depth = 0
        type_square_depth = 0
        impl_header = False
        impl_typeparam_depth = 0
        struct_header = False
        fn_header = False
        allowed_in_type_args = {
            # Type refs.
            "NAME",
            "DOT",
            "DCOLON",
            # Type arg lists.
            "COMMA",
            "LT",
            "GT",
            "SHR",
            # Ref types.
            "AMP",
            "MUT",
            # Square type args (e.g., Array[T], if enabled).
            "LSQB",
            "RSQB",
        }
        type_mode_start = {"COLON", "RETURNS", "FOR"}
        type_mode_end = {
            "COMMA",
            "LPAR",
            "RPAR",
            "RBRACE",
            "LBRACE",
            "EQUAL",
            "SEMI",
            "NEWLINE",
            "BAR",
            "FOR",
            "REQUIRE",
        }

        def _emit(tok: Token):
            recent.append(tok)
            if len(recent) > 4:
                recent.pop(0)
            return tok

        def _enter_type_mode() -> None:
            nonlocal type_mode, type_mode_next, type_angle_depth, type_square_depth
            if type_mode:
                return
            type_mode = True
            type_mode_next = False
            type_angle_depth = 0
            type_square_depth = 0

        def _is_pre_base_context(_: Token) -> bool:
            # ... NAME < ...
            return len(recent) >= 1 and recent[-1].type == "NAME"

        def _is_type_arg_span(tokens: list[Token]) -> bool:
            # Only accept spans that look like a type-argument list by token shape.
            # This intentionally rejects obvious expression operators.
            allowed = {
                "NAME",
                "DOT",
                "DCOLON",
                "COMMA",
                "LT",
                "GT",
                "SHR",
                "AMP",
                "MUT",
                "LSQB",
                "RSQB",
            }
            for tok in tokens[1:-1]:
                if tok.type not in allowed:
                    return False
            return True

        def _emit_pre_type_args(first_lt: Token) -> bool:
            # Buffer the <...> span and rewrite to QUAL_TYPE_LT/GT if followed by ::.
            buf: list[Token] = [first_lt]
            depth = 1
            while True:
                try:
                    tok = pushback.pop() if pushback else next(it)
                except StopIteration:
                    for t in buf:
                        yield _emit(t)
                    return True

                if tok.type not in allowed_in_type_args:
                    for t in buf:
                        yield _emit(t)
                    pushback.append(tok)
                    return True

                buf.append(tok)
                if tok.type == "LT":
                    depth += 1
                    continue
                if tok.type == "GT":
                    depth -= 1
                elif tok.type == "SHR":
                    depth -= 2

                if depth > 0:
                    continue
                if depth < 0:
                    for t in buf:
                        yield _emit(t)
                    return True

                try:
                    next_tok = pushback.pop() if pushback else next(it)
                except StopIteration:
                    for t in buf:
                        yield _emit(t)
                    return True

                commit = next_tok.type == "DCOLON" and _is_type_arg_span(buf)
                if commit:
                    depth = 0
                    for t in buf:
                        if t.type == "LT":
                            if depth == 0:
                                yield _emit(Token.new_borrow_pos("QUAL_TYPE_LT", t.value, t))
                            else:
                                yield _emit(t)
                            depth += 1
                            continue
                        if t.type == "GT":
                            if depth == 1:
                                yield _emit(Token.new_borrow_pos("QUAL_TYPE_GT", t.value, t))
                            else:
                                yield _emit(t)
                            depth -= 1
                            continue
                        if t.type == "SHR":
                            for _ in range(2):
                                if depth == 1:
                                    yield _emit(Token.new_borrow_pos("QUAL_TYPE_GT", ">", t))
                                else:
                                    yield _emit(Token.new_borrow_pos("GT", ">", t))
                                depth -= 1
                            continue
                        yield _emit(t)
                else:
                    for t in buf:
                        yield _emit(t)
                pushback.append(next_tok)
                return True

        it = iter(stream)
        pushback: list[Token] = []

        while True:
            try:
                token = pushback.pop() if pushback else next(it)
            except StopIteration:
                break

            if type_mode_next:
                _enter_type_mode()

            tt = token.type
            if call_angle_depth:
                if tt in {"LT", "TYPE_LT", "QUAL_TYPE_LT"}:
                    call_angle_depth += 1
                    yield _emit(token)
                    continue
                if tt in {"GT", "TYPE_GT", "QUAL_TYPE_GT"}:
                    call_angle_depth -= 1
                    yield _emit(token)
                    if call_angle_depth <= 0:
                        call_angle_depth = 0
                    continue
                if tt == "SHR":
                    for _ in range(2):
                        yield _emit(Token.new_borrow_pos("GT", ">", token))
                        call_angle_depth -= 1
                        if call_angle_depth <= 0:
                            call_angle_depth = 0
                            break
                    continue
                yield _emit(token)
                continue
            if impl_header:
                if tt in {"LT", "TYPE_LT"}:
                    impl_typeparam_depth += 1
                elif tt in {"GT", "TYPE_GT"} and impl_typeparam_depth:
                    impl_typeparam_depth -= 1
                elif tt == "SHR" and impl_typeparam_depth >= 2:
                    for _ in range(2):
                        impl_typeparam_depth -= 1
                        yield _emit(Token.new_borrow_pos("GT", ">", token))
                    continue
                if impl_typeparam_depth == 0 and tt == "NAME":
                    impl_header = False
                    _enter_type_mode()
                yield _emit(token)
                continue

            if struct_header and tt == "NAME":
                struct_header = False
                _enter_type_mode()
                yield _emit(token)
                continue

            if fn_header and tt == "NAME":
                fn_header = False
                _enter_type_mode()
                yield _emit(token)
                continue

            if tt == "IMPLEMENT":
                impl_header = True
                yield _emit(token)
                continue

            if tt == "CALL_TYPE_LT":
                yield _emit(token)
                call_angle_depth = 1
                continue

            if tt == "STRUCT":
                struct_header = True
                yield _emit(token)
                continue

            if tt == "FN":
                fn_header = True
                yield _emit(token)
                continue

            if type_mode:
                if tt in {"LT", "TYPE_LT", "QUAL_TYPE_LT"}:
                    type_angle_depth += 1
                    yield _emit(token)
                    continue
                if tt in {"GT", "TYPE_GT", "QUAL_TYPE_GT"}:
                    if type_angle_depth > 0:
                        type_angle_depth -= 1
                    yield _emit(token)
                    if type_angle_depth == 0 and type_square_depth == 0 and tt in type_mode_end:
                        type_mode = False
                    continue
                if tt == "LSQB":
                    type_square_depth += 1
                    yield _emit(token)
                    continue
                if tt == "RSQB":
                    if type_square_depth > 0:
                        type_square_depth -= 1
                    yield _emit(token)
                    continue
                if tt == "SHR" and type_angle_depth >= 2:
                    for _ in range(2):
                        type_angle_depth -= 1
                        yield _emit(Token.new_borrow_pos("GT", ">", token))
                    continue
                yield _emit(token)
                if type_angle_depth == 0 and type_square_depth == 0 and tt in type_mode_end:
                    type_mode = False
                    if tt in type_mode_start:
                        type_mode_next = True
                continue

            if tt == "LT":
                is_pre = _is_pre_base_context(token)
                if is_pre:
                    yield from _emit_pre_type_args(token)
                    continue
            yield _emit(token)
            if tt in type_mode_start:
                type_mode_next = True
            continue


class DriftPostLex:
	"""Combined post-lexer: type-arg disambiguation, then terminator insertion."""

	# Lark may drop tokens that aren't referenced in the grammar unless the
	# post-lexer asks to keep them. We need newlines/semicolons for terminator
	# insertion.
	always_accept = TerminatorInserter.always_accept

	def __init__(self) -> None:
		self._type_args = QualifiedTypeArgInserter()
		self._terminators = TerminatorInserter()

	def process(self, stream):
		return self._terminators.process(self._type_args.process(stream))



_PARSER = Lark(
    _GRAMMAR_SRC,
    parser="lalr",
    lexer="basic",
    start="program",
    propagate_positions=True,
    maybe_placeholders=False,
    postlex=DriftPostLex(),
)


def parse_program(source: str) -> Program:
    tree = _PARSER.parse(source)
    return _build_program(tree)


class ModuleDeclError(ValueError):
	"""
	User-facing error for invalid `module ...` declarations.

	The driver converts this into a pinned parser-phase diagnostic (it is not an
	internal compiler bug).
	"""

	def __init__(self, message: str, *, loc: object | None) -> None:
		super().__init__(message)
		self.loc = loc


class QualifiedMemberParseError(ValueError):
	"""
	User-facing parse error for invalid `TypeRef::member` syntax.

	This is raised from the parser AST builder (not from the grammar) so the
	driver can report a pinned parser diagnostic instead of crashing with a raw
	Python exception.
	"""

	def __init__(self, message: str, *, loc: object | None) -> None:
		super().__init__(message)
		self.loc = loc


def _build_program(tree: Tree) -> Program:
	functions: List[FunctionDef] = []
	consts: List["ConstDef"] = []
	implements: List[ImplementDef] = []
	traits: List[TraitDef] = []
	imports: List[ImportStmt] = []
	exports: List[ExportStmt] = []
	statements: List[ExprStmt | LetStmt | ReturnStmt | RaiseStmt] = []
	structs: List[StructDef] = []
	exceptions: List[ExceptionDef] = []
	variants: List[VariantDef] = []
	module_name: Optional[str] = None
	module_loc: Optional[Located] = None
	seen_non_module_item = False
	for child in tree.children:
		if not isinstance(child, Tree):
			continue
		kind = _name(child)
		if kind == "module_decl":
			if seen_non_module_item:
				raise ModuleDeclError(
					"`module ...` must be the first top-level declaration in the file",
					loc=_loc(child),
				)
			if module_name is not None:
				raise ModuleDeclError(
					"duplicate `module ...` declaration in the same file",
					loc=_loc(child),
				)
			module_name = _build_module_decl(child)
			module_loc = _loc(child)
			continue
		seen_non_module_item = True
		if kind == "pub_item":
			pub_child = next((c for c in child.children if isinstance(c, Tree)), None)
			if pub_child is None:
				continue
			kind = _name(pub_child)
			child = pub_child
			is_pub = True
		else:
			is_pub = False
		if kind == "func_def":
			fn = _build_function(child)
			fn.is_pub = is_pub
			functions.append(fn)
		elif kind == "const_def":
			const_def = _build_const_def(child)
			const_def.is_pub = is_pub
			consts.append(const_def)
		elif kind == "implement_def":
			impl = _build_implement_def(child)
			impl.is_pub = is_pub
			implements.append(impl)
		elif kind == "struct_def":
			struct_def = _build_struct_def(child)
			struct_def.is_pub = is_pub
			structs.append(struct_def)
		elif kind == "exception_def":
			exc = _build_exception_def(child)
			exc.is_pub = is_pub
			exceptions.append(exc)
		elif kind == "variant_def":
			var_def = _build_variant_def(child)
			var_def.is_pub = is_pub
			variants.append(var_def)
		elif kind == "trait_def":
			tr_def = _build_trait_def(child)
			tr_def.is_pub = is_pub
			traits.append(tr_def)
		else:
			stmt = _build_stmt(child)
			if stmt is None:
				continue
			if isinstance(stmt, ImportStmt):
				imports.append(stmt)
			elif isinstance(stmt, ExportStmt):
				exports.append(stmt)
			else:
				statements.append(stmt)
	return Program(
		functions=functions,
		consts=consts,
		implements=implements,
		traits=traits,
		imports=imports,
		exports=exports,
		statements=statements,
		structs=structs,
		exceptions=exceptions,
		variants=variants,
		module=module_name,
		module_loc=module_loc,
	)


def _build_const_def(tree: Tree) -> "ConstDef":
	"""
	Build a top-level constant definition.

	Grammar:
	  const_def: CONST NAME COLON type_expr EQUAL expr TERMINATOR
	"""
	loc = _loc(tree)
	name_tok = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
	type_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) == "type_expr")
	# `expr` is a rule alias (`?expr`), so the parse tree usually contains the
	# concrete expression rule node directly (e.g., `postfix`, `sum`, ...), not an
	# intermediate `expr` node.
	expr_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) != "type_expr")
	return ConstDef(
		loc=loc,
		name=name_tok.value,
		type_expr=_build_type_expr(type_node),
		value=_build_expr(expr_node),
	)


def _build_module_decl(tree: Tree) -> str:
    from lark import Token as LarkToken

    path_parts = [tok.value for tok in tree.scan_values(lambda v: isinstance(v, LarkToken) and v.type == "NAME")]
    return ".".join(path_parts) if path_parts else "main"


def _build_exception_def(tree: Tree) -> ExceptionDef:
    loc = _loc(tree)
    name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
    args: List[ExceptionArg] = []
    params_node = next(
        (child for child in tree.children if isinstance(child, Tree) and _name(child) == "exception_params"),
        None,
    )
    domain_node = next(
        (child for child in tree.children if isinstance(child, Tree) and _name(child) == "exception_domain_param"),
        None,
    )
    domain_val = None
    if domain_node:
        str_node = next((c for c in domain_node.children if isinstance(c, Token) and c.type == "STRING"), None)
        if str_node:
            domain_val = _decode_string_token(str_node)
    if params_node:
        args = []
        for child in params_node.children:
            if isinstance(child, Tree) and _name(child) == "exception_param":
                args.append(_build_exception_arg(child))
            if isinstance(child, Tree) and _name(child) == "exception_domain_param":
                str_node = next((c for c in child.children if isinstance(c, Token) and c.type == "STRING"), None)
                if str_node:
                    domain_val = _decode_string_token(str_node)
    return ExceptionDef(name=name_token.value, args=args, loc=loc, domain=domain_val)


def _build_variant_def(tree: Tree) -> VariantDef:
	"""
	Build a variant definition.

	Grammar:
	  variant_def: VARIANT NAME type_params? variant_body
	"""
	loc = _loc(tree)
	name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
	type_params_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "type_params"), None)
	type_params: list[str] = []
	if type_params_node is not None:
		type_params = [tok.value for tok in type_params_node.children if isinstance(tok, Token) and tok.type == "NAME"]
	body_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) == "variant_body")
	arms: list[VariantArm] = []
	for arm_node in (c for c in body_node.children if isinstance(c, Tree) and _name(c) == "variant_arm"):
		arm_name_token = next(child for child in arm_node.children if isinstance(child, Token) and child.type == "NAME")
		fields_node = next((c for c in arm_node.children if isinstance(c, Tree) and _name(c) == "variant_fields"), None)
		fields: list[VariantField] = []
		if fields_node is not None:
			field_list = next((c for c in fields_node.children if isinstance(c, Tree) and _name(c) == "variant_field_list"), None)
			if field_list is not None:
				for field_node in (c for c in field_list.children if isinstance(c, Tree) and _name(c) == "variant_field"):
					fname_tok = next(child for child in field_node.children if isinstance(child, Token) and child.type == "NAME")
					ftype_node = next(child for child in field_node.children if isinstance(child, Tree) and _name(child) == "type_expr")
					fields.append(VariantField(name=fname_tok.value, type_expr=_build_type_expr(ftype_node)))
		arms.append(VariantArm(name=arm_name_token.value, fields=fields, loc=_loc(arm_node)))
	return VariantDef(name=name_token.value, type_params=type_params, arms=arms, loc=loc)


def _build_exception_arg(tree: Tree) -> ExceptionArg:
    if _name(tree) != "exception_param":
        raise ValueError("expected exception_param")
    name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
    type_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) == "type_expr")
    return ExceptionArg(name=name_token.value, type_expr=_build_type_expr(type_node))


def _build_trait_expr(node: Tree) -> TraitExpr:
	name = _name(node)
	if name == "trait_expr":
		child = next((c for c in node.children if isinstance(c, Tree)), None)
		if child is None:
			raise ValueError("trait_expr missing child")
		return _build_trait_expr(child)
	if name in {"trait_or", "trait_and"}:
		op_name = "or" if name == "trait_or" else "and"
		current: TraitExpr | None = None
		pending_op: str | None = None
		for child in node.children:
			if isinstance(child, Token) and child.type in {"OR", "AND"}:
				pending_op = child.value
				continue
			if not isinstance(child, Tree):
				continue
			rhs = _build_trait_expr(child)
			if current is None:
				current = rhs
				continue
			if pending_op is None:
				raise ValueError("trait boolean chain missing operator")
			if pending_op == "and":
				current = TraitAnd(loc=_loc(node), left=current, right=rhs)
			elif pending_op == "or":
				current = TraitOr(loc=_loc(node), left=current, right=rhs)
			else:
				raise ValueError(f"unexpected trait boolean op {pending_op}")
			pending_op = None
		if current is None:
			raise ValueError("trait boolean chain missing operands")
		return current
	if name == "trait_not":
		child = next((c for c in node.children if isinstance(c, Tree)), None)
		if child is None:
			raise ValueError("trait_not missing operand")
		inner = _build_trait_expr(child)
		if _name(child) == "trait_not" or any(isinstance(c, Token) and c.value == "not" for c in node.children):
			return TraitNot(loc=_loc(node), expr=inner)
		return inner
	if name == "trait_atom":
		child_expr = next(
			(
				c
				for c in node.children
				if isinstance(c, Tree) and _name(c) in {"trait_expr", "trait_or", "trait_and", "trait_not", "trait_atom"}
			),
			None,
		)
		if child_expr is not None:
			return _build_trait_expr(child_expr)
		subject_tok = next((c for c in node.children if isinstance(c, Token) and c.type == "NAME"), None)
		if subject_tok is None:
			subject_node = next((c for c in node.children if isinstance(c, Tree) and _name(c) == "trait_subject"), None)
			if subject_node is not None:
				subject_tok = next(
					(c for c in subject_node.children if isinstance(c, Token) and c.type == "NAME"),
					None,
				)
		trait_node = next(
			(
				c
				for c in node.children
				if isinstance(c, Tree) and _name(c) in {"trait_name", "base_type", "qualified_base_type"}
			),
			None,
		)
		if subject_tok is None or trait_node is None:
			raise ValueError("trait atom missing subject or trait name")
		if _name(trait_node) == "trait_name":
			trait_child = next((c for c in trait_node.children if isinstance(c, Tree)), None)
			if trait_child is None:
				raise ValueError("trait name missing base type")
			trait_node = trait_child
		return TraitIs(loc=_loc(node), subject=subject_tok.value, trait=_build_type_expr(trait_node))
	raise ValueError(f"unsupported trait expr node: {name}")


def _build_require_clause(tree: Tree) -> RequireClause:
	exprs: list[TraitExpr] = []
	for child in tree.children:
		if isinstance(child, Tree) and _name(child).startswith("trait_"):
			exprs.append(_build_trait_expr(child))
	if not exprs:
		raise ValueError("require clause missing trait expressions")
	combined = exprs[0]
	for expr in exprs[1:]:
		combined = TraitAnd(loc=_loc(tree), left=combined, right=expr)
	return RequireClause(expr=combined, loc=_loc(tree))


def _build_trait_method_sig(tree: Tree) -> TraitMethodSig:
	loc = _loc(tree)
	children = list(tree.children)
	idx = 0
	name_token = _unwrap_ident(children[idx])
	idx += 1
	params: List[Param] = []
	if idx < len(children) and _name(children[idx]) == "params":
		params = [_build_param(p) for p in children[idx].children if isinstance(p, Tree)]
		idx += 1
	return_sig = children[idx]
	type_child = next(child for child in return_sig.children if isinstance(child, Tree))
	return_type = _build_type_expr(type_child)
	return TraitMethodSig(name=name_token.value, params=params, return_type=return_type, loc=loc)


def _build_trait_def(tree: Tree) -> TraitDef:
	loc = _loc(tree)
	name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
	require_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "require_clause"), None)
	body_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "trait_body"), None)
	methods: list[TraitMethodSig] = []
	if body_node is not None:
		for item in body_node.children:
			if not isinstance(item, Tree):
				continue
			if _name(item) == "trait_item":
				sig_node = next((c for c in item.children if isinstance(c, Tree) and _name(c) == "trait_method_sig"), None)
				if sig_node is not None:
					methods.append(_build_trait_method_sig(sig_node))
			elif _name(item) == "trait_method_sig":
				methods.append(_build_trait_method_sig(item))
	require = _build_require_clause(require_node) if require_node is not None else None
	return TraitDef(name=name_token.value, methods=methods, require=require, loc=loc)


def _build_function(tree: Tree) -> FunctionDef:
	loc = _loc(tree)
	children = list(tree.children)
	idx = 0
	name_token = _unwrap_ident(children[idx])
	idx += 1
	orig_name = name_token.value
	type_params: list[str] = []
	type_param_locs: list[Located] = []
	if idx < len(children) and _name(children[idx]) == "type_params":
		for tok in children[idx].children:
			if isinstance(tok, Token) and tok.type == "NAME":
				type_params.append(tok.value)
				type_param_locs.append(_loc_from_token(tok))
		idx += 1
	params: List[Param] = []
	if idx < len(children) and _name(children[idx]) == "params":
		params = [_build_param(p) for p in children[idx].children if isinstance(p, Tree)]
		idx += 1
	return_sig = children[idx]
	type_child = next(child for child in return_sig.children if isinstance(child, Tree))
	return_type = _build_type_expr(type_child)
	idx += 1
	require = None
	if idx < len(children) and isinstance(children[idx], Tree) and _name(children[idx]) == "require_clause":
		require = _build_require_clause(children[idx])
		idx += 1
	body = _build_block(children[idx])
	return FunctionDef(
		name=name_token.value,
		orig_name=orig_name,
		type_params=type_params,
		type_param_locs=type_param_locs,
		params=params,
		return_type=return_type,
		body=body,
		loc=loc,
		require=require,
	)


def _build_implement_def(tree: Tree) -> ImplementDef:
	loc = _loc(tree)
	type_params_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "type_params"), None)
	type_params: list[str] = []
	type_param_locs: list[Located] = []
	if type_params_node is not None:
		for tok in type_params_node.children:
			if isinstance(tok, Token) and tok.type == "NAME":
				type_params.append(tok.value)
				type_param_locs.append(_loc_from_token(tok))
	type_nodes = [child for child in tree.children if isinstance(child, Tree) and _name(child) == "type_expr"]
	if not type_nodes:
		raise ValueError("implement missing target type")
	trait = None
	if len(type_nodes) >= 2:
		trait = _build_type_expr(type_nodes[0])
		target = _build_type_expr(type_nodes[1])
	else:
		target = _build_type_expr(type_nodes[0])
	require_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "require_clause"), None)
	require = _build_require_clause(require_node) if require_node is not None else None
	methods: List[FunctionDef] = []
	body_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) == "implement_body")
	for item in body_node.children:
		# Grammar: implement_item: func_def -> implement_func
		#
		# So we accept either:
		# - `implement_func(func_def)` (preferred), or
		# - legacy/alternate shapes that may appear during grammar evolution.
		if not isinstance(item, Tree):
			continue
		item_kind = _name(item)
		if item_kind not in {"implement_func", "implement_item", "func_def"}:
			continue
		fn_node: Tree | None = None
		if item_kind == "func_def":
			fn_node = item
		else:
			fn_node = next((c for c in item.children if isinstance(c, Tree) and _name(c) == "func_def"), None)
		if fn_node is None:
			continue
		fn = _build_function(fn_node)
		fn.is_method = True
		fn.impl_target = target
		methods.append(fn)
	return ImplementDef(
		target=target,
		loc=loc,
		type_params=type_params,
		type_param_locs=type_param_locs,
		trait=trait,
		require=require,
		methods=methods,
	)


def _build_block(tree: Tree) -> Block:
    statements: List[ExprStmt | LetStmt | ReturnStmt | RaiseStmt] = []
    for child in tree.children:
        if not isinstance(child, Tree):
            continue
        if _name(child) == "stmt":
            stmt = _build_stmt(child)
            if stmt is not None:
                statements.append(stmt)
    return Block(statements=statements)


def _build_value_block(tree: Tree) -> Block:
    """
    Build a "value block": a braced block that ends with a trailing expression
    that does not require a terminator (though one may be present).

    Grammar:
        value_block: "{" (stmt | TERMINATOR)* expr terminator_opt "}"

    We represent the trailing expression as a final `ExprStmt` in the block.
    Downstream stages (AstToHIR / checker) are responsible for enforcing any
    "yields a value" rules (e.g. try/catch expression catch arms).
    """

    statements: List[ExprStmt | LetStmt | ReturnStmt | RaiseStmt] = []
    result_expr_node: Tree | None = None

    for child in tree.children:
        if not isinstance(child, Tree):
            continue
        name = _name(child)
        if name == "stmt":
            stmt = _build_stmt(child)
            if stmt is not None:
                statements.append(stmt)
            continue
        if name == "terminator_opt":
            continue
        result_expr_node = child

    if result_expr_node is None:
        raise ValueError("value_block missing trailing expression")

    result_expr = _build_expr(result_expr_node)
    statements.append(ExprStmt(loc=result_expr.loc, value=result_expr))
    return Block(statements=statements)


def _build_param(tree: Tree) -> Param:
	non_escaping = any(isinstance(child, Token) and child.type == "NONESCAPING" for child in tree.children)
	ident_node = next(
		child
		for child in tree.children
		if (isinstance(child, Token) and child.type in {"NAME", "MOVE"})
		or (isinstance(child, Tree) and _name(child) == "ident")
	)
	name_token = _unwrap_ident(ident_node)
	type_node = next((child for child in tree.children if isinstance(child, Tree) and _name(child) == "type_expr"), None)
	type_expr = _build_type_expr(type_node) if type_node is not None else None
	return Param(name=name_token.value, type_expr=type_expr, non_escaping=non_escaping)


def _build_lambda(tree: Tree) -> Lambda:
	params: list[Param] = []
	body_expr: Expr | None = None
	body_block: Block | None = None
	ret_type: TypeExpr | None = None
	for child in tree.children:
		if isinstance(child, Tree) and _name(child) == "lambda_params":
			for param_node in child.children:
				if isinstance(param_node, Tree) and _name(param_node) == "lambda_param":
					name_tok = next(tok for tok in param_node.children if isinstance(tok, Token) and tok.type == "NAME")
					type_node = next(
						(c for c in param_node.children if isinstance(c, Tree) and _name(c) == "type_expr"), None
					)
					params.append(
						Param(
							name=name_tok.value,
							type_expr=_build_type_expr(type_node) if type_node is not None else None,
						)
					)
		elif isinstance(child, Tree) and _name(child) == "lambda_returns":
			type_node = next((c for c in child.children if isinstance(c, Tree) and _name(c) == "type_expr"), None)
			ret_type = _build_type_expr(type_node) if type_node is not None else None
		elif isinstance(child, Tree):
			# lambda_body or direct expr/block if the parser simplified.
			target = child
			if _name(child) == "lambda_body" and child.children:
				target = next(c for c in child.children if isinstance(c, Tree))
			if _name(target) == "block":
				body_block = _build_block(target)
			elif _name(target) == "value_block":
				body_block = _build_value_block(target)
			else:
				body_expr = _build_expr(target)
	return Lambda(loc=_loc(tree), params=params, ret_type=ret_type, body_expr=body_expr, body_block=body_block)


def _build_type_expr(tree: Tree) -> TypeExpr:
	name = _name(tree)
	if name == "ref_type":
		# '&' ['mut'] type_expr
		inner = _build_type_expr(next(child for child in tree.children if isinstance(child, Tree)))
		mut = any(isinstance(child, Token) and child.type == "MUT" for child in tree.children)
		ref_name = "&mut" if mut else "&"
		return TypeExpr(name=ref_name, args=[inner])
	if name == "type_expr":
		for child in tree.children:
			if isinstance(child, Tree) and _name(child) in {"base_type", "qualified_base_type", "type_expr", "ref_type"}:
				return _build_type_expr(child)
		return TypeExpr(name="<unknown>")
	if name == "base_type":
		name_token = tree.children[0]
		args: List[TypeExpr] = []
		if len(tree.children) > 1:
			type_args = tree.children[1]
			if isinstance(type_args, Tree):
				children = [arg for arg in type_args.children if isinstance(arg, Tree)]
				# Unwrap type_args wrappers that nest the real angle/square args.
				if len(children) == 1 and _name(children[0]) in {"angle_type_args", "square_type_args"}:
					children = [arg for arg in children[0].children if isinstance(arg, Tree)]
				args = [_build_type_expr(arg) for arg in children]
		return TypeExpr(name=name_token.value, args=args, loc=Located(line=name_token.line, column=name_token.column))
	if name == "qualified_base_type":
		# NAME "." NAME type_args?
		alias_tok = tree.children[0]
		name_tok = tree.children[2]
		args: List[TypeExpr] = []
		if len(tree.children) > 3:
			type_args = tree.children[3]
			if isinstance(type_args, Tree):
				children = [arg for arg in type_args.children if isinstance(arg, Tree)]
				if len(children) == 1 and _name(children[0]) in {"angle_type_args", "square_type_args"}:
					children = [arg for arg in children[0].children if isinstance(arg, Tree)]
				args = [_build_type_expr(arg) for arg in children]
		return TypeExpr(
			name=name_tok.value,
			args=args,
			module_alias=alias_tok.value,
			loc=Located(line=alias_tok.line, column=alias_tok.column),
		)
	# fallback for other wrappers
	if tree.children:
		last = tree.children[-1]
		if isinstance(last, Tree):
			return _build_type_expr(last)
	return TypeExpr(name="<unknown>")


def _build_stmt(tree: Tree):
	kind = _name(tree)
	if kind == "stmt":
		for child in tree.children:
			if isinstance(child, Tree):
				inner_kind = _name(child)
				if inner_kind in {"simple_stmt", "if_stmt", "try_stmt"}:
					return _build_stmt(child)
		return None
	if kind == "simple_stmt":
		target = tree.children[0]
		stmt_kind = _name(target)
		if stmt_kind == "let_stmt":
			return _build_let_stmt(target)
		if stmt_kind == "for_stmt":
			return _build_for_stmt(target)
		if stmt_kind == "aug_assign_stmt":
			return _build_aug_assign_stmt(target)
		if stmt_kind == "assign_stmt":
			return _build_assign_stmt(target)
		if stmt_kind == "return_stmt":
			return _build_return_stmt(target)
		if stmt_kind == "rethrow_stmt":
			return _build_rethrow_stmt(target)
		if stmt_kind == "raise_stmt":
			return _build_raise_stmt(target)
		if stmt_kind == "expr_stmt":
			return _build_expr_stmt(target)
		if stmt_kind == "import_stmt":
			return _build_import_stmt(target)
		if stmt_kind == "export_stmt":
			return _build_export_stmt(target)
		if stmt_kind == "while_stmt":
			return _build_while_stmt(target)
		if stmt_kind == "break_stmt":
			return _build_break_stmt(target)
		if stmt_kind == "continue_stmt":
			return _build_continue_stmt(target)
		return None
	if kind == "if_stmt":
		return _build_if_stmt(tree)
	if kind == "while_stmt":
		return _build_while_stmt(tree)
	if kind == "try_stmt":
		return _build_try_stmt(tree)
	if kind == "let_stmt":
		return _build_let_stmt(tree)
	if kind == "assign_stmt":
		return _build_assign_stmt(tree)
	if kind == "aug_assign_stmt":
		return _build_aug_assign_stmt(tree)
	if kind == "return_stmt":
		return _build_return_stmt(tree)
	if kind == "rethrow_stmt":
		return _build_rethrow_stmt(tree)
	if kind == "raise_stmt":
		return _build_raise_stmt(tree)
	if kind == "expr_stmt":
		return _build_expr_stmt(tree)
	if kind == "import_stmt":
		return _build_import_stmt(tree)
	if kind == "export_stmt":
		return _build_export_stmt(tree)
	return None


def _build_let_stmt(tree: Tree) -> LetStmt:
    loc = _loc(tree)
    child_nodes = [child for child in tree.children if isinstance(child, Tree)]
    idx = 0
    binder_node = child_nodes[idx]
    idx += 1
    mutable = _binder_is_mutable(binder_node)
    binding_node = child_nodes[idx]
    idx += 1
    name_token, capture = _parse_binding_name(binding_node)
    type_expr = None
    if idx < len(child_nodes) and _name(child_nodes[idx]) == "type_spec":
        type_spec_node = child_nodes[idx]
        idx += 1
        type_expr_node = next(
            (
                child
                for child in type_spec_node.children
                if isinstance(child, Tree) and _name(child) == "type_expr"
            ),
            None,
        )
        if type_expr_node is None:
            raise ValueError("type_spec missing type expression")
        type_expr = _build_type_expr(type_expr_node)
    capture_alias = None
    if idx < len(child_nodes) and _name(child_nodes[idx]) == "alias_clause":
        capture_alias = _parse_alias(child_nodes[idx])
        idx += 1
    value_expr = _build_expr(child_nodes[idx])
    return LetStmt(
        loc=loc,
        name=name_token.value,
        type_expr=type_expr,
        value=value_expr,
        mutable=mutable,
        capture=capture,
        capture_alias=capture_alias,
    )


def _build_assign_stmt(tree: Tree) -> AssignStmt:
    loc = _loc(tree)
    tree_children = [child for child in tree.children if isinstance(child, Tree)]
    target_node = tree_children[0]
    value_node = tree_children[1]
    target = _build_expr(target_node)
    value = _build_expr(value_node)
    return AssignStmt(loc=loc, target=target, value=value)


def _build_aug_assign_stmt(tree: Tree) -> "AugAssignStmt":
    """
    Build an augmented-assignment statement.

    Surface syntax (MVP):
      <lvalue> += <expr>   <lvalue> -= <expr>
      <lvalue> *= <expr>   <lvalue> /= <expr>
      <lvalue> %= <expr>
      <lvalue> &= <expr>   <lvalue> |= <expr>   <lvalue> ^= <expr>
      <lvalue> <<= <expr>  <lvalue> >>= <expr>

    We parse `+=` as its own statement form rather than desugaring to
    `x = x + y` / `x = x - y` in the parser. This keeps later lowering correct for complex
    lvalues (e.g., `arr[i] += 1`) because the target is evaluated once at the
    MIR level (address-of + load + add + store) instead of being duplicated.
    """
    loc = _loc(tree)
    # Children include: <assign_target> <op token> <expr>
    target_node = next(child for child in tree.children if isinstance(child, Tree))
    op_tok = next(
        child
        for child in tree.children
        if isinstance(child, Token)
        and child.type
        in {
            "PLUS_EQ",
            "MINUS_EQ",
            "STAR_EQ",
            "SLASH_EQ",
            "PERCENT_EQ",
            "AMP_EQ",
            "BAR_EQ",
            "CARET_EQ",
            "LSHIFT_EQ",
            "SHR_EQ",
        }
    )
    value_node = next(
        child
        for child in tree.children
        if isinstance(child, Tree) and child is not target_node
    )
    target = _build_expr(target_node)
    value = _build_expr(value_node)
    return AugAssignStmt(loc=loc, target=target, op=op_tok.value, value=value)


def _build_for_stmt(tree: Tree) -> ForStmt:
    loc = _loc(tree)
    ident_node = next(
        child
        for child in tree.children
        if (isinstance(child, Token) and child.type in {"NAME", "MOVE"})
        or (isinstance(child, Tree) and _name(child) == "ident")
    )
    name_token = _unwrap_ident(ident_node)
    # `for` statement shape:
    #   for <ident> in <expr> terminator_opt <block>
    #
    # The parse tree includes both the loop binding `ident` and the iterable
    # `expr` as Tree nodes. We must select the iterable expression here, not
    # the binding identifier; otherwise we end up trying to build an expression
    # from the `ident` node and crash on its raw Token child.
    expr_node = next(
        child
        for child in tree.children
        if isinstance(child, Tree) and _name(child) not in {"ident", "block", "terminator_opt"}
    )
    block_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) == "block")
    iter_expr = _build_expr(expr_node)
    body_stmts = [
        _build_stmt(child) for child in block_node.children if isinstance(child, Tree) and _name(child) == "stmt"
    ]
    body_stmts = [s for s in body_stmts if s is not None]
    return ForStmt(loc=loc, var=name_token.value, iter_expr=iter_expr, body=Block(statements=body_stmts))


def _parse_binding_name(tree: Tree) -> tuple[Token, bool]:
    capture = False
    name_token: Optional[Token] = None
    for child in tree.children:
        if isinstance(child, Token) and child.type in {"NAME", "MOVE"}:
            name_token = child
        elif isinstance(child, Tree):
            if _name(child) == "capture_marker":
                capture = True
            elif _name(child) == "ident":
                name_token = _unwrap_ident(child)
            else:
                # capture_marker child holds the caret token; ignore
                for grand in child.children:
                    if isinstance(grand, Token) and grand.type == "CARET":
                        capture = True
    if name_token is None:
        raise ValueError("binding name missing identifier")
    return name_token, capture


def _parse_alias(tree: Tree) -> str:
	string_token = next(
		child for child in tree.children if isinstance(child, Token) and child.type == "STRING"
	)
	return _decode_string_token(string_token)


def _binder_is_mutable(node: Tree) -> bool:
    for child in node.children:
        if isinstance(child, Token):
            if child.type == "VAR":
                return True
    return False


def _build_return_stmt(tree: Tree) -> ReturnStmt:
	loc = _loc(tree)
	children = [child for child in tree.children if not isinstance(child, Token) or child.type != "RETURN"]
	value = _build_expr(children[0]) if children else None
	return ReturnStmt(loc=loc, value=value)


def _build_rethrow_stmt(tree: Tree) -> RethrowStmt:
	loc = _loc(tree)
	return RethrowStmt(loc=loc)


def _build_raise_stmt(tree: Tree) -> RaiseStmt:
    loc = _loc(tree)
    children = [
        child
        for child in tree.children
        if not isinstance(child, Token) or child.type not in {"RAISE", "THROW"}
    ]
    idx = 0
    domain: Optional[str] = None
    if idx < len(children) and _name(children[idx]) == "domain_clause":
        domain = children[idx].children[0].value
        idx += 1
    value = _build_expr(children[idx])
    return RaiseStmt(loc=loc, value=value, domain=domain)


def _build_expr_stmt(tree: Tree) -> ExprStmt:
    loc = _loc(tree)
    expr = _build_expr(tree.children[0])
    return ExprStmt(loc=loc, value=expr)


def _build_struct_def(tree: Tree) -> StructDef:
    loc = _loc(tree)
    name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
    type_params_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "type_params"), None)
    type_params: list[str] = []
    type_param_locs: list[Located] = []
    if type_params_node is not None:
        for tok in type_params_node.children:
            if isinstance(tok, Token) and tok.type == "NAME":
                type_params.append(tok.value)
                type_param_locs.append(_loc_from_token(tok))
    require_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "require_clause"), None)
    body = next(
        (c for c in tree.children if isinstance(c, Tree) and _name(c) in {"struct_body", "tuple_struct", "block_struct"}),
        None,
    )
    if body is None:
        raise ValueError("struct definition missing body")
    field_nodes = _collect_struct_fields(body)
    fields = [_build_struct_field(node) for node in field_nodes]
    require = _build_require_clause(require_node) if require_node is not None else None
    return StructDef(
        name=name_token.value,
        fields=fields,
        type_params=type_params,
        type_param_locs=type_param_locs,
        require=require,
        loc=loc,
    )


def _collect_struct_fields(tree: Tree) -> List[Tree]:
    """
    Collect `struct_field` nodes from a struct body, preserving source order.

    Struct declarations have two surface forms:
      - tuple form: `struct S(x: Int, y: Int)`
      - block form: `struct S { x: Int, y: Int }`

    The parser needs the declared field order to be stable and predictable:
      - it defines the positional constructor argument order (`S(1, 2)`),
      - it defines the layout order in the TypeTable (field indices),
      - it defines the LLVM struct layout order (GEP indices).

    This walk is intentionally order-preserving (left-to-right pre-order). Avoid
    stack-based DFS here: it reverses field ordering and silently miscompiles
    field access/borrows.
    """
    result: List[Tree] = []

    def walk(node: object) -> None:
        if not isinstance(node, Tree):
            return
        if _name(node) == "struct_field":
            result.append(node)
            return
        for child in node.children:
            walk(child)

    walk(tree)
    return result


def _build_struct_field(tree: Tree) -> StructField:
    name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
    type_node = next(child for child in tree.children if isinstance(child, Tree) and _name(child) == "type_expr")
    return StructField(name=name_token.value, type_expr=_build_type_expr(type_node))


def _build_import_stmt(tree: Tree) -> ImportStmt:
    loc = _loc(tree)
    path_node = tree.children[0]
    parts = [child.value for child in path_node.children if isinstance(child, Token) and child.type == "NAME"]
    alias = None
    if len(tree.children) > 1 and isinstance(tree.children[1], Tree) and _name(tree.children[1]) == "import_alias":
        alias_tok = next((c for c in tree.children[1].children if isinstance(c, Token) and c.type == "NAME"), None)
        if alias_tok:
            alias = alias_tok.value
    return ImportStmt(loc=loc, path=parts, alias=alias)


def _build_export_stmt(tree: Tree) -> ExportStmt:
	loc = _loc(tree)
	items_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "export_items"), None)
	items: List[ExportItem] = []
	if items_node is not None:
		for child in items_node.children:
			if isinstance(child, Tree) and _name(child) == "export_item":
				items.append(_build_export_item(child))
	return ExportStmt(loc=loc, items=items)


def _build_export_item(tree: Tree) -> ExportItem:
	loc = _loc(tree)
	module_node = next(
		(c for c in tree.children if isinstance(c, Tree) and _name(c) in {"module_path", "module_path_star"}),
		None,
	)
	if module_node is not None:
		parts = [tok.value for tok in module_node.children if isinstance(tok, Token) and tok.type == "NAME"]
		return ExportModuleStar(loc=loc, module_path=parts)
	name_tok = next((c for c in tree.children if isinstance(c, Token) and c.type == "NAME"), None)
	if name_tok is not None:
		return ExportName(loc=loc, name=name_tok.value)
	raise ValueError("export item missing name or module path")


def _build_if_stmt(tree: Tree) -> IfStmt:
    loc = _loc(tree)
    condition_node = None
    then_block_node = None
    else_block_node = None
    for child in tree.children:
        if not isinstance(child, Tree):
            continue
        name = _name(child)
        if condition_node is None and name != "terminator_opt":
            condition_node = child
            continue
        if then_block_node is None and name == "block":
            then_block_node = child
            continue
        if name == "else_clause":
            for grand in child.children:
                if isinstance(grand, Tree) and _name(grand) == "block":
                    else_block_node = grand
                    break
            continue
        if name == "block":
            else_block_node = child
            break
    if condition_node is None or then_block_node is None:
        raise ValueError("malformed if statement")
    cond_node = condition_node
    if _name(cond_node) == "if_cond":
        cond_node = next((c for c in cond_node.children if isinstance(c, Tree)), cond_node)
    if isinstance(cond_node, Tree) and _name(cond_node).startswith("trait_"):
        condition = _build_trait_expr(cond_node)
    else:
        condition = _build_expr(cond_node)
    then_block = _build_block(then_block_node)
    else_block = _build_block(else_block_node) if else_block_node else None
    return IfStmt(loc=loc, condition=condition, then_block=then_block, else_block=else_block)


def _build_while_stmt(tree: Tree) -> WhileStmt:
    loc = _loc(tree)
    condition_node = None
    body_node = None
    for child in tree.children:
        if isinstance(child, Tree) and _name(child) == "terminator_opt":
            continue
        if condition_node is None and isinstance(child, Tree):
            condition_node = child
            continue
        if isinstance(child, Tree) and _name(child) == "block":
            body_node = child
            break
    if condition_node is None or body_node is None:
        raise ValueError("malformed while statement")
    condition = _build_expr(condition_node)
    body = _build_block(body_node)
    return WhileStmt(loc=loc, condition=condition, body=body)


def _build_break_stmt(tree: Tree) -> BreakStmt:
    return BreakStmt(loc=_loc(tree))


def _build_continue_stmt(tree: Tree) -> ContinueStmt:
    return ContinueStmt(loc=_loc(tree))


def _build_try_stmt(tree: Tree) -> TryStmt:
    loc = _loc(tree)
    try_block = None
    catches: list[CatchClause] = []
    for child in tree.children:
        if not isinstance(child, Tree):
            continue
        name = _name(child)
        if try_block is None and name != "catch_clause":
            # The try statement supports both block form:
            #   try { ... } catch ...
            # and call/expression shorthand:
            #   try foo() catch ...
            #
            # For the shorthand we wrap the expression into a single ExprStmt
            # so the rest of the pipeline continues to treat TryStmt.body as a block.
            if name == "block":
                try_block = _build_block(child)
            else:
                expr = _build_expr(child)
                try_block = Block(statements=[ExprStmt(loc=_loc(child), value=expr)])
        elif name == "catch_clause":
            catches.append(_build_catch_clause(child))
    if try_block is None:
        raise ValueError("try statement missing body")
    if not catches:
        raise ValueError("try statement requires at least one catch clause")
    return TryStmt(loc=loc, body=try_block, catches=catches)


def _fqn_from_tree(tree: Tree) -> str:
    parts: list[str] = []
    for child in tree.children:
        if isinstance(child, Token):
            parts.append(child.value)
        elif isinstance(child, Tree):
            parts.append(_fqn_from_tree(child))
    return "".join(parts)


def _build_catch_clause(tree: Tree) -> CatchClause:
	event: str | None = None
	binder: str | None = None
	block_node = None
	for child in tree.children:
		if isinstance(child, Tree):
			name = _name(child)
			if name in {"catch_pattern", "catch_event", "catch_all", "catch_all_empty"}:
				event_node = next((c for c in child.children if isinstance(c, Tree) and _name(c) == "event_fqn"), None)
				if event_node is not None:
					event = _fqn_from_tree(event_node)
				ident_node = next(
					(
						c
						for c in child.children
						if (isinstance(c, Token) and c.type in {"NAME", "MOVE"})
						or (isinstance(c, Tree) and _name(c) == "ident")
					),
					None,
				)
				if ident_node is not None:
					binder = _unwrap_ident(ident_node).value
			elif name == "block":
				block_node = child
	if block_node is None:
		raise ValueError("catch clause missing block")
	block = _build_block(block_node)
	return CatchClause(event=event, binder=binder, block=block)


def _build_expr(node) -> Expr:
    if isinstance(node, Tree):
        # Some grammar nodes carry a Token as .data (e.g., *_tail); unwrap selected
        # cases, otherwise treat the token value as the rule name.
        if isinstance(node.data, Token):
            name = node.data.value
        else:
            name = _name(node)
    else:
        raise TypeError(f"Unexpected node type: {type(node)}")

    if name in {
        "logic_and_tail",
        "logic_or_tail",
        # These tail nodes are part of left-associative binary chains. They
        # should never be built as standalone expressions, but the parser may
        # surface them as intermediate nodes during error recovery. Treat them
        # as “yield the RHS expression” so callers don’t silently drop terms.
        "pipeline_tail",
        "shift_tail",
        "bit_or_tail",
        "bit_xor_tail",
        "bit_and_tail",
    }:
        rhs = next((c for c in node.children if isinstance(c, Tree)), None)
        if rhs is None:
            raise TypeError(f"Unexpected tail shape: {node.children!r}")
        return _build_expr(rhs)

    if name == "logic_or":
        return _fold_chain(node, "logic_or_tail")
    if name == "try_catch_expr":
        return _build_try_catch_expr(node)
    if name == "match_expr":
        return _build_match_expr(node)
    if name == "ternary":
        return _build_ternary(node)
    if name == "pipeline":
        # Pipeline operators are a left-associative chain of `pipeline_tail`
        # nodes (token + rhs).
        return _fold_chain(node, "pipeline_tail")
    if name == "lambda_expr":
        return _build_lambda(node)
    if name == "exception_ctor":
        return _build_exception_ctor(node)
    if name == "logic_and":
        return _fold_chain(node, "logic_and_tail")
    if name == "bit_or":
        return _fold_chain(node, "bit_or_tail")
    if name == "bit_xor":
        return _fold_chain(node, "bit_xor_tail")
    if name == "bit_and":
        return _fold_chain(node, "bit_and_tail")
    if name == "equality":
        return _fold_chain(node, "equality_tail")
    if name == "comparison":
        return _fold_chain(node, "comparison_tail")
    if name == "shift":
        return _fold_chain(node, "shift_tail")
    if name == "sum":
        return _fold_chain(node, "sum_tail")
    if name == "term":
        return _fold_chain(node, "term_tail")
    if name == "factor":
        return _build_expr(node.children[0])
    if name == "borrow":
        mut = any(isinstance(child, Token) and child.type == "MUT" for child in node.children)
        target = _build_expr(next(child for child in node.children if isinstance(child, Tree)))
        return Unary(loc=_loc(node), op="&mut" if mut else "&", operand=target)
    if name == "bit_not":
        target = next((c for c in node.children if isinstance(c, Tree)), None)
        if target is None:
            raise TypeError(f"bit_not expects an operand, got {node.children!r}")
        expr = _build_expr(target)
        return Unary(loc=_loc(node), op="~", operand=expr)
    if name == "deref":
        # Pointer dereference: `*expr`. The type checker restricts this to
        # references, and assignment restricts it to `*place = ...` for `&mut`.
        target = next((c for c in node.children if isinstance(c, Tree)), None)
        if target is None:
            raise TypeError(f"deref expects an operand, got {node.children!r}")
        expr = _build_expr(target)
        return Unary(loc=_loc(node), op="*", operand=expr)
    if name == "move_op":
        # Ownership transfer marker: `move <expr>`.
        #
        # Note: this is syntax-only today; semantic enforcement (no move from
        # borrows/vals, moved-from state) is handled by later phases once
        # move-only types are fully implemented.
        target = next((c for c in node.children if isinstance(c, Tree)), None)
        if target is None:
            raise TypeError(f"move_op expects an operand, got {node.children!r}")
        expr = _build_expr(target)
        return Move(loc=_loc(node), value=expr)
    if name == "postfix":
        return _build_postfix(node)
    if name == "qualified_member":
        name_toks = [c for c in node.children if isinstance(c, Token) and c.type == "NAME"]
        if len(name_toks) != 2:
            raise QualifiedMemberParseError(
                "E-PARSE-QMEM-SHAPE: qualified member must have exactly 2 NAME tokens",
                loc=_loc(node),
            )

        type_arg_nodes = [
            c
            for c in node.children
            if isinstance(c, Tree)
            and _name(c) in {"qualified_pre_type_args", "qualified_post_type_args"}
        ]
        if len(type_arg_nodes) > 1:
            # MVP: accept at most one explicit type-argument list. Supporting both
            # `Optional<T>::Ctor(...)` and `Optional::Ctor<T>(...)` is enough; mixing
            # both is ambiguous and not needed yet.
            raise QualifiedMemberParseError(
                "E-PARSE-QMEM-DUP-TYPEARGS: qualified member may specify type arguments only once",
                loc=_loc(type_arg_nodes[1]),
            )

        type_args: list[TypeExpr] = []
        if type_arg_nodes:
            type_args = [
                _build_type_expr(t)
                for t in type_arg_nodes[0].children
                if isinstance(t, Tree) and _name(t) == "type_expr"
            ]

        base_first = name_toks[0]
        base_type = TypeExpr(
            name=base_first.value,
            args=type_args,
            loc=Located(line=base_first.line, column=base_first.column),
        )
        member_tok = name_toks[1]
        return QualifiedMember(loc=_loc(node), base_type=base_type, member=member_tok.value)
    if name == "leading_dot":
        return _build_leading_dot(node)
    if name == "primary":
        return _build_expr(node.children[0])
    if name == "neg":
        # Unary minus: children include the '-' token and the operand expression.
        target = next((c for c in node.children if isinstance(c, Tree)), None)
        if target is None:
            raise TypeError(f"neg expects an operand, got {node.children!r}")
        expr = _build_expr(target)
        return Unary(loc=_loc(node), op="-", operand=expr)
    if name == "pos":
        # Unary plus: `+<expr>`.
        #
        # This is a no-op semantically, but we keep it as an AST node so later
        # phases can diagnose/handle it consistently (e.g., const evaluation).
        target = next((c for c in node.children if isinstance(c, Tree)), None)
        if target is None:
            raise TypeError(f"pos expects an operand, got {node.children!r}")
        expr = _build_expr(target)
        return Unary(loc=_loc(node), op="+", operand=expr)
    if name == "not_op":
        # Logical negation: children include the '!' token (or 'not') and the operand.
        target = next((c for c in node.children if isinstance(c, Tree)), None)
        if target is None:
            raise TypeError(f"not_op expects an operand, got {node.children!r}")
        expr = _build_expr(target)
        return Unary(loc=_loc(node), op="not", operand=expr)
    if name == "var":
        ident_token = _unwrap_ident(node.children[0])
        return Name(loc=_loc(node), ident=ident_token.value)
    if name == "placeholder":
        return Placeholder(loc=_loc(node))
    if name == "int_lit":
        return Literal(loc=_loc(node), value=int(node.children[0].value))
    if name == "float_lit":
        return Literal(loc=_loc(node), value=float(node.children[0].value))
    if name == "str_lit":
        raw_tok = node.children[0]
        return Literal(loc=_loc(node), value=_decode_string_token(raw_tok))
    if name == "fstr_lit":
        # Children: FSTRING_PREFIX token and STRING token.
        string_tok = next((c for c in node.children if isinstance(c, Token) and c.type == "STRING"), None)
        if string_tok is None:
            raise ValueError("f-string missing STRING token")
        return _parse_fstring(_loc(node), string_tok)
    if name == "true_lit":
        return Literal(loc=_loc(node), value=True)
    if name == "false_lit":
        return Literal(loc=_loc(node), value=False)
    if name == "array_literal":
        return _build_array_literal(node)
    if node.children:
        return _build_expr(node.children[0])
    raise ValueError(f"Unsupported expression node: {name}")


def _build_array_literal(tree: Tree) -> ArrayLiteral:
    elements = [_build_expr(child) for child in tree.children if isinstance(child, Tree)]
    return ArrayLiteral(loc=_loc(tree), elements=elements)


def _apply_index_suffix(base: Expr, suffix_node: Tree) -> Index:
    idx_child = next((c for c in suffix_node.children if isinstance(c, Tree)), None)
    if idx_child is None:
        raise ValueError("index suffix missing expression")
    index_expr = _build_expr(idx_child)
    return Index(loc=_loc(suffix_node), value=base, index=index_expr)


def _fold_chain(tree: Tree, tail_name: str) -> Expr:
    child_nodes = [child for child in tree.children if isinstance(child, Tree)]
    head = _build_expr(child_nodes[0])
    result = head
    for child in child_nodes[1:]:
        if _name(child) != tail_name:
            continue
        result = _binary_tail(result, child)
    return result


def _binary_tail(left: Expr, tail: Tree) -> Expr:
    op_token = tail.children[0]
    right = _build_expr(tail.children[1])
    return Binary(loc=_loc_from_token(op_token), op=op_token.value, left=left, right=right)


def _build_pipeline(tree: Tree) -> Expr:
    children = list(tree.children)
    if not children:
        raise ValueError("pipeline requires at least one child")
    idx = 0
    result = _build_expr(children[idx])
    idx += 1
    while idx < len(children):
        token = children[idx]
        idx += 1
        if not isinstance(token, Token):
            raise ValueError("Expected pipeline operator token")
        right = _build_expr(children[idx])
        idx += 1
        result = Binary(loc=_loc_from_token(token), op=token.value, left=result, right=right)
    return result


def _build_try_catch_expr(tree: Tree) -> TryCatchExpr:
    parts = [child for child in tree.children if isinstance(child, Tree)]
    if not parts:
        raise ValueError("try_catch_expr expects attempt and at least one catch arm")

    attempt_node: Tree | None = None
    arm_nodes: list[Tree] = []
    for node in parts:
        name = _name(node)
        if name == "terminator_opt":
            continue
        if name.startswith("catch_expr_"):
            arm_nodes.append(node)
            continue
        if attempt_node is None:
            attempt_node = node
            continue
        raise ValueError("try_catch_expr has unexpected extra expression nodes")

    if attempt_node is None or not arm_nodes:
        raise ValueError("try_catch_expr expects attempt and at least one catch arm")

    attempt = _build_expr(attempt_node)
    arms: List[CatchExprArm] = []
    for arm_node in arm_nodes:
        arm_name = _name(arm_node)
        if arm_name == "catch_expr_event":
            event_node = next((c for c in arm_node.children if isinstance(c, Tree) and _name(c) == "event_fqn"), None)
            if event_node is None:
                raise ValueError("event catch arm requires event")
            binder_node = next(
                (
                    c
                    for c in arm_node.children
                    if (isinstance(c, Token) and c.type in {"NAME", "MOVE"})
                    or (isinstance(c, Tree) and _name(c) == "ident")
                ),
                None,
            )
            if binder_node is None:
                raise ValueError("event catch arm requires binder")
            binder_token = _unwrap_ident(binder_node)
            block_node = next(
                child for child in arm_node.children if isinstance(child, Tree) and _name(child) == "value_block"
            )
            arms.append(
                CatchExprArm(
                    event=_fqn_from_tree(event_node),
                    binder=binder_token.value,
                    block=_build_value_block(block_node),
                )
            )
        elif arm_name == "catch_expr_binder":
            binder_node = next(
                c
                for c in arm_node.children
                if (isinstance(c, Token) and c.type in {"NAME", "MOVE"})
                or (isinstance(c, Tree) and _name(c) == "ident")
            )
            binder_token = _unwrap_ident(binder_node)
            block_node = next(
                child for child in arm_node.children if isinstance(child, Tree) and _name(child) == "value_block"
            )
            arms.append(CatchExprArm(event=None, binder=binder_token.value, block=_build_value_block(block_node)))
        elif arm_name == "catch_expr_block":
            block_node = next(
                child for child in arm_node.children if isinstance(child, Tree) and _name(child) == "value_block"
            )
            arms.append(CatchExprArm(event=None, binder=None, block=_build_value_block(block_node)))
        else:
            raise ValueError(f"unexpected catch expr arm {arm_name}")
    return TryCatchExpr(loc=_loc(tree), attempt=attempt, catch_arms=arms)


def _build_match_expr(tree: Tree) -> MatchExpr:
	"""
	Build a match expression.

	Grammar:
	  match_expr: MATCH expr "{" (match_arm (TERMINATOR | COMMA)*)+ "}"
	"""
	# The scrutinee is the first expression subtree directly under `match_expr`.
	#
	# Note: because `expr` is an inlined rule (`?expr`), Lark does not always
	# materialize it as a distinct `Tree("expr")`. The first child that is a
	# `Tree` and is *not* a `match_arm` is the scrutinee expression.
	scrutinee_node = next(
		(c for c in tree.children if isinstance(c, Tree) and _name(c) != "match_arm"),
		None,
	)
	if scrutinee_node is None:
		raise ValueError("match_expr missing scrutinee expression")
	scrutinee = _build_expr(scrutinee_node)

	arms: list[MatchArm] = []
	for arm_node in (c for c in tree.children if isinstance(c, Tree) and _name(c) == "match_arm"):
		pat: Optional[Tree] = None
		for child in (c for c in arm_node.children if isinstance(c, Tree)):
			child_name = _name(child)
			if child_name == "match_pat":
				pat = next((c for c in child.children if isinstance(c, Tree)), None)
				break
			# `match_pat` is not inlined, but because each alternative uses `->`,
			# Lark typically produces the alternative node directly (e.g.
			# `match_ctor_paren`) rather than a `match_pat` wrapper.
			if child_name in (
				"match_default",
				"match_ctor",
				"match_ctor0",
				"match_ctor_named",
				"match_ctor_paren",
			):
				pat = child
				break
		if pat is None:
			raise ValueError("match_arm missing pattern")

		pat_kind = _name(pat)
		ctor: Optional[str] = None
		binders: list[str] = []
		binder_fields: list[str] | None = None
		pattern_arg_form = "positional"
		if pat_kind == "match_default":
			ctor = None
			pattern_arg_form = "bare"
		elif pat_kind == "match_ctor":
			name_tok = next(c for c in pat.children if isinstance(c, Token) and c.type == "NAME")
			ctor = name_tok.value
			binders_node = next((c for c in pat.children if isinstance(c, Tree) and _name(c) == "match_binders"), None)
			if binders_node is not None:
				binders = [c.value for c in binders_node.children if isinstance(c, Token) and c.type == "NAME"]
			pattern_arg_form = "positional"
		elif pat_kind == "match_ctor_named":
			name_tok = next(c for c in pat.children if isinstance(c, Token) and c.type == "NAME")
			ctor = name_tok.value
			fields_node = next((c for c in pat.children if isinstance(c, Tree) and _name(c) == "match_named_binders"), None)
			if fields_node is None:
				raise ValueError("match_ctor_named missing match_named_binders")
			binders = []
			binder_fields = []
			for nb in (c for c in fields_node.children if isinstance(c, Tree) and _name(c) == "match_named_binder"):
				parts = [c for c in nb.children if isinstance(c, Token) and c.type == "NAME"]
				if len(parts) != 2:
					raise ValueError("match_named_binder expects two NAME tokens")
				field_name, binder_name = parts[0].value, parts[1].value
				binder_fields.append(field_name)
				binders.append(binder_name)
			pattern_arg_form = "named"
		elif pat_kind == "match_ctor_paren":
			name_tok = next(c for c in pat.children if isinstance(c, Token) and c.type == "NAME")
			ctor = name_tok.value
			binders = []
			binder_fields = None
			pattern_arg_form = "paren"
		elif pat_kind == "match_ctor0":
			name_tok = next(c for c in pat.children if isinstance(c, Token) and c.type == "NAME")
			ctor = name_tok.value
			pattern_arg_form = "bare"
		else:
			raise ValueError(f"Unsupported match_pat shape: {pat_kind}")

		body_node = next((c for c in arm_node.children if isinstance(c, Tree) and _name(c) == "match_arm_body"), None)
		if body_node is None:
			raise ValueError("match_arm missing body")
		inner = next((c for c in body_node.children if isinstance(c, Tree)), None)
		if inner is None:
			raise ValueError("match_arm_body missing block")
		block = _build_value_block(inner) if _name(inner) == "value_block" else _build_block(inner)
		arms.append(
			MatchArm(
				loc=_loc(arm_node),
				ctor=ctor,
				pattern_arg_form=pattern_arg_form,
				binders=binders,
				binder_fields=binder_fields,
				block=block,
			)
		)

	if not arms:
		raise ValueError("match_expr requires at least one arm")
	return MatchExpr(loc=_loc(tree), scrutinee=scrutinee, arms=arms)


def _build_ternary(tree: Tree) -> Ternary:
    parts = [child for child in tree.children if isinstance(child, Tree)]
    if len(parts) != 3:
        raise ValueError("ternary expects condition, then, else expressions")
    cond = _build_expr(parts[0])
    then_expr = _build_expr(parts[1])
    else_expr = _build_expr(parts[2])
    return Ternary(loc=_loc(tree), condition=cond, then_value=then_expr, else_value=else_expr)


def _build_exception_ctor(tree: Tree) -> ExceptionCtor:
	"""
	Exception constructor expression (throw-only):
	  Name(arg0, field = expr, ...)

	This mirrors call argument parsing rules:
	  - Positional arguments must precede keyword arguments.
	"""
	name_tok = next((c for c in tree.children if isinstance(c, Token) and c.type == "NAME"), None)
	if name_tok is None:
		raise ValueError("exception_ctor missing name")

	# Grammar shape: NAME "(" [call_args] ")"
	args_node = next((c for c in tree.children if isinstance(c, Tree) and _name(c) == "call_args"), None)
	args, kwargs = _build_call_args(args_node)
	return ExceptionCtor(name=name_tok.value, args=args, kwargs=kwargs, loc=_loc(tree))


def _build_postfix(tree: Tree) -> Expr:
    if not tree.children:
        raise ValueError("postfix node missing children")
    expr = _build_expr(tree.children[0])
    suffix_nodes = tree.children[1:]
    return _apply_postfix_suffixes(expr, suffix_nodes)


def _build_leading_dot(tree: Tree) -> Expr:
    # Base receiver placeholder with an initial attribute name from the DOT NAME.
    name_tok = next(tok for tok in tree.children if isinstance(tok, Token) and tok.type == "NAME")
    base_loc = _loc(tree)
    expr: Expr = Attr(loc=_loc_from_token(name_tok), value=Placeholder(loc=base_loc), attr=name_tok.value)
    suffix_nodes = [child for child in tree.children if isinstance(child, Tree)]
    return _apply_postfix_suffixes(expr, suffix_nodes)


def _apply_postfix_suffixes(expr: Expr, suffix_nodes: List[Tree]) -> Expr:
    for child in suffix_nodes:
        if not isinstance(child, Tree):
            raise ValueError("Unexpected postfix child token")
        suffix_node = child
        suffix_name = _name(suffix_node)
        if suffix_name == "postfix_suffix" and suffix_node.children:
            suffix_node = suffix_node.children[0]
            suffix_name = _name(suffix_node)
        child = suffix_node
        child_name = suffix_name
        if child_name == "call_suffix":
            type_args_node = next(
                (
                    c
                    for c in child.children
                    if isinstance(c, Tree) and _name(c) == "call_type_args"
                ),
                None,
            )
            args_node = next(
                (
                    c
                    for c in child.children
                    if isinstance(c, Tree) and _name(c) == "call_args"
                ),
                None,
            )
            type_args = _build_call_type_args(type_args_node)
            if isinstance(expr, TypeApp):
                if type_args is not None:
                    raise QualifiedMemberParseError(
                        "E-PARSE-CALL-DUP-TYPEARGS: call may specify type arguments only once",
                        loc=_loc(child),
                    )
                type_args = list(expr.type_args)
                expr = expr.func
            if type_args is not None and isinstance(expr, QualifiedMember) and expr.base_type.args:
                raise QualifiedMemberParseError(
                    "E-PARSE-QMEM-DUP-TYPEARGS: qualified member may specify type arguments only once",
                    loc=_loc(child),
                )
            args, kwargs = _build_call_args(args_node)
            expr = Call(loc=_loc(child), func=expr, args=args, kwargs=kwargs, type_args=type_args)
        elif child_name == "attr_suffix":
            attr_token = next(
                token for token in child.children if isinstance(token, Token) and token.type == "NAME"
            )
            expr = Attr(loc=_loc(child), value=expr, attr=attr_token.value)
        elif child_name == "arrow_suffix":
            attr_token = next(
                token for token in child.children if isinstance(token, Token) and token.type == "NAME"
            )
            expr = Attr(loc=_loc(child), value=expr, attr=attr_token.value, op="->")
        elif child_name == "index_suffix":
            expr = _apply_index_suffix(expr, child)
        elif child_name == "type_app_suffix":
            type_args_node = next((c for c in child.children if isinstance(c, Tree)), None)
            type_args = _build_call_type_args(type_args_node)
            if type_args is None:
                raise ValueError("type application suffix missing type arguments")
            if isinstance(expr, TypeApp):
                raise QualifiedMemberParseError(
                    "E-PARSE-TYPEAPP-DUP-TYPEARGS: type application may specify type arguments only once",
                    loc=_loc(child),
                )
            if isinstance(expr, QualifiedMember) and expr.base_type.args:
                raise QualifiedMemberParseError(
                    "E-PARSE-QMEM-DUP-TYPEARGS: qualified member may specify type arguments only once",
                    loc=_loc(child),
                )
            expr = TypeApp(loc=_loc(child), func=expr, type_args=type_args)
        else:
            raise ValueError(f"Unexpected postfix child: {child_name}")
    return expr


def _build_call_type_args(node: Tree | None) -> list[TypeExpr] | None:
    if node is None:
        return None
    if _name(node) != "call_type_args":
        return None
    args: list[TypeExpr] = []
    for child in node.children:
        if not isinstance(child, Tree):
            continue
        args.append(_build_type_expr(child))
    return args


def _build_call_args(node: Tree | None) -> tuple[List[Expr], List[KwArg]]:
    args: List[Expr] = []
    kwargs: List[KwArg] = []
    if node is None:
        return args, kwargs
    positional_done = False
    for arg in node.children:
        if not isinstance(arg, Tree):
            continue
        kind = _name(arg)
        if kind == "kwarg":
            positional_done = True
            kwargs.append(_build_kwarg(arg))
        else:
            if positional_done:
                raise SyntaxError("Positional arguments cannot follow keyword arguments")
            args.append(_build_expr(arg))
    return args, kwargs


def _build_kwarg(tree: Tree) -> KwArg:
    name_token = next(child for child in tree.children if isinstance(child, Token) and child.type == "NAME")
    value_node = next(child for child in tree.children if isinstance(child, Tree))
    value = _build_expr(value_node)
    return KwArg(name=name_token.value, value=value, loc=_loc(tree))


def _loc(tree: Tree) -> Located:
    meta = tree.meta
    return Located(line=meta.line, column=meta.column)


def _loc_from_token(token: Token) -> Located:
    return Located(line=token.line, column=token.column)


def _name(node: Tree | Token) -> str:
    if isinstance(node, Tree):
        data = node.data
        if isinstance(data, Token):
            return data.value
        return data
    if isinstance(node, Token):
        return node.type
    return str(node)

[==== File: staged/lang2/tests/driver/tests/test_abi_boundary_calls.py =====]
from __future__ import annotations

import re
from pathlib import Path

from lang2.driftc.driftc import compile_to_llvm_ir_for_tests
from lang2.driftc.parser import parse_drift_workspace_to_hir


def _extract_llvm_function(ir: str, fn: str) -> str:
	"""
	Extract the textual LLVM IR for a single function definition.

	This is intentionally simple and stable for tests: locate `define ... @<fn>`
	and return everything up to (but not including) the next `define` line.
	"""
	lines = ir.splitlines()
	start = None
	for i, line in enumerate(lines):
		if line.startswith("define ") and re.search(rf"@{re.escape(fn)}\b", line):
			start = i
			break
	if start is None:
		raise AssertionError(f"missing LLVM function definition for {fn}")
	end = len(lines)
	for i in range(start + 1, len(lines)):
		if lines[i].startswith("define "):
			end = i
			break
	return "\n".join(lines[start:end])


def test_cross_module_exported_call_uses_wrapper_not_impl(tmp_path: Path) -> None:
	"""
	Cross-module calls to exported entrypoints must call the public wrapper symbol,
	not the private `__impl` body.
	"""
	(tmp_path / "acme" / "point").mkdir(parents=True)
	(tmp_path / "acme" / "point" / "lib.drift").write_text(
		"\n".join(
			[
				"module acme.point",
				"",
				"export { Point, make_point }",
				"",
				"pub struct Point(x: Int, y: Int)",
				"",
				"pub fn make_point() returns Point {",
				"\treturn Point(x = 1, y = 2);",
				"}",
				"",
			]
		)
	)
	(tmp_path / "main.drift").write_text(
		"\n".join(
			[
				"module main",
				"",
				"import acme.point as ap",
				"",
				"fn main() returns Int {",
				"\tval p: ap.Point = ap.make_point();",
				"\treturn p.x + p.y;",
				"}",
				"",
			]
		)
	)

	drift_files = sorted(tmp_path.rglob("*.drift"))
	func_hirs, signatures, _fn_ids_by_name, type_table, exception_catalog, _exports, _deps, diags = parse_drift_workspace_to_hir(
		drift_files,
		module_paths=[tmp_path],
	)
	assert not diags

	ir, checked = compile_to_llvm_ir_for_tests(
		func_hirs=func_hirs,
		signatures=signatures,
		exc_env=exception_catalog,
		entry="main",
		type_table=type_table,
	)
	assert not checked.diagnostics

	main_ir = _extract_llvm_function(ir, "main")
	assert '@"acme.point::make_point"' in main_ir
	assert "__impl" not in main_ir

[==== File: staged/lang2/tests/driver/tests/test_driftc_package_v0.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import base64
import json
from dataclasses import dataclass
from hashlib import sha256
from pathlib import Path

import pytest

from lang2.driftc.driftc import main as driftc_main
from lang2.driftc.packages import dmir_pkg_v0
from lang2.driftc.packages.provider_v0 import discover_package_files
from lang2.driftc.packages.provider_v0 import load_package_v0
from lang2.driftc.packages.dmir_pkg_v0 import canonical_json_bytes, sha256_hex, write_dmir_pkg_v0
from lang2.driftc.packages.signature_v0 import compute_ed25519_kid

from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey


def _emit_pkg_args(package_id: str) -> list[str]:
	return [
		"--package-id",
		package_id,
		"--package-version",
		"0.0.0",
		"--package-target",
		"test-target",
	]


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def _patch_file_bytes(path: Path, offset: int, patch: bytes) -> None:
	data = path.read_bytes()
	if offset < 0 or offset + len(patch) > len(data):
		raise ValueError("patch out of range")
	new_data = data[:offset] + patch + data[offset + len(patch) :]
	path.write_bytes(new_data)


def _patch_pkg_header(path: Path, *, manifest_sha256: bytes | None = None, toc_sha256: bytes | None = None) -> None:
	header_bytes = path.read_bytes()[: dmir_pkg_v0.HEADER_SIZE_V0]
	(
		magic,
		version,
		flags,
		header_size,
		manifest_len,
		manifest_sha,
		toc_len,
		toc_entry_size,
		toc_sha,
		reserved,
	) = dmir_pkg_v0._HEADER_STRUCT.unpack(header_bytes)
	if manifest_sha256 is not None:
		manifest_sha = manifest_sha256
	if toc_sha256 is not None:
		toc_sha = toc_sha256
	new_header = dmir_pkg_v0._HEADER_STRUCT.pack(
		magic,
		version,
		flags,
		header_size,
		manifest_len,
		manifest_sha,
		toc_len,
		toc_entry_size,
		toc_sha,
		reserved,
	)
	_patch_file_bytes(path, 0, new_header)


def _patch_pkg_manifest_bytes_same_len(path: Path, patch_fn) -> None:
	"""
	Patch the manifest bytes in-place without changing `manifest_len`.

	This helper is intentionally strict: it requires the new bytes to be the same
	length as the old bytes so TOC offsets remain valid.
	"""
	header_bytes = path.read_bytes()[: dmir_pkg_v0.HEADER_SIZE_V0]
	(
		_magic,
		_version,
		_flags,
		_header_size,
		manifest_len,
		_manifest_sha,
		_toc_len,
		_toc_entry_size,
		_toc_sha,
		_reserved,
	) = dmir_pkg_v0._HEADER_STRUCT.unpack(header_bytes)
	manifest_off = dmir_pkg_v0.HEADER_SIZE_V0
	old = path.read_bytes()[manifest_off : manifest_off + int(manifest_len)]
	new = patch_fn(old)
	if len(new) != len(old):
		raise ValueError("manifest patch must not change length")
	_patch_file_bytes(path, manifest_off, new)
	_patch_pkg_header(path, manifest_sha256=dmir_pkg_v0.sha256_bytes(new))


def _b64(data: bytes) -> str:
	return base64.b64encode(data).decode("ascii")


def _sha256_hex(data: bytes) -> str:
	return sha256(data).hexdigest()


@dataclass(frozen=True)
class _SignedPkg:
	root: Path
	pkg_path: Path
	trust_path: Path
	kid: str
	pub_b64: str


def _emit_lib_pkg(tmp_path: Path, *, module_id: str = "acme.lib") -> Path:
	module_dir = tmp_path.joinpath(*module_id.split("."))
	_write_file(
		module_dir / "lib.drift",
		f"""
module {module_id}

export {{ add }}

pub fn add(a: Int, b: Int) returns Int {{
	return a + b
}}
""".lstrip(),
	)
	pkg_path = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "lib.drift"),
				*_emit_pkg_args(module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _emit_hidden_fn_pkg(tmp_path: Path, *, module_id: str = "acme.hidden") -> Path:
	module_dir = tmp_path.joinpath(*module_id.split("."))
	_write_file(
		module_dir / "lib.drift",
		f"""
module {module_id}

export {{ add }}

pub fn add(a: Int, b: Int) returns Int {{
	return a + b
}}

fn hidden() returns Int {{
	return 1
}}
""".lstrip(),
	)
	pkg_path = tmp_path / "hidden.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "lib.drift"),
				*_emit_pkg_args(module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _emit_const_pkg(tmp_path: Path, *, module_id: str = "acme.consts") -> Path:
	module_dir = tmp_path.joinpath(*module_id.split("."))
	_write_file(
		module_dir / "consts.drift",
		f"""
module {module_id}

export {{ ANSWER }}

pub const ANSWER: Int = 42;
""".lstrip(),
	)
	pkg_path = tmp_path / "consts.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "consts.drift"),
				*_emit_pkg_args(module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _emit_point_type_only_pkg(tmp_path: Path, *, module_id: str = "acme.point") -> Path:
	module_dir = tmp_path.joinpath(*module_id.split("."))
	_write_file(
		module_dir / "point.drift",
		f"""
module {module_id}

export {{ Point }}

pub struct Point {{ x: Int, y: Int }}

fn make() returns Point {{
	return Point(x = 1, y = 2)
}}
""".lstrip(),
	)
	pkg_path = tmp_path / "point.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "point.drift"),
				*_emit_pkg_args(module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _emit_point_pkg(tmp_path: Path, *, module_id: str) -> Path:
	"""
	Emit a package that exports a `struct Point` and an exported constructor-like `make()`.

	This is used to validate module-scoped nominal type identity across multiple
	package-provided modules that share the same short type name.
	"""
	module_dir = tmp_path.joinpath(*module_id.split("."))
	_write_file(
		module_dir / "point.drift",
		f"""
module {module_id}

export {{ Point, make }}

pub struct Point {{ x: Int, y: Int }}

pub fn make() returns Point {{
	return Point(x = 1, y = 0)
}}
""".lstrip(),
	)
	pkg_path = tmp_path / f"{module_id.replace('.', '_')}.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "point.drift"),
				*_emit_pkg_args(module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _emit_optional_variant_pkg(
	tmp_path: Path,
	*,
	module_id: str = "acme.opt",
	extra_arm: bool = False,
	pkg_name: str | None = None,
	package_id: str | None = None,
) -> Path:
	"""
	Emit a package that exports a generic `variant Optional<T>` and a function
	that returns `Optional<Int>`.

	This is used to validate package TypeTable linking for variants.
	"""
	module_dir = tmp_path.joinpath(*module_id.split("."))
	arms = (
		"""
	Some(value: T),
	None,
	Extra
""".lstrip()
		if extra_arm
		else """
	Some(value: T),
	None
""".lstrip()
	)
	_write_file(
		module_dir / "opt.drift",
		f"""
module {module_id}

export {{ Optional, foo }}

pub variant Optional<T> {{
{arms}
}}

pub fn foo() returns Optional<Int> {{
	return Some(41)
}}
""".lstrip(),
	)
	pkg_path = tmp_path / (pkg_name or f"{module_id.replace('.', '_')}.dmp")
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "opt.drift"),
				*_emit_pkg_args(package_id or module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _emit_exception_pkg(tmp_path: Path, *, module_id: str = "acme.exc") -> Path:
	"""
Emit a package that exports an exception type (in the type namespace).

We include a dummy function so the module has at least one signature/MIR body and
is emitted into the package.
	"""
	module_dir = tmp_path.joinpath(*module_id.split("."))
	_write_file(
		module_dir / "exc.drift",
		f"""
module {module_id}

export {{ Boom }}

pub exception Boom(a: Int, b: String)

fn dummy() returns Int {{
	return 0
}}
""".lstrip(),
	)
	pkg_path = tmp_path / f"{module_id.replace('.', '_')}.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(module_dir / "exc.drift"),
				*_emit_pkg_args(module_id),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	return pkg_path


def _write_trust_store(path: Path, *, kid: str, pub_b64: str, ns: str = "acme.*", revoked: list[str] | None = None) -> None:
	revoked = revoked or []
	obj = {
		"format": "drift-trust",
		"version": 0,
		"keys": {
			kid: {"algo": "ed25519", "pubkey": pub_b64},
		},
		"namespaces": {
			ns: [kid],
		},
		"revoked": revoked,
	}
	_write_file(path, json.dumps(obj, separators=(",", ":"), sort_keys=True))


def _write_sig_sidecar(
	pkg_path: Path,
	*,
	pkg_bytes: bytes,
	kid: str,
	sig_raw: bytes,
	pub_b64: str | None = None,
	package_sha256_override: str | None = None,
	extra_entries: list[dict] | None = None,
) -> Path:
	pkg_sha_hex = package_sha256_override or _sha256_hex(pkg_bytes)
	entry = {"algo": "ed25519", "kid": kid, "sig": _b64(sig_raw)}
	if pub_b64 is not None:
		entry["pubkey"] = pub_b64
	sigs = [entry]
	if extra_entries:
		sigs.extend(extra_entries)
	sidecar = Path(str(pkg_path) + ".sig")
	obj = {
		"format": "dmir-pkg-sig",
		"version": 0,
		"package_sha256": f"sha256:{pkg_sha_hex}",
		"signatures": sigs,
	}
	_write_file(sidecar, json.dumps(obj, separators=(",", ":"), sort_keys=True))
	return sidecar


def _make_signed_package(tmp_path: Path) -> _SignedPkg:
	priv = Ed25519PrivateKey.generate()
	pub_raw = priv.public_key().public_bytes_raw()
	kid = compute_ed25519_kid(pub_raw)
	pub_b64 = _b64(pub_raw)

	pkg_path = _emit_lib_pkg(tmp_path)
	pkg_bytes = pkg_path.read_bytes()
	sig_raw = priv.sign(pkg_bytes)
	_write_sig_sidecar(pkg_path, pkg_bytes=pkg_bytes, kid=kid, sig_raw=sig_raw)

	trust_path = tmp_path / "trust.json"
	_write_trust_store(trust_path, kid=kid, pub_b64=pub_b64)
	return _SignedPkg(root=tmp_path, pkg_path=pkg_path, trust_path=trust_path, kid=kid, pub_b64=pub_b64)


def _run_driftc_json(argv: list[str], capsys: pytest.CaptureFixture[str]) -> tuple[int, dict]:
	rc = driftc_main(argv + ["--json"])
	out = capsys.readouterr().out
	payload = json.loads(out) if out.strip() else {}
	return rc, payload


def test_emit_package_is_deterministic(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)

	out1 = tmp_path / "p1.dmp"
	out2 = tmp_path / "p2.dmp"

	argv_common = [
		"-M",
		str(tmp_path),
		str(tmp_path / "main.drift"),
		str(tmp_path / "lib" / "lib.drift"),
		*_emit_pkg_args("test.determinism"),
		"--emit-package",
	]
	assert driftc_main(argv_common + [str(out1)]) == 0
	assert driftc_main(argv_common + [str(out2)]) == 0

	assert out1.read_bytes() == out2.read_bytes()

	pkg = load_package_v0(out1)
	assert pkg.manifest["payload_kind"] == "provisional-dmir"
	assert pkg.manifest["payload_version"] == 0
	assert pkg.manifest["unstable_format"] is True


def test_emit_package_is_deterministic_with_permuted_package_roots(tmp_path: Path) -> None:
	"""
	Determinism guard: package root CLI ordering must not affect build output.

	This locks that `--package-root A --package-root B` yields identical package
	bytes as `--package-root B --package-root A` when inputs are the same.
	"""
	src_root = tmp_path / "src"
	pkgs_a = tmp_path / "pkgs_a"
	pkgs_b = tmp_path / "pkgs_b"
	src_root.mkdir(parents=True, exist_ok=True)
	pkgs_a.mkdir(parents=True, exist_ok=True)
	pkgs_b.mkdir(parents=True, exist_ok=True)

	# Build two packages under separate roots.
	_emit_lib_pkg(pkgs_a, module_id="acme.liba")
	_emit_optional_variant_pkg(pkgs_b, module_id="acme.optb")

	_write_file(
		src_root / "main.drift",
		"""
module main

import acme.liba as liba
import acme.optb as optb

fn main() returns Int {
	val x = liba.add(40, 2)
	val y: optb.Optional<Int> = optb.foo()
	val z = match y {
		Some(v) => { v }
		default => { 0 }
	}
	return x + z
}
""".lstrip(),
	)

	out1 = tmp_path / "out_ab.dmp"
	out2 = tmp_path / "out_ba.dmp"

	argv_common = ["-M", str(src_root), str(src_root / "main.drift")]
	argv_ab = argv_common + [
		"--package-root",
		str(pkgs_a),
		"--package-root",
		str(pkgs_b),
		"--allow-unsigned-from",
		str(pkgs_a),
		"--allow-unsigned-from",
		str(pkgs_b),
	]
	argv_ba = argv_common + [
		"--package-root",
		str(pkgs_b),
		"--package-root",
		str(pkgs_a),
		"--allow-unsigned-from",
		str(pkgs_a),
		"--allow-unsigned-from",
		str(pkgs_b),
	]

	assert driftc_main(argv_ab + [*_emit_pkg_args("test.determinism"), "--emit-package", str(out1)]) == 0
	assert driftc_main(argv_ba + [*_emit_pkg_args("test.determinism"), "--emit-package", str(out2)]) == 0
	assert out1.read_bytes() == out2.read_bytes()


def test_emit_package_is_deterministic_with_changed_package_filenames(tmp_path: Path) -> None:
	"""
	Determinism guard: package discovery ordering (filename sorting / rglob order)
	must not affect build output.
	"""
	src_root = tmp_path / "src"
	pkgs = tmp_path / "pkgs"
	src_root.mkdir(parents=True, exist_ok=True)
	pkgs.mkdir(parents=True, exist_ok=True)

	_emit_lib_pkg(pkgs, module_id="acme.liba")
	_emit_optional_variant_pkg(pkgs, module_id="acme.optb")

	_write_file(
		src_root / "main.drift",
		"""
module main

import acme.liba as liba
import acme.optb as optb

fn main() returns Int {
	val x = liba.add(40, 2)
	val y: optb.Optional<Int> = optb.foo()
	val z = match y {
		Some(v) => { v }
		default => { 0 }
	}
	return x + z
}
""".lstrip(),
	)

	out1 = tmp_path / "out_before.dmp"
	out2 = tmp_path / "out_after.dmp"
	argv = [
		"-M",
		str(src_root),
		str(src_root / "main.drift"),
		"--package-root",
		str(pkgs),
		"--allow-unsigned-from",
		str(pkgs),
		*_emit_pkg_args("test.determinism"),
		"--emit-package",
	]

	assert driftc_main(argv + [str(out1)]) == 0

	# Rename package files to flip sorted discovery order.
	# The content is identical; only the filesystem ordering changes.
	(pkg1, pkg2) = (pkgs / "lib.dmp", pkgs / "acme_optb.dmp")
	assert pkg1.exists()
	assert pkg2.exists()
	pkg1.rename(pkgs / "z_lib.dmp")
	pkg2.rename(pkgs / "a_optb.dmp")

	assert driftc_main(argv + [str(out2)]) == 0
	assert out1.read_bytes() == out2.read_bytes()


def test_emit_package_is_deterministic_with_three_packages_and_derived_types(tmp_path: Path) -> None:
	"""
	Determinism stress test:
	- multiple single-module packages in one root (discovery order perturbations),
	- derived/instantiated types created in the consuming build (Optional<geom.Point>),
	- output bytes must remain identical.
	"""
	src_root = tmp_path / "src"
	pkgs = tmp_path / "pkgs"
	src_root.mkdir(parents=True, exist_ok=True)
	pkgs.mkdir(parents=True, exist_ok=True)

	_emit_lib_pkg(pkgs, module_id="acme.liba")
	_emit_point_pkg(pkgs, module_id="acme.geom")
	_emit_optional_variant_pkg(pkgs, module_id="acme.opt")

	_write_file(
		src_root / "main.drift",
		"""
module main

import acme.geom as g

import acme.liba as liba
import acme.opt as opt

fn main() returns Int {
	val p: g.Point = g.make()
	val o: opt.Optional<g.Point> = Some(p)
	val x = match o {
		Some(v) => { v.x }
		default => { 0 }
	}
	return liba.add(40, 2) + x
}
""".lstrip(),
	)

	out1 = tmp_path / "out_before.dmp"
	out2 = tmp_path / "out_after.dmp"
	argv = [
		"-M",
		str(src_root),
		str(src_root / "main.drift"),
		"--package-root",
		str(pkgs),
		"--allow-unsigned-from",
		str(pkgs),
		*_emit_pkg_args("test.determinism"),
		"--emit-package",
	]

	assert driftc_main(argv + [str(out1)]) == 0

	# Rename package files to change discovery order.
	(pkg1, pkg2, pkg3) = (pkgs / "lib.dmp", pkgs / "acme_geom.dmp", pkgs / "acme_opt.dmp")
	assert pkg1.exists()
	assert pkg2.exists()
	assert pkg3.exists()
	pkg1.rename(pkgs / "z_lib.dmp")
	pkg2.rename(pkgs / "m_geom.dmp")
	pkg3.rename(pkgs / "a_opt.dmp")

	assert driftc_main(argv + [str(out2)]) == 0
	assert out1.read_bytes() == out2.read_bytes()


def test_load_package_v0_round_trip(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)

	out = tmp_path / "p.dmp"
	assert driftc_main(
		[
			"-M",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			str(tmp_path / "lib" / "lib.drift"),
			*_emit_pkg_args("test.roundtrip"),
			"--emit-package",
			str(out),
		]
	) == 0

	pkg = load_package_v0(out)
	assert pkg.manifest["payload_kind"] == "provisional-dmir"
	assert set(pkg.modules_by_id.keys()) >= {"lib", "main"}

	lib_iface = pkg.modules_by_id["lib"].interface
	assert lib_iface["module_id"] == "lib"
	assert "add" in lib_iface["exports"]["values"]


def test_load_package_rejects_bad_blob_hash(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_path = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)

	# Load once to discover a concrete blob offset, then corrupt the blob bytes.
	pkg_ok = load_package_v0(pkg_path)
	assert pkg_ok.toc, "package should have at least one blob"
	blob = pkg_ok.toc[0]
	# Flip one byte at the start of the blob.
	orig = pkg_path.read_bytes()[blob.offset : blob.offset + 1]
	_patch_file_bytes(pkg_path, blob.offset, bytes([orig[0] ^ 0xFF]))

	with pytest.raises(ValueError, match="blob sha256 mismatch for"):
		load_package_v0(pkg_path)


def test_load_package_rejects_bad_manifest_hash(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_path = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)

	_patch_pkg_header(pkg_path, manifest_sha256=b"\0" * 32)
	with pytest.raises(ValueError, match="manifest sha256 mismatch"):
		load_package_v0(pkg_path)


def test_load_package_rejects_bad_toc_hash(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_path = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)

	_patch_pkg_header(pkg_path, toc_sha256=b"\0" * 32)
	with pytest.raises(ValueError, match="toc sha256 mismatch"):
		load_package_v0(pkg_path)


def test_load_package_rejects_duplicate_toc_blob_hash(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_path = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)

	# Duplicate the first TOC entry's sha256 into the second entry.
	header_bytes = pkg_path.read_bytes()[: dmir_pkg_v0.HEADER_SIZE_V0]
	(
		_magic,
		_version,
		_flags,
		_header_size,
		manifest_len,
		_manifest_sha,
		toc_len,
		_toc_entry_size,
		_toc_sha,
		_reserved,
	) = dmir_pkg_v0._HEADER_STRUCT.unpack(header_bytes)
	assert toc_len >= 2
	toc_start = dmir_pkg_v0.HEADER_SIZE_V0 + int(manifest_len)
	first_entry_off = toc_start
	second_entry_off = toc_start + dmir_pkg_v0.TOC_ENTRY_SIZE_V0
	first_sha = pkg_path.read_bytes()[first_entry_off : first_entry_off + 32]
	_patch_file_bytes(pkg_path, second_entry_off, first_sha)

	# Update toc_sha256 in the header so we reach TOC parsing.
	toc_bytes = pkg_path.read_bytes()[toc_start : toc_start + int(toc_len) * dmir_pkg_v0.TOC_ENTRY_SIZE_V0]
	_patch_pkg_header(pkg_path, toc_sha256=dmir_pkg_v0.sha256_bytes(toc_bytes))

	with pytest.raises(ValueError, match="duplicate blob sha256 in toc"):
		load_package_v0(pkg_path)


def test_driftc_rejects_duplicate_module_id_across_packages(tmp_path: Path) -> None:
	# Create two packages that both provide module `lib`.
	for n in (1, 2):
		root = tmp_path / f"p{n}"
		_write_file(
			root / "lib" / "lib.drift",
			f"""
module lib

export {{ add }}

pub fn add(a: Int, b: Int) returns Int {{
	return a + b + {n}
}}
""".lstrip(),
		)
		pkg = tmp_path / f"lib{n}.dmp"
		assert (
			driftc_main(
				[
					"-M",
					str(root),
					str(root / "lib" / "lib.drift"),
					*_emit_pkg_args(f"test.lib{n}"),
					"--emit-package",
					str(pkg),
				]
			)
			== 0
		)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		]
	)
	assert rc != 0


def test_driftc_can_consume_package_with_additional_types_via_type_table_linking(tmp_path: Path) -> None:
	# Package defines an extra user type that is not present in the consuming build,
	# and exports it across the module boundary. The consumer should succeed via
	# link-time TypeTable unification + TypeId remapping.
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { S, make }

pub struct S(x: Int)

pub fn make() returns S {
	return S(x = 42)
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	val s: lib.S = lib.make()
	return s.x
}
""".lstrip(),
	)
	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		]
	)
	assert rc == 0


def test_package_embedding_includes_only_call_graph_closure(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}

fn unused() returns Int {
	return 999
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	ir_path = tmp_path / "out.ll"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				"--package-root",
				str(tmp_path),
				"--allow-unsigned-from",
				str(tmp_path),
				str(tmp_path / "main.drift"),
				"--emit-ir",
				str(ir_path),
			]
		)
		== 0
	)
	ir = ir_path.read_text(encoding="utf-8")
	assert "lib::unused" not in ir
	# Exported functions are ABI boundary entrypoints: they are emitted as
	# `FnResult<ok, Error*>` wrappers, with a private `__impl` body that keeps
	# the internal calling convention.
	assert "define %FnResult_Int_Error @\"lib::add\"" in ir
	assert "define i64 @\"lib::add__impl\"" in ir
	assert "define i64 @lib::unused" not in ir


def test_discover_package_files_accepts_package_file_path(tmp_path: Path) -> None:
	pkg = tmp_path / "one.dmp"
	pkg.write_bytes(b"")
	assert discover_package_files([pkg]) == [pkg]


def test_driftc_rejects_unsigned_package_outside_allowlist(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_root = tmp_path / "pkgs"
	pkg_root.mkdir(parents=True, exist_ok=True)
	pkg = pkg_root / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)

	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(pkg_root),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		]
	)
	assert rc != 0


def test_driftc_missing_explicit_trust_store_is_reported_as_diagnostic(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	_write_file(
		tmp_path / "main.drift",
		"""
module main

fn main() returns Int {
	return 0
}
""".lstrip(),
	)
	missing = tmp_path / "nope-trust.json"
	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(missing),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
			"--json",
		]
	)
	assert rc != 0
	out = capsys.readouterr().out
	obj = json.loads(out)
	assert obj["exit_code"] == 1
	assert obj["diagnostics"][0]["phase"] == "package"
	assert "trust store not found" in obj["diagnostics"][0]["message"]


def test_driftc_accepts_signed_package_when_required(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)

	ir_path = tmp_path / "out.ll"
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(ir_path),
		],
		capsys,
	)
	assert rc == 0
	assert payload.get("exit_code") == 0
	assert payload.get("diagnostics") == []


def test_driftc_rejects_signature_missing_module_in_strict_mode(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	pkg_path = _emit_lib_pkg(tmp_path, module_id="acme.badmod")
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["acme.badmod"]

	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	def _strip_sig_module(obj: dict) -> dict[str, dict]:
		sigs = dict(obj.get("signatures") or {})
		add_key = "acme.badmod::add"
		sd = dict(sigs.get(add_key) or {})
		sd.pop("module", None)
		sd["name"] = "add"
		return {"add": sd}

	payload_obj["signatures"] = _strip_sig_module(payload_obj)
	iface_obj["signatures"] = {}

	iface_exports = dict(iface_obj.get("exports") or {})
	iface_exports["values"] = []
	iface_obj["exports"] = iface_exports
	payload_exports = dict(payload_obj.get("exports") or {})
	payload_exports["values"] = []
	payload_obj["exports"] = payload_exports

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	write_dmir_pkg_v0(
		pkg_path,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "acme.badmod",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "acme.badmod",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:acme.badmod", payload_sha: "dmir:acme.badmod"},
	)

	priv = Ed25519PrivateKey.generate()
	pub_raw = priv.public_key().public_bytes_raw()
	kid = compute_ed25519_kid(pub_raw)
	pub_b64 = _b64(pub_raw)
	pkg_bytes = pkg_path.read_bytes()
	sig_raw = priv.sign(pkg_bytes)
	_write_sig_sidecar(pkg_path, pkg_bytes=pkg_bytes, kid=kid, sig_raw=sig_raw)

	trust_path = tmp_path / "trust.json"
	_write_trust_store(trust_path, kid=kid, pub_b64=pub_b64)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.badmod as badmod

fn main() returns Int {
	return 0
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "missing module" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_missing_sidecar_when_signatures_required(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	# Emit an unsigned package but do not write a `.sig` file.
	pkg_path = _emit_lib_pkg(tmp_path)
	trust_path = tmp_path / "trust.json"
	_write_trust_store(trust_path, kid="ed25519:dummy", pub_b64=_b64(b"\0" * 32))

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "missing signature sidecar" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_malformed_signature_sidecar_json(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)
	Path(str(signed.pkg_path) + ".sig").write_text("{", encoding="utf-8")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "invalid JSON" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_sidecar_package_sha_mismatch(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)
	pkg_bytes = signed.pkg_path.read_bytes()
	bad_sha = "0" * 64
	_write_sig_sidecar(signed.pkg_path, pkg_bytes=pkg_bytes, kid=signed.kid, sig_raw=b"\0" * 64, package_sha256_override=bad_sha)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "package_sha256 mismatch" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_sidecar_invalid_base64(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)
	sidecar = Path(str(signed.pkg_path) + ".sig")
	obj = json.loads(sidecar.read_text(encoding="utf-8"))
	obj["signatures"][0]["sig"] = "!!!"
	sidecar.write_text(json.dumps(obj, separators=(",", ":"), sort_keys=True), encoding="utf-8")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "invalid base64" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_sidecar_wrong_sig_length(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)
	sidecar = Path(str(signed.pkg_path) + ".sig")
	obj = json.loads(sidecar.read_text(encoding="utf-8"))
	obj["signatures"][0]["sig"] = _b64(b"\0" * 63)
	sidecar.write_text(json.dumps(obj, separators=(",", ":"), sort_keys=True), encoding="utf-8")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "signature must be 64 bytes" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_signed_package_when_kid_revoked(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	priv = Ed25519PrivateKey.generate()
	pub_raw = priv.public_key().public_bytes_raw()
	kid = compute_ed25519_kid(pub_raw)
	pub_b64 = _b64(pub_raw)
	pkg_path = _emit_lib_pkg(tmp_path)
	pkg_bytes = pkg_path.read_bytes()
	sig_raw = priv.sign(pkg_bytes)
	_write_sig_sidecar(pkg_path, pkg_bytes=pkg_bytes, kid=kid, sig_raw=sig_raw)

	trust_path = tmp_path / "trust.json"
	_write_trust_store(trust_path, kid=kid, pub_b64=pub_b64, revoked=[kid])

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	msg = str(payload["diagnostics"][0]["message"])
	assert ("no valid signatures" in msg) or ("revoked" in msg.lower())


def test_driftc_accepts_if_any_signature_entry_is_valid(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	pkg_path = _emit_lib_pkg(tmp_path)
	pkg_bytes = pkg_path.read_bytes()

	# Two keys: first signature is invalid, second is valid. Both are trusted.
	priv1 = Ed25519PrivateKey.generate()
	pub1 = priv1.public_key().public_bytes_raw()
	kid1 = compute_ed25519_kid(pub1)
	priv2 = Ed25519PrivateKey.generate()
	pub2 = priv2.public_key().public_bytes_raw()
	kid2 = compute_ed25519_kid(pub2)

	invalid_sig = b"\0" * 64
	valid_sig = priv2.sign(pkg_bytes)

	# Sidecar includes both signatures (no pubkey needed; trust store provides it).
	_write_sig_sidecar(
		pkg_path,
		pkg_bytes=pkg_bytes,
		kid=kid1,
		sig_raw=invalid_sig,
		extra_entries=[{"algo": "ed25519", "kid": kid2, "sig": _b64(valid_sig)}],
	)

	trust_path = tmp_path / "trust.json"
	obj = {
		"format": "drift-trust",
		"version": 0,
		"keys": {
			kid1: {"algo": "ed25519", "pubkey": _b64(pub1)},
			kid2: {"algo": "ed25519", "pubkey": _b64(pub2)},
		},
		"namespaces": {
			"acme.*": [kid1, kid2],
		},
		"revoked": [],
	}
	_write_file(trust_path, json.dumps(obj, separators=(",", ":"), sort_keys=True))

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc == 0
	assert payload["exit_code"] == 0
	assert payload["diagnostics"] == []


def test_driftc_rejects_valid_signature_when_kid_not_trusted(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	# Signed package exists, but the trust store does not contain the kid/pubkey.
	# driftc must not TOFU from sidecar pubkey bytes.
	priv = Ed25519PrivateKey.generate()
	pub_raw = priv.public_key().public_bytes_raw()
	kid = compute_ed25519_kid(pub_raw)
	pub_b64 = _b64(pub_raw)

	pkg_path = _emit_lib_pkg(tmp_path)
	pkg_bytes = pkg_path.read_bytes()
	sig_raw = priv.sign(pkg_bytes)
	_write_sig_sidecar(pkg_path, pkg_bytes=pkg_bytes, kid=kid, sig_raw=sig_raw, pub_b64=pub_b64)

	# Trust store does not contain the key (keys table empty), even though it
	# claims the namespace would allow it.
	trust_path = tmp_path / "trust.json"
	obj = {
		"format": "drift-trust",
		"version": 0,
		"keys": {},
		"namespaces": {"acme.*": [kid]},
		"revoked": [],
	}
	_write_file(trust_path, json.dumps(obj, separators=(",", ":"), sort_keys=True))

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "no valid signatures" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_valid_signature_when_namespace_disallows_kid(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	# Signed package exists and kid is in trust store, but namespace allowlist
	# does not include the kid.
	priv = Ed25519PrivateKey.generate()
	pub_raw = priv.public_key().public_bytes_raw()
	kid = compute_ed25519_kid(pub_raw)
	pub_b64 = _b64(pub_raw)

	other_priv = Ed25519PrivateKey.generate()
	other_pub = other_priv.public_key().public_bytes_raw()
	other_kid = compute_ed25519_kid(other_pub)

	pkg_path = _emit_lib_pkg(tmp_path)
	pkg_bytes = pkg_path.read_bytes()
	sig_raw = priv.sign(pkg_bytes)
	_write_sig_sidecar(pkg_path, pkg_bytes=pkg_bytes, kid=kid, sig_raw=sig_raw)

	trust_path = tmp_path / "trust.json"
	obj = {
		"format": "drift-trust",
		"version": 0,
		"keys": {
			kid: {"algo": "ed25519", "pubkey": pub_b64},
			other_kid: {"algo": "ed25519", "pubkey": _b64(other_pub)},
		},
		# Allow only the other key for the namespace.
		"namespaces": {"acme.*": [other_kid]},
		"revoked": [],
	}
	_write_file(trust_path, json.dumps(obj, separators=(",", ":"), sort_keys=True))

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "not trusted for module" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_sidecar_wrong_pubkey_length(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)
	sidecar = Path(str(signed.pkg_path) + ".sig")
	obj = json.loads(sidecar.read_text(encoding="utf-8"))
	obj["signatures"][0]["pubkey"] = _b64(b"\0" * 31)
	sidecar.write_text(json.dumps(obj, separators=(",", ":"), sort_keys=True), encoding="utf-8")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "pubkey must be 32 bytes" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_sidecar_invalid_pubkey_base64(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	signed = _make_signed_package(tmp_path)
	sidecar = Path(str(signed.pkg_path) + ".sig")
	obj = json.loads(sidecar.read_text(encoding="utf-8"))
	obj["signatures"][0]["pubkey"] = "!!!"
	sidecar.write_text(json.dumps(obj, separators=(",", ":"), sort_keys=True), encoding="utf-8")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--trust-store",
			str(signed.trust_path),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "invalid base64" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_unsigned_package_without_manifest_marker(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_root = tmp_path / "pkgs"
	pkg_root.mkdir(parents=True, exist_ok=True)
	pkg = pkg_root / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	# Remove the "unsigned": true marker without changing manifest length.
	def patch_manifest(old: bytes) -> bytes:
		needle = b"\"unsigned\":true"
		if needle not in old:
			raise ValueError("expected unsigned marker in manifest")
		return old.replace(needle, b"\"unsigned\":null")

	_patch_pkg_manifest_bytes_same_len(pkg, patch_manifest)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(pkg_root),
			"--allow-unsigned-from",
			str(pkg_root),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		]
	)
	assert rc != 0


def test_driftc_can_consume_package_exporting_generic_variant_optional(tmp_path: Path) -> None:
	# Package exports a generic variant and a function returning an instantiation.
	_emit_optional_variant_pkg(tmp_path)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.opt as opt

fn main() returns Int {
	val x: opt.Optional<Int> = opt.foo()
	val y = match x {
		Some(v) => { v + 1 }
		None => { 0 }
	}
	return y
}
""".lstrip(),
	)

	ir_path = tmp_path / "out.ll"
	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(ir_path),
		]
	)
	assert rc == 0


def test_driftc_rejects_variant_schema_collision_between_source_and_package(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	# Package defines `variant Optional<T>` in module `acme.opt`, while source defines
	# a different `variant Optional<T>` in module `main`. With module-scoped nominal
	# type identity, these are distinct and must not collide.
	_emit_optional_variant_pkg(tmp_path)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

// Collides by name with the package's `Optional<T>` schema.
variant Optional<T> {
	Some(value: T),
	None,
	Extra
}

fn main() returns Int {
	return 0
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc == 0
	assert payload["exit_code"] == 0
	assert payload["diagnostics"] == []


def test_driftc_rejects_variant_schema_collision_between_packages(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	# The same module id must not be provided by multiple packages.
	_emit_optional_variant_pkg(tmp_path, module_id="acme.opt", pkg_name="opt_a.dmp", package_id="test.opt_a")
	_emit_optional_variant_pkg(
		tmp_path, module_id="acme.opt", extra_arm=True, pkg_name="opt_b.dmp", package_id="test.opt_b"
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

fn main() returns Int {
	return 0
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "provided by multiple packages" in payload["diagnostics"][0]["message"]
	assert "acme.opt" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_import_of_non_exported_value_from_package(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	_emit_hidden_fn_pkg(tmp_path)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.hidden as hidden

fn main() returns Int {
	return hidden.hidden()
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "parser"
	assert "does not export symbol 'hidden'" in payload["diagnostics"][0]["message"]


def test_driftc_allows_import_of_exported_const_from_package(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	_emit_const_pkg(tmp_path)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.consts as consts

fn main() returns Int {
	return consts.ANSWER
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc == 0
	assert payload["exit_code"] == 0
	assert payload["diagnostics"] == []


def test_driftc_allows_import_of_exported_type_but_rejects_non_exported_value_from_package(
	tmp_path: Path, capsys: pytest.CaptureFixture[str]
) -> None:
	_emit_point_type_only_pkg(tmp_path)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.point as point

fn main() returns Int {
	val p: point.Point = point.make()
	return p.x
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "parser"
	assert "does not export symbol 'make'" in payload["diagnostics"][0]["message"]


def test_driftc_allows_two_modules_with_same_struct_name_from_packages(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	_emit_point_pkg(tmp_path, module_id="a.geom")
	_emit_point_pkg(tmp_path, module_id="b.geom")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import a.geom as ag
import b.geom as bg

fn main() returns Int {
	val p1: ag.Point = ag.make()
	val p2: bg.Point = bg.make()
	return p1.x + p2.x
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc == 0
	assert payload["exit_code"] == 0


def test_driftc_rejects_package_with_exported_value_missing_entrypoint_flag(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	"""
	ABI-boundary invariant: exported values must correspond to entrypoint signatures.

	This constructs a malformed package where the interface exports `add`, but the
	payload signature for `add` is not marked as an exported entrypoint.
	"""
	pkg_path = _emit_lib_pkg(tmp_path, module_id="acme.badiface")
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["acme.badiface"]

	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	sigs = dict(payload_obj.get("signatures") or {})
	add_key = "acme.badiface::add"
	sd = dict(sigs.get(add_key) or {})
	sd["is_exported_entrypoint"] = False
	sigs[add_key] = sd
	payload_obj["signatures"] = sigs
	# Keep interface and payload signatures consistent; the interface table is
	# strict and must match the payload.
	iface_sigs = dict(iface_obj.get("signatures") or {})
	iface_sd = dict(iface_sigs.get(add_key) or {})
	iface_sd["is_exported_entrypoint"] = False
	iface_sigs[add_key] = iface_sd
	iface_obj["signatures"] = iface_sigs

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	out_pkg = pkg_path
	write_dmir_pkg_v0(
		out_pkg,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "acme.badiface",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "acme.badiface",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:acme.badiface", payload_sha: "dmir:acme.badiface"},
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.badiface as badiface

fn main() returns Int {
	return badiface.add(40, 2)
}
""".lstrip(),
	)

	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "exported value 'add' is missing exported entrypoint signature metadata" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_package_with_exported_value_missing_interface_signature(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	"""
	Interface table tightening: exported values must have interface signature entries.
	"""
	pkg_path = _emit_lib_pkg(tmp_path, module_id="acme.badiface2")
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["acme.badiface2"]

	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	add_key = "acme.badiface2::add"
	iface_sigs = dict(iface_obj.get("signatures") or {})
	iface_sigs.pop(add_key, None)
	iface_obj["signatures"] = iface_sigs

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	out_pkg = pkg_path
	write_dmir_pkg_v0(
		out_pkg,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "acme.badiface2",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "acme.badiface2",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:acme.badiface2", payload_sha: "dmir:acme.badiface2"},
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.badiface2 as badiface2

fn main() returns Int {
	return badiface2.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "missing interface signature metadata" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_package_with_exports_mismatch_between_interface_and_payload(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	"""
	Interface table tightening: interface exports must match payload exports exactly.
	"""
	pkg_path = _emit_lib_pkg(tmp_path, module_id="acme.badiface3")
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["acme.badiface3"]

	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	# Remove an exported value from the interface, leaving payload unchanged.
	exports = dict(iface_obj.get("exports") or {})
	values = list(exports.get("values") or [])
	values = [v for v in values if v != "add"]
	exports["values"] = values
	iface_obj["exports"] = exports
	# Also keep interface signature table consistent with its exports.
	iface_sigs = dict(iface_obj.get("signatures") or {})
	iface_sigs.pop("acme.badiface3::add", None)
	iface_obj["signatures"] = iface_sigs

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	out_pkg = pkg_path
	write_dmir_pkg_v0(
		out_pkg,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "acme.badiface3",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "acme.badiface3",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:acme.badiface3", payload_sha: "dmir:acme.badiface3"},
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.badiface3 as badiface3

fn main() returns Int {
	return badiface3.add(40, 2)
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "interface exports do not match payload exports" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_package_with_exported_exception_missing_schema(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	"""
Interface completeness: exported exceptions must have interface schema entries.
	"""
	pkg_path = _emit_exception_pkg(tmp_path, module_id="acme.badexc")
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["acme.badexc"]

	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	iface_exc = dict(iface_obj.get("exception_schemas") or {})
	# Remove the exported exception schema entry.
	iface_exc.pop("acme.badexc:Boom", None)
	iface_obj["exception_schemas"] = iface_exc

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	write_dmir_pkg_v0(
		pkg_path,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "acme.badexc",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "acme.badexc",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:acme.badexc", payload_sha: "dmir:acme.badexc"},
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.badexc as badexc

fn main() returns Int {
	return 0
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "missing interface schema" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_package_with_exported_variant_missing_schema(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	"""
Interface completeness: exported variants must have interface schema entries.
	"""
	pkg_path = _emit_optional_variant_pkg(tmp_path, module_id="acme.badvar")
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["acme.badvar"]

	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	iface_var = dict(iface_obj.get("variant_schemas") or {})
	iface_var.pop("Optional", None)
	iface_obj["variant_schemas"] = iface_var

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	write_dmir_pkg_v0(
		pkg_path,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "acme.badvar",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "acme.badvar",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:acme.badvar", payload_sha: "dmir:acme.badvar"},
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import acme.badvar as badvar

fn main() returns Int {
	val o: badvar.Optional<Int> = None
	return 0
}
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "missing interface schema" in payload["diagnostics"][0]["message"]


def test_driftc_rejects_package_exporting_method_value(tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
	"""
MVP guardrail: exported methods are forbidden.
	"""
	_write_file(
		tmp_path / "m" / "lib.drift",
		"""
module m

export { Point }

pub struct Point { x: Int }

pub implement Point {
	fn move_by(self: &mut Point, dx: Int) returns Void {
		self->x += dx;
	}
}

fn dummy() returns Int { return 0; }
""".lstrip(),
	)
	pkg_path = tmp_path / "m.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "m" / "lib.drift"),
				*_emit_pkg_args("m"),
				"--emit-package",
				str(pkg_path),
			]
		)
		== 0
	)
	pkg = load_package_v0(pkg_path)
	mod = pkg.modules_by_id["m"]
	iface_obj = dict(mod.interface)
	payload_obj = dict(mod.payload)

	# Identify the method symbol key in the payload signatures.
	payload_sigs = dict(payload_obj.get("signatures") or {})
	method_sym = None
	for k, sd in payload_sigs.items():
		if isinstance(sd, dict) and sd.get("is_method") and "move_by" in k:
			method_sym = str(k)
			break
	assert method_sym is not None
	local_name = method_sym.split("::", 1)[1]

	# Malform the package: export the method as a value.
	exports = dict(payload_obj.get("exports") or {})
	values = list(exports.get("values") or [])
	if local_name not in values:
		values.append(local_name)
	exports["values"] = values
	payload_obj["exports"] = exports

	# Mark the method signature as an exported entrypoint and mirror it into the
	# interface signature table so all other checks pass.
	sd = dict(payload_sigs[method_sym])
	sd["is_exported_entrypoint"] = True
	payload_sigs[method_sym] = sd
	payload_obj["signatures"] = payload_sigs

	iface_exports = dict(iface_obj.get("exports") or {})
	iface_values = list(iface_exports.get("values") or [])
	if local_name not in iface_values:
		iface_values.append(local_name)
	iface_exports["values"] = iface_values
	iface_obj["exports"] = iface_exports

	iface_sigs = dict(iface_obj.get("signatures") or {})
	iface_sigs[method_sym] = sd
	iface_obj["signatures"] = iface_sigs

	iface_bytes = canonical_json_bytes(iface_obj)
	payload_bytes = canonical_json_bytes(payload_obj)
	iface_sha = sha256_hex(iface_bytes)
	payload_sha = sha256_hex(payload_bytes)
	write_dmir_pkg_v0(
		pkg_path,
		manifest_obj={
			"format": "dmir-pkg",
			"format_version": 0,
			"package_id": "m",
			"package_version": "0.0.0",
			"target": "test-target",
			"unsigned": True,
			"unstable_format": True,
			"payload_kind": "provisional-dmir",
			"payload_version": 0,
			"modules": [
				{
					"module_id": "m",
					"exports": iface_obj.get("exports", {}),
					"interface_blob": f"sha256:{iface_sha}",
					"payload_blob": f"sha256:{payload_sha}",
				}
			],
			"blobs": {
				f"sha256:{iface_sha}": {"type": "exports", "length": len(iface_bytes)},
				f"sha256:{payload_sha}": {"type": "dmir", "length": len(payload_bytes)},
			},
		},
		blobs={iface_sha: iface_bytes, payload_sha: payload_bytes},
		blob_types={iface_sha: 2, payload_sha: 1},
		blob_names={iface_sha: "iface:m", payload_sha: "dmir:m"},
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

fn main() returns Int { return 0 }
""".lstrip(),
	)
	rc, payload = _run_driftc_json(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--allow-unsigned-from",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		],
		capsys,
	)
	assert rc != 0
	assert payload["exit_code"] == 1
	assert payload["diagnostics"][0]["phase"] == "package"
	assert "must not be a method" in payload["diagnostics"][0]["message"]

def test_driftc_require_signatures_rejects_unsigned_packages(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg_root = tmp_path / "pkgs"
	pkg_root.mkdir(parents=True, exist_ok=True)
	pkg = pkg_root / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				*_emit_pkg_args("lib"),
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	rc = driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(pkg_root),
			"--allow-unsigned-from",
			str(pkg_root),
			"--require-signatures",
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
		]
	)
	assert rc != 0

[==== File: staged/lang2/tests/driver/tests/test_drift_doctor.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import json
import subprocess
import sys
from pathlib import Path

from lang2.driftc.driftc import main as driftc_main


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def _run_drift(argv: list[str]) -> subprocess.CompletedProcess[str]:
	return subprocess.run([sys.executable, "-m", "lang2.drift", *argv], text=True, capture_output=True)


def _write_trust_store(path: Path) -> Path:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(json.dumps({"format": "drift-trust", "version": 0, "namespaces": {}, "keys": {}, "revoked": {}}), encoding="utf-8")
	return path


def test_drift_doctor_json_is_strict_json_only_and_sorted(tmp_path: Path) -> None:
	repo = tmp_path / "repo"
	repo.mkdir(parents=True, exist_ok=True)
	(repo / "index.json").write_text(json.dumps({"format": "drift-index", "version": 0, "packages": {}}), encoding="utf-8")

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)

	trust = tmp_path / "drift" / "trust.json"
	trust.parent.mkdir(parents=True, exist_ok=True)
	trust.write_text(json.dumps({"format": "drift-trust", "version": 0, "namespaces": {}, "keys": {}, "revoked": {}}), encoding="utf-8")

	lock = tmp_path / "drift.lock.json"
	lock.write_text(json.dumps({"format": "drift-lock", "version": 0, "packages": {}}), encoding="utf-8")

	cp = _run_drift(["doctor", "--sources", str(sources), "--trust-store", str(trust), "--lock", str(lock), "--json"])
	assert cp.returncode == 0
	assert (cp.stderr or "").strip() == ""
	assert cp.stdout.lstrip().startswith("{")
	report = json.loads(cp.stdout)
	assert report["ok"] is True
	assert isinstance(report["checks"], list)
	check_ids = [c["check_id"] for c in report["checks"]]
	assert check_ids == sorted(check_ids)


def test_drift_doctor_json_failure_missing_package_file_deep(tmp_path: Path) -> None:
	repo = tmp_path / "repo"
	repo.mkdir(parents=True, exist_ok=True)
	(repo / "index.json").write_text(
		json.dumps(
			{
				"format": "drift-index",
				"version": 0,
				"packages": {
					"lib": {
						"package_version": "0.0.0",
						"target": "test-target",
						"sha256": "sha256:" + ("00" * 32),
						"filename": "lib-0.0.0-test-target.dmp",
						"signers": [],
						"unsigned": True,
					}
				},
			}
		),
		encoding="utf-8",
	)
	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	trust = tmp_path / "drift" / "trust.json"
	trust.parent.mkdir(parents=True, exist_ok=True)
	trust.write_text(json.dumps({"format": "drift-trust", "version": 0, "namespaces": {}, "keys": {}, "revoked": {}}), encoding="utf-8")
	lock = tmp_path / "drift.lock.json"
	lock.write_text(json.dumps({"format": "drift-lock", "version": 0, "packages": {}}), encoding="utf-8")

	cp = _run_drift(
		["doctor", "--sources", str(sources), "--trust-store", str(trust), "--lock", str(lock), "--deep", "--json"]
	)
	assert cp.returncode == 2
	assert (cp.stderr or "").strip() == ""
	report = json.loads(cp.stdout)
	assert report["ok"] is False
	index_check = next(c for c in report["checks"] if c["check_id"] == "indexes")
	assert index_check["status"] == "fatal"
	assert any(f["reason_code"] == "INDEX_MISSING_PACKAGE_FILE" for f in index_check["findings"])


def test_drift_doctor_exit_code_degraded_vs_fatal(tmp_path: Path) -> None:
	# Missing sources file is degraded by default.
	trust = tmp_path / "drift" / "trust.json"
	trust.parent.mkdir(parents=True, exist_ok=True)
	trust.write_text(json.dumps({"format": "drift-trust", "version": 0, "namespaces": {}, "keys": {}, "revoked": {}}), encoding="utf-8")
	lock = tmp_path / "drift.lock.json"
	lock.write_text(json.dumps({"format": "drift-lock", "version": 0, "packages": {}}), encoding="utf-8")

	sources = tmp_path / "drift-sources.json"
	assert not sources.exists()

	cp = _run_drift(["doctor", "--sources", str(sources), "--trust-store", str(trust), "--lock", str(lock), "--json", "--fail-on", "fatal"])
	assert cp.returncode == 0
	report = json.loads(cp.stdout)
	assert report["degraded_count"] >= 1

	cp = _run_drift(
		["doctor", "--sources", str(sources), "--trust-store", str(trust), "--lock", str(lock), "--json", "--fail-on", "degraded"]
	)
	assert cp.returncode == 1


def test_drift_doctor_vendor_missing_artifact_deep(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	trust = _write_trust_store(tmp_path / "drift" / "trust.json")
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr

	vendor_dir = tmp_path / "vendor" / "driftpkgs"
	lock = tmp_path / "drift.lock.json"
	cp = _run_drift(
		["vendor", "--cache-dir", str(cache), "--dest-dir", str(vendor_dir), "--lock", str(lock), "--json"]
	)
	assert cp.returncode == 0, cp.stderr

	lock_obj = json.loads(lock.read_text(encoding="utf-8"))
	vendored_path = vendor_dir / lock_obj["packages"]["lib"]["path"]
	assert vendored_path.exists()
	vendored_path.unlink()

	cp = _run_drift(
		[
			"doctor",
			"--sources",
			str(sources),
			"--trust-store",
			str(trust),
			"--lock",
			str(lock),
			"--cache-dir",
			str(cache),
			"--vendor-dir",
			str(vendor_dir),
			"--json",
			"--deep",
			"--fail-on",
			"degraded",
		]
	)
	assert cp.returncode == 1
	assert (cp.stderr or "").strip() == ""
	report = json.loads(cp.stdout)
	vendor_check = next(c for c in report["checks"] if c["check_id"] == "vendor_consistency")
	assert vendor_check["status"] == "degraded"
	assert any(f["reason_code"] == "VENDOR_MISSING_ARTIFACT" and f["identity"]["package_id"] == "lib" for f in vendor_check["findings"])


def test_drift_doctor_cache_divergence_detected(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	trust = _write_trust_store(tmp_path / "drift" / "trust.json")
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr

	vendor_dir = tmp_path / "vendor" / "driftpkgs"
	lock = tmp_path / "drift.lock.json"
	cp = _run_drift(
		["vendor", "--cache-dir", str(cache), "--dest-dir", str(vendor_dir), "--lock", str(lock), "--json"]
	)
	assert cp.returncode == 0, cp.stderr

	lock_obj = json.loads(lock.read_text(encoding="utf-8"))
	cache_pkg = cache / "pkgs" / lock_obj["packages"]["lib"]["path"]
	data = bytearray(cache_pkg.read_bytes())
	assert data, "cache package should not be empty"
	data[-1] ^= 0xFF
	cache_pkg.write_bytes(bytes(data))

	common_args = [
		"doctor",
		"--sources",
		str(sources),
		"--trust-store",
		str(trust),
		"--lock",
		str(lock),
		"--cache-dir",
		str(cache),
		"--vendor-dir",
		str(vendor_dir),
		"--json",
		"--deep",
	]

	cp = _run_drift(common_args)
	assert cp.returncode == 0
	assert (cp.stderr or "").strip() == ""
	report = json.loads(cp.stdout)
	cache_check = next(c for c in report["checks"] if c["check_id"] == "cache_consistency")
	assert cache_check["status"] == "degraded"
	assert any(f["reason_code"] == "LOCK_CACHE_DIVERGENCE" for f in cache_check["findings"])

	cp_fail = _run_drift([*common_args, "--fail-on", "degraded"])
	assert cp_fail.returncode == 1
	assert (cp_fail.stderr or "").strip() == ""

[==== File: staged/lang2/tests/driver/tests/test_drift_multisig_policy.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import base64
import json
import os
import subprocess
import sys
from pathlib import Path

from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey

from lang2.drift.crypto import compute_ed25519_kid


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_driftc_accepts_sidecar_when_any_signature_valid_and_allowed(tmp_path: Path) -> None:
	"""
	Lock policy: if a sidecar contains multiple signatures, driftc accepts the
	package if any signature is valid and trusted/allowed, even if another
	signature is revoked.
	"""
	repo_root = Path.cwd()

	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	build_pkg = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.driftc.driftc",
			"-M",
			str(tmp_path),
			str(tmp_path / "lib" / "lib.drift"),
			"--package-id",
			"test.pkg",
			"--package-version",
			"0.0.0",
			"--package-target",
			"test-target",
			"--emit-package",
			str(pkg),
			"--json",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert build_pkg.returncode == 0, build_pkg.stderr
	out = json.loads(build_pkg.stdout or "{}")
	assert out.get("exit_code") == 0

	# Prepare two signing keys.
	seed1 = os.urandom(32)
	seed2 = os.urandom(32)
	key1 = tmp_path / "k1.seed"
	key2 = tmp_path / "k2.seed"
	key1.write_text(base64.b64encode(seed1).decode("ascii") + "\n", encoding="utf-8")
	key2.write_text(base64.b64encode(seed2).decode("ascii") + "\n", encoding="utf-8")

	# Compute kids/public keys (for trust store).
	priv1 = Ed25519PrivateKey.from_private_bytes(seed1)
	priv2 = Ed25519PrivateKey.from_private_bytes(seed2)
	pub1_raw = priv1.public_key().public_bytes_raw()
	pub2_raw = priv2.public_key().public_bytes_raw()
	kid1 = compute_ed25519_kid(pub1_raw)
	kid2 = compute_ed25519_kid(pub2_raw)

	# Sign once, then append a second signature.
	sign1 = subprocess.run(
		[sys.executable, "-m", "lang2.drift", "sign", str(pkg), "--key", str(key1)],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert sign1.returncode == 0, sign1.stderr
	sign2 = subprocess.run(
		[sys.executable, "-m", "lang2.drift", "sign", str(pkg), "--key", str(key2), "--add-signature"],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert sign2.returncode == 0, sign2.stderr

	# Trust both keys for lib.*, but revoke one of them.
	trust = {
		"format": "drift-trust",
		"version": 0,
		"keys": {
			kid1: {"algo": "ed25519", "pubkey": base64.b64encode(pub1_raw).decode("ascii")},
			kid2: {"algo": "ed25519", "pubkey": base64.b64encode(pub2_raw).decode("ascii")},
		},
		"namespaces": {"lib.*": [kid1, kid2]},
		"revoked": {kid1: {"reason": "test"}},
	}
	trust_path = tmp_path / "drift" / "trust.json"
	trust_path.parent.mkdir(parents=True, exist_ok=True)
	trust_path.write_text(json.dumps(trust, sort_keys=True, separators=(",", ":")) + "\n", encoding="utf-8")

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	consume = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.driftc.driftc",
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--require-signatures",
			"--trust-store",
			str(trust_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
			"--json",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert consume.returncode == 0, consume.stderr
	payload = json.loads(consume.stdout or "{}")
	assert payload.get("exit_code") == 0

[==== File: staged/lang2/tests/driver/tests/test_drift_publish_fetch_vendor.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import base64
import json
import subprocess
import sys
import shutil
import hashlib
from pathlib import Path

from cryptography.hazmat.primitives.asymmetric.ed25519 import Ed25519PrivateKey
from cryptography.hazmat.primitives import serialization

from lang2.driftc.driftc import main as driftc_main


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def _run_drift(argv: list[str]) -> subprocess.CompletedProcess[str]:
	return subprocess.run([sys.executable, "-m", "lang2.drift", *argv], text=True, capture_output=True)


def test_phase5_e2e_smoke_dir_source(tmp_path: Path) -> None:
	"""
	Phase 5 authoritative smoke: dir source as remote, lock authority, vendor path layout, doctor deep.
	"""
	src = tmp_path / "lib" / "lib.drift"
	_write_file(
		src,
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib-0.1.0-test.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(src),
				"--package-id",
				"lib",
				"--package-version",
				"0.1.0",
				"--package-target",
				"test",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr
	assert (repo / "index.json").exists()

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "origin", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)

	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr
	cache_index = json.loads((cache / "index.json").read_text(encoding="utf-8"))
	cache_path = cache_index["packages"]["lib"]["path"]
	assert cache_path and isinstance(cache_path, str)

	vendor_dir = tmp_path / "vendor" / "driftpkgs"
	lock_path = tmp_path / "drift.lock.json"
	cp = _run_drift(
		[
			"vendor",
			"--cache-dir",
			str(cache),
			"--dest-dir",
			str(vendor_dir),
			"--lock",
			str(lock_path),
		]
	)
	assert cp.returncode == 0, cp.stderr
	lock = json.loads(lock_path.read_text(encoding="utf-8"))
	assert lock["packages"]["lib"]["path"] == cache_path
	vendored_pkg = vendor_dir / lock["packages"]["lib"]["path"]
	assert vendored_pkg.exists()

	# Lock authority: delete cache and refetch with lock only.
	shutil.rmtree(cache)
	cache.mkdir(parents=True, exist_ok=True)
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--lock", str(lock_path)])
	assert cp.returncode == 0, cp.stderr

	# Doctor deep should be clean and JSON-only.
	trust = tmp_path / "drift" / "trust.json"
	trust.parent.mkdir(parents=True, exist_ok=True)
	trust.write_text(json.dumps({"format": "drift-trust", "version": 0, "namespaces": {}, "keys": {}, "revoked": {}}), encoding="utf-8")
	cp = _run_drift(
		[
			"doctor",
			"--sources",
			str(sources),
			"--trust-store",
			str(trust),
			"--lock",
			str(lock_path),
			"--cache-dir",
			str(cache),
			"--vendor-dir",
			str(vendor_dir),
			"--json",
			"--deep",
			"--fail-on",
			"degraded",
		]
	)
	assert cp.returncode == 0
	assert (cp.stderr or "").strip() == ""
	report = json.loads(cp.stdout)
	assert report["ok"] is True
	assert report["fatal_count"] == 0
	assert report["degraded_count"] == 0


def test_drift_publish_fetch_vendor_round_trip(tmp_path: Path) -> None:
	# Build a tiny unsigned package.
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	# Sign it (publisher role).
	priv = Ed25519PrivateKey.generate()
	try:
		seed = priv.private_bytes_raw()
	except AttributeError:
		seed = priv.private_bytes(
			encoding=serialization.Encoding.Raw,
			format=serialization.PrivateFormat.Raw,
			encryption_algorithm=serialization.NoEncryption(),
		)
	key_seed = tmp_path / "key.seed"
	key_seed.write_text(base64.b64encode(seed).decode("ascii") + "\n", encoding="utf-8")
	cp = _run_drift(["sign", str(pkg), "--key", str(key_seed)])
	assert cp.returncode == 0, cp.stderr
	assert Path(str(pkg) + ".sig").exists()

	# Publish to a local directory repository.
	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), str(pkg)])
	assert cp.returncode == 0, cp.stderr
	index_path = repo / "index.json"
	assert index_path.exists()
	index = json.loads(index_path.read_text(encoding="utf-8"))
	assert index["format"] == "drift-index"
	assert "lib" in index["packages"]

	# Fetch into project-local cache.
	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr
	assert (cache / "index.json").exists()

	# Vendor from cache and write a lockfile.
	vendor_dir = tmp_path / "vendor" / "driftpkgs"
	lock_path = tmp_path / "drift.lock.json"
	cp = _run_drift(
		[
			"vendor",
			"--cache-dir",
			str(cache),
			"--dest-dir",
			str(vendor_dir),
			"--lock",
			str(lock_path),
		]
	)
	assert cp.returncode == 0, cp.stderr
	assert lock_path.exists()
	lock = json.loads(lock_path.read_text(encoding="utf-8"))
	assert lock["format"] == "drift-lock"
	assert "lib" in lock["packages"]
	assert lock["packages"]["lib"]["pkg_sha256"].startswith("sha256:")
	assert lock["packages"]["lib"]["observed_identity"]["package_id"] == "lib"
	assert lock["packages"]["lib"]["observed_identity"]["version"] == "0.0.0"
	assert lock["packages"]["lib"]["observed_identity"]["target"] == "test-target"

	# Lock is authoritative: delete cache and reproduce exactly.
	shutil.rmtree(cache)
	cache.mkdir(parents=True, exist_ok=True)
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--lock", str(lock_path)])
	assert cp.returncode == 0, cp.stderr
	rebuilt = (cache / "pkgs").glob("*.dmp")
	pkgs = list(rebuilt)
	assert len(pkgs) == 1
	pkg_bytes = pkgs[0].read_bytes()
	assert lock["packages"]["lib"]["pkg_sha256"] == "sha256:" + hashlib.sha256(pkg_bytes).hexdigest()


def test_drift_fetch_selects_deterministically_across_sources(tmp_path: Path) -> None:
	"""
When two sources provide the same package id, fetch must pick deterministically
by (priority, source_id), not by file order or scan order.
	"""

	def _build_pkg(*, version: str, lib_body: str, out_pkg: Path) -> tuple[str, bytes]:
		_write_file(
			tmp_path / "lib" / "lib.drift",
			f"""
module lib

export {{ add }}

pub fn add(a: Int, b: Int) returns Int {{
	{lib_body}
}}
""".lstrip(),
		)
		assert (
			driftc_main(
				[
					"-M",
					str(tmp_path),
					str(tmp_path / "lib" / "lib.drift"),
					"--package-id",
					"lib",
					"--package-version",
					version,
					"--package-target",
					"test-target",
					"--emit-package",
					str(out_pkg),
				]
			)
			== 0
		)
		pkg_bytes = out_pkg.read_bytes()
		return hashlib.sha256(pkg_bytes).hexdigest(), pkg_bytes

	# Two packages with the same package_id but different identities.
	pkg_a = tmp_path / "lib_a.dmp"
	pkg_b = tmp_path / "lib_b.dmp"
	sha_a, bytes_a = _build_pkg(version="0.0.0", lib_body="return a + b", out_pkg=pkg_a)
	sha_b, _bytes_b = _build_pkg(version="0.0.1", lib_body="return a + b + 1", out_pkg=pkg_b)
	assert sha_a != sha_b

	# Publish to two repos; both will have the same deterministic filename but
	# different sha256.
	repo_a = tmp_path / "repo_a"
	repo_b = tmp_path / "repo_b"
	cp = _run_drift(["publish", "--dest-dir", str(repo_a), "--allow-unsigned", str(pkg_a)])
	assert cp.returncode == 0, cp.stderr
	cp = _run_drift(["publish", "--dest-dir", str(repo_b), "--allow-unsigned", str(pkg_b)])
	assert cp.returncode == 0, cp.stderr

	# Sources are listed in the opposite order from the deterministic winner.
	# Both have the same priority; tie-break is source_id.
	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [
					{"kind": "dir", "id": "b", "priority": 0, "path": str(repo_b)},
					{"kind": "dir", "id": "a", "priority": 0, "path": str(repo_a)},
				],
			}
		),
		encoding="utf-8",
	)

	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr

	# Cache must contain repo_a's bytes (source_id "a" wins tie-break).
	cache_pkgs = list((cache / "pkgs").glob("*.dmp"))
	assert len(cache_pkgs) == 1
	got = cache_pkgs[0].read_bytes()
	assert hashlib.sha256(got).hexdigest() == sha_a
	assert got == bytes_a

	cache_index = json.loads((cache / "index.json").read_text(encoding="utf-8"))
	assert cache_index["packages"]["lib"]["source_id"] == "a"


def test_drift_fetch_rejects_ambiguous_identity_across_sources_unlocked(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)

	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	repo_a = tmp_path / "repo_a"
	repo_b = tmp_path / "repo_b"
	cp = _run_drift(["publish", "--dest-dir", str(repo_a), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr
	cp = _run_drift(["publish", "--dest-dir", str(repo_b), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [
					{"kind": "dir", "id": "a", "priority": 0, "path": str(repo_a)},
					{"kind": "dir", "id": "b", "priority": 0, "path": str(repo_b)},
				],
			}
		),
		encoding="utf-8",
	)

	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode != 0
	assert "ambiguous package identity" in (cp.stderr or cp.stdout)


def test_drift_fetch_json_success_is_strict_json_only(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)
	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--json"])
	assert cp.returncode == 0
	assert (cp.stderr or "").strip() == ""
	report = json.loads(cp.stdout)
	assert report["ok"] is True
	assert report["mode"] == "unlocked"
	assert report["errors"] == []
	assert report["cache_index_written"] is True
	assert len(report["selected"]) == 1
	assert report["selected"][0]["source_id"] == "repo"
	assert report["selected"][0]["identity"] == {"package_id": "lib", "version": "0.0.0", "target": "test-target"}
	assert isinstance(report["selected"][0]["artifact_path"], str) and report["selected"][0]["artifact_path"].endswith(".dmp")
	assert isinstance(report["selected"][0]["cache_path"], str) and report["selected"][0]["cache_path"].endswith(".dmp")


def test_drift_fetch_json_failure_ambiguous_identity(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)
	repo_a = tmp_path / "repo_a"
	repo_b = tmp_path / "repo_b"
	cp = _run_drift(["publish", "--dest-dir", str(repo_a), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr
	cp = _run_drift(["publish", "--dest-dir", str(repo_b), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [
					{"kind": "dir", "id": "a", "priority": 0, "path": str(repo_a)},
					{"kind": "dir", "id": "b", "priority": 0, "path": str(repo_b)},
				],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--json"])
	assert cp.returncode != 0
	assert (cp.stderr or "").strip() == ""
	report = json.loads(cp.stdout)
	assert report["ok"] is False
	assert report["mode"] == "unlocked"
	assert report["cache_index_written"] is False
	assert len(report["errors"]) == 1
	assert report["errors"][0]["reason_code"] == "AMBIGUOUS_IDENTITY"


def test_drift_fetch_json_failure_missing_package_file(tmp_path: Path) -> None:
	repo = tmp_path / "repo"
	repo.mkdir(parents=True, exist_ok=True)
	(repo / "index.json").write_text(
		json.dumps(
			{
				"format": "drift-index",
				"version": 0,
				"packages": {
					"lib": {
						"package_version": "0.0.0",
						"target": "test-target",
						"sha256": "sha256:" + ("00" * 32),
						"filename": "lib-0.0.0-test-target.dmp",
						"signers": [],
						"unsigned": True,
					}
				},
			}
		),
		encoding="utf-8",
	)

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)

	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--json"])
	assert cp.returncode != 0
	assert (cp.stderr or "").strip() == ""
	assert cp.stdout.lstrip().startswith("{")
	report = json.loads(cp.stdout)
	assert report["ok"] is False
	assert report["mode"] == "unlocked"
	assert report["cache_index_written"] is False
	assert any(e.get("reason_code") == "INDEX_MISSING_PACKAGE_FILE" for e in report["errors"])
	err = next(e for e in report["errors"] if e.get("reason_code") == "INDEX_MISSING_PACKAGE_FILE")
	assert isinstance(err.get("index_path"), str) and err["index_path"].endswith("index.json")
	assert isinstance(err.get("artifact_path"), str) and err["artifact_path"].endswith("lib-0.0.0-test-target.dmp")
	assert err.get("identity", {}).get("package_id") == "lib"


def test_drift_vendor_skips_bad_entries_and_refuses_lock_with_json_report(tmp_path: Path) -> None:
	# Build and publish two packages, then fetch them into cache.
	_write_file(
		tmp_path / "a" / "a.drift",
		"""
module a

export { add }

pub fn add(x: Int, y: Int) returns Int {
	return x + y
}
""".lstrip(),
	)
	_write_file(
		tmp_path / "b" / "b.drift",
		"""
module b

export { add }

pub fn add(x: Int, y: Int) returns Int {
	return x + y + 1
}
""".lstrip(),
	)
	pkg_a = tmp_path / "a.dmp"
	pkg_b = tmp_path / "b.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "a" / "a.drift"),
				"--package-id",
				"a",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg_a),
			]
		)
		== 0
	)
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "b" / "b.drift"),
				"--package-id",
				"b",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg_b),
			]
		)
		== 0
	)
	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg_a), str(pkg_b)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr

	# Corrupt cache index entry for "b" by removing its source_id.
	cache_index_path = cache / "index.json"
	cache_index = json.loads(cache_index_path.read_text(encoding="utf-8"))
	assert cache_index["format"] == "drift-index"
	assert "b" in cache_index["packages"]
	cache_index["packages"]["b"].pop("source_id", None)
	cache_index_path.write_text(json.dumps(cache_index), encoding="utf-8")

	vendor_dir = tmp_path / "vendor" / "driftpkgs"
	lock_path = tmp_path / "drift.lock.json"
	cp = _run_drift(
		[
			"vendor",
			"--cache-dir",
			str(cache),
			"--dest-dir",
			str(vendor_dir),
			"--lock",
			str(lock_path),
			"--json",
		]
	)
	assert cp.returncode != 0
	assert not lock_path.exists()
	report = json.loads(cp.stdout)
	assert report["ok"] is False
	assert report["lock_written"] is False
	assert report["error_count"] >= 1
	assert any(e.get("package_id") == "b" and e.get("reason_code") == "MISSING_SOURCE_ID" for e in report["errors"])

	# "a" is still vendored; "b" is skipped.
	vendored = [p.name for p in vendor_dir.glob("*.dmp")]
	assert any(name.startswith("a-") for name in vendored)
	assert not any(name.startswith("b-") for name in vendored)


def test_drift_fetch_lock_mode_emits_structured_error_code_on_sha_mismatch(tmp_path: Path) -> None:
	# Build, publish, fetch, vendor lock.
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)
	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr
	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode == 0, cp.stderr

	vendor_dir = tmp_path / "vendor" / "driftpkgs"
	lock_path = tmp_path / "drift.lock.json"
	cp = _run_drift(["vendor", "--cache-dir", str(cache), "--dest-dir", str(vendor_dir), "--lock", str(lock_path)])
	assert cp.returncode == 0, cp.stderr
	lock = json.loads(lock_path.read_text(encoding="utf-8"))
	locked_rel = lock["packages"]["lib"]["path"]

	# Corrupt repo bytes after lock creation (lock-mode should fail on sha mismatch).
	repo_pkg = repo / locked_rel
	data = bytearray(repo_pkg.read_bytes())
	data[-1] ^= 0xFF
	repo_pkg.write_bytes(bytes(data))

	# Fetch with lock: should fail with structured reason code and no argparse usage spam.
	shutil.rmtree(cache)
	cache.mkdir(parents=True, exist_ok=True)
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--lock", str(lock_path)])
	assert cp.returncode != 0
	msg = (cp.stderr or cp.stdout)
	assert "[LOCK_SHA_MISMATCH]" in msg
	assert "identity=(lib, 0.0.0, test-target)" in msg


def test_drift_fetch_rejects_sha_mismatch_between_index_and_bytes(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.0",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)

	repo = tmp_path / "repo"
	cp = _run_drift(["publish", "--dest-dir", str(repo), "--allow-unsigned", str(pkg)])
	assert cp.returncode == 0, cp.stderr

	# Corrupt the bytes after publishing without updating the index.
	repo_pkg = repo / "lib-0.0.0-test-target.dmp"
	data = bytearray(repo_pkg.read_bytes())
	data[-1] ^= 0xFF
	repo_pkg.write_bytes(bytes(data))

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode != 0
	assert "sha256 mismatch" in (cp.stderr or cp.stdout)


def test_drift_fetch_rejects_identity_mismatch_in_index(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	# Build a package with version 0.0.1, but index will claim 0.0.0.
	pkg = tmp_path / "lib-0.0.0-test-target.dmp"
	assert (
		driftc_main(
			[
				"-M",
				str(tmp_path),
				str(tmp_path / "lib" / "lib.drift"),
				"--package-id",
				"lib",
				"--package-version",
				"0.0.1",
				"--package-target",
				"test-target",
				"--emit-package",
				str(pkg),
			]
		)
		== 0
	)
	repo = tmp_path / "repo"
	repo.mkdir(parents=True, exist_ok=True)
	shutil.copyfile(pkg, repo / "lib-0.0.0-test-target.dmp")
	pkg_sha = hashlib.sha256((repo / "lib-0.0.0-test-target.dmp").read_bytes()).hexdigest()
	(repo / "index.json").write_text(
		json.dumps(
			{
				"format": "drift-index",
				"version": 0,
				"packages": {
					"lib": {
						"package_version": "0.0.0",
						"target": "test-target",
						"sha256": "sha256:" + pkg_sha,
						"filename": "lib-0.0.0-test-target.dmp",
						"signers": [],
						"unsigned": True,
					}
				},
			}
		),
		encoding="utf-8",
	)
	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode != 0
	assert "identity mismatch" in (cp.stderr or cp.stdout)


def test_drift_fetch_rejects_malformed_index_json(tmp_path: Path) -> None:
	repo = tmp_path / "repo"
	repo.mkdir(parents=True, exist_ok=True)
	(repo / "index.json").write_text(
		json.dumps({"format": "drift-index", "version": 0, "packages": {"lib": {"filename": "x"}}}),
		encoding="utf-8",
	)
	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [{"kind": "dir", "id": "repo", "priority": 0, "path": str(repo)}],
			}
		),
		encoding="utf-8",
	)
	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache)])
	assert cp.returncode != 0
	assert "invalid index entry" in (cp.stderr or cp.stdout)


def test_drift_fetch_rejects_lock_with_unknown_source_id_when_ambiguous(tmp_path: Path) -> None:
	"""
The lockfile must be authoritative. A legacy/placeholder source_id must not
silently re-enable heuristic selection when multiple sources provide the same
package id.
	"""

	def _build_pkg(lib_body: str, out_pkg: Path) -> None:
		_write_file(
			tmp_path / "lib" / "lib.drift",
			f"""
module lib

export {{ add }}

pub fn add(a: Int, b: Int) returns Int {{
	{lib_body}
}}
""".lstrip(),
		)
		assert (
			driftc_main(
				[
					"-M",
					str(tmp_path),
					str(tmp_path / "lib" / "lib.drift"),
					"--package-id",
					"lib",
					"--package-version",
					"0.0.0",
					"--package-target",
					"test-target",
					"--emit-package",
					str(out_pkg),
				]
			)
			== 0
		)

	pkg_a = tmp_path / "lib_a.dmp"
	pkg_b = tmp_path / "lib_b.dmp"
	_build_pkg("return a + b", pkg_a)
	_build_pkg("return a + b + 1", pkg_b)

	repo_a = tmp_path / "repo_a"
	repo_b = tmp_path / "repo_b"
	cp = _run_drift(["publish", "--dest-dir", str(repo_a), "--allow-unsigned", str(pkg_a)])
	assert cp.returncode == 0, cp.stderr
	cp = _run_drift(["publish", "--dest-dir", str(repo_b), "--allow-unsigned", str(pkg_b)])
	assert cp.returncode == 0, cp.stderr

	sources = tmp_path / "drift-sources.json"
	sources.write_text(
		json.dumps(
			{
				"format": "drift-sources",
				"version": 0,
				"sources": [
					{"kind": "dir", "id": "a", "priority": 0, "path": str(repo_a)},
					{"kind": "dir", "id": "b", "priority": 0, "path": str(repo_b)},
				],
			}
		),
		encoding="utf-8",
	)

	# Manually create a legacy/broken lockfile with source_id 'unknown'.
	lock_path = tmp_path / "drift.lock.json"
	lock_path.write_text(
		json.dumps(
			{
				"format": "drift-lock",
				"version": 0,
				"packages": {
					"lib": {
						"version": "0.0.0",
						"target": "test-target",
						"observed_identity": {"package_id": "lib", "version": "0.0.0", "target": "test-target"},
						"pkg_sha256": json.loads((repo_a / "index.json").read_text(encoding="utf-8"))["packages"]["lib"][
							"sha256"
						],
						"sig_sha256": None,
						"sig_kids": [],
						"modules": ["lib"],
						"source_id": "unknown",
						"path": "lib-0.0.0-test-target.dmp",
					}
				},
			}
		),
		encoding="utf-8",
	)

	cache = tmp_path / "cache" / "driftpm"
	cp = _run_drift(["fetch", "--sources", str(sources), "--cache-dir", str(cache), "--lock", str(lock_path)])
	assert cp.returncode != 0
	assert "missing source_id" in (cp.stderr or cp.stdout)

[==== File: staged/lang2/tests/driver/tests/test_drift_sign_cli.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import base64
import json
import os
import subprocess
import sys
from pathlib import Path

from lang2.drift.crypto import compute_ed25519_kid


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_drift_sign_produces_sidecar_accepted_by_driftc(tmp_path: Path) -> None:
	# Build an unsigned package locally (unsigned is allowed for local build
	# outputs, but driftc will still require signatures when asked to).
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	repo_root = Path.cwd()
	assert (repo_root / "lang2").exists()
	build_pkg = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.driftc.driftc",
			"-M",
			str(tmp_path),
			str(tmp_path / "lib" / "lib.drift"),
			"--package-id",
			"test.pkg",
			"--package-version",
			"0.0.0",
			"--package-target",
			"test-target",
			"--emit-package",
			str(pkg),
			"--json",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert build_pkg.returncode == 0, build_pkg.stderr
	out = json.loads(build_pkg.stdout or "{}")
	assert out.get("exit_code") == 0

	# Generate a deterministic key seed file (base64 raw 32 bytes).
	seed32 = os.urandom(32)
	key_path = tmp_path / "key.seed"
	key_path.write_text(base64.b64encode(seed32).decode("ascii") + "\n", encoding="utf-8")

	# Sign via drift CLI (publisher-side).
	# Use the repo root as cwd so `python -m lang2.drift` resolves correctly.
	res = subprocess.run(
		[sys.executable, "-m", "lang2.drift", "sign", str(pkg), "--key", str(key_path), "--include-pubkey"],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert res.returncode == 0, res.stderr

	sig_path = Path(str(pkg) + ".sig")
	assert sig_path.exists()

	# Create a minimal project trust store that trusts the signer for `lib`.
	pkg_bytes = pkg.read_bytes()
	sig_obj = json.loads(sig_path.read_text(encoding="utf-8"))
	pub_b64 = sig_obj["signatures"][0].get("pubkey")
	assert isinstance(pub_b64, str)
	pub_raw = base64.b64decode(pub_b64.encode("ascii"), validate=True)
	kid = compute_ed25519_kid(pub_raw)

	trust = {
		"format": "drift-trust",
		"version": 0,
		"keys": {
			kid: {"algo": "ed25519", "pubkey": base64.b64encode(pub_raw).decode("ascii")},
		},
		"namespaces": {
			"lib.*": [kid],
		},
	}
	trust_path = tmp_path / "trust.json"
	trust_path.write_text(json.dumps(trust, sort_keys=True, separators=(",", ":")) + "\n", encoding="utf-8")

	# Consume the package with signatures required. This proves driftc accepts the
	# publisher-produced signature sidecar.
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	consume = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.driftc.driftc",
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			"--require-signatures",
			"--trust-store",
			str(trust_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(tmp_path / "out.ll"),
			"--json",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert consume.returncode == 0, consume.stderr
	out = json.loads(consume.stdout or "{}")
	assert out.get("exit_code") == 0

[==== File: staged/lang2/tests/driver/tests/test_drift_trust_cli.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import base64
import json
import os
import subprocess
import sys
from pathlib import Path

from lang2.drift.crypto import compute_ed25519_kid


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_drift_trust_revoke_blocks_package_consumption(tmp_path: Path) -> None:
	# Build a tiny module package we can sign and then consume from a separate
	# source module.
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	repo_root = Path.cwd()

	build_pkg = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.driftc.driftc",
			"-M",
			str(tmp_path),
			str(tmp_path / "lib" / "lib.drift"),
			"--package-id",
			"test.pkg",
			"--package-version",
			"0.0.0",
			"--package-target",
			"test-target",
			"--emit-package",
			str(pkg),
			"--json",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert build_pkg.returncode == 0, build_pkg.stderr
	out = json.loads(build_pkg.stdout or "{}")
	assert out.get("exit_code") == 0

	seed32 = os.urandom(32)
	key_path = tmp_path / "key.seed"
	key_path.write_text(base64.b64encode(seed32).decode("ascii") + "\n", encoding="utf-8")

	# Publisher-side signing.
	sign = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.drift",
			"sign",
			str(pkg),
			"--key",
			str(key_path),
			"--include-pubkey",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert sign.returncode == 0, sign.stderr
	sidecar = Path(str(pkg) + ".sig")
	sig_obj = json.loads(sidecar.read_text(encoding="utf-8"))
	pub_b64 = sig_obj["signatures"][0].get("pubkey")
	assert isinstance(pub_b64, str)
	pub_raw = base64.b64decode(pub_b64.encode("ascii"), validate=True)
	kid = compute_ed25519_kid(pub_raw)

	# Create trust store using drift tooling.
	trust_path = tmp_path / "drift" / "trust.json"
	add_key = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.drift",
			"trust",
			"add-key",
			"--trust-store",
			str(trust_path),
			"--namespace",
			"lib.*",
			"--pubkey",
			pub_b64,
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert add_key.returncode == 0, add_key.stderr

	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)

	# Verify package consumption succeeds before revocation.
	def _consume() -> dict:
		res = subprocess.run(
			[
				sys.executable,
				"-m",
				"lang2.driftc.driftc",
				"-M",
				str(tmp_path),
				"--package-root",
				str(tmp_path),
				"--require-signatures",
				"--trust-store",
				str(trust_path),
				str(tmp_path / "main.drift"),
				"--emit-ir",
				str(tmp_path / "out.ll"),
				"--json",
			],
			cwd=str(repo_root),
			check=False,
			capture_output=True,
			text=True,
		)
		assert res.returncode in (0, 1), res.stderr
		return json.loads(res.stdout or "{}")

	ok = _consume()
	assert ok.get("exit_code") == 0

	# Revoke and verify driftc rejects the package.
	revoke = subprocess.run(
		[
			sys.executable,
			"-m",
			"lang2.drift",
			"trust",
			"revoke",
			"--trust-store",
			str(trust_path),
			"--kid",
			kid,
			"--reason",
			"test",
		],
		cwd=str(repo_root),
		check=False,
		capture_output=True,
		text=True,
	)
	assert revoke.returncode == 0, revoke.stderr

	out2 = _consume()
	assert out2.get("exit_code") == 1
	diags = out2.get("diagnostics") or []
	assert any("revoked" in str(d.get("message", "")).lower() for d in diags), diags

[==== File: staged/lang2/tests/driver/tests/test_method_resolution_multimodule.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

from pathlib import Path

from lang2.driftc import stage1 as H
from lang2.driftc.core.function_id import FunctionId
from lang2.driftc.method_registry import CallableRegistry, CallableSignature, SelfMode, Visibility
from lang2.driftc.parser import parse_drift_workspace_to_hir
from lang2.driftc.type_checker import TypeChecker


def _write_file(path: Path, content: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(content)


def _callable_name(fn_id: FunctionId) -> str:
	return fn_id.name if fn_id.module == "main" else f"{fn_id.module}::{fn_id.name}"


def _build_registry(signatures: dict[FunctionId, object]) -> tuple[CallableRegistry, dict[object, int]]:
	registry = CallableRegistry()
	module_ids: dict[object, int] = {None: 0}
	next_id = 1
	for fn_id, sig in signatures.items():
		if sig.param_type_ids is None or sig.return_type_id is None:
			continue
		module_id = module_ids.setdefault(sig.module, len(module_ids))
		if sig.is_method:
			if sig.impl_target_type_id is None or sig.self_mode is None:
				continue
			self_mode = {
				"value": SelfMode.SELF_BY_VALUE,
				"ref": SelfMode.SELF_BY_REF,
				"ref_mut": SelfMode.SELF_BY_REF_MUT,
			}.get(sig.self_mode)
			if self_mode is None:
				continue
			registry.register_inherent_method(
				callable_id=next_id,
				name=sig.method_name or sig.name,
				module_id=module_id,
				visibility=Visibility.public(),
				signature=CallableSignature(param_types=tuple(sig.param_type_ids), result_type=sig.return_type_id),
				fn_id=fn_id,
				impl_id=next_id,
				impl_target_type_id=sig.impl_target_type_id,
				self_mode=self_mode,
				is_generic=bool(sig.type_params or getattr(sig, "impl_type_params", [])),
			)
		else:
			registry.register_free_function(
				callable_id=next_id,
				name=_callable_name(fn_id),
				module_id=module_id,
				visibility=Visibility.public(),
				signature=CallableSignature(param_types=tuple(sig.param_type_ids), result_type=sig.return_type_id),
				fn_id=fn_id,
				is_generic=bool(sig.type_params),
			)
		next_id += 1
	return registry, module_ids


def _visible_modules_for(
	module_name: str, module_deps: dict[str, set[str]], module_ids: dict[object, int]
) -> tuple[int, ...]:
	visible = set(module_deps.get(module_name, set()))
	visible.add(module_name)
	return tuple(sorted(module_ids.get(mod, 0) for mod in visible))


def _collect_method_calls(block: H.HBlock) -> list[H.HMethodCall]:
	calls: list[H.HMethodCall] = []

	def walk_expr(expr: H.HExpr) -> None:
		if isinstance(expr, H.HMethodCall):
			calls.append(expr)
			walk_expr(expr.receiver)
			for a in expr.args:
				walk_expr(a)
			for kw in getattr(expr, "kwargs", []) or []:
				if getattr(kw, "value", None) is not None:
					walk_expr(kw.value)
			return
		for child in getattr(expr, "__dict__", {}).values():
			if isinstance(child, H.HExpr):
				walk_expr(child)
			elif isinstance(child, H.HBlock):
				walk_block(child)
			elif isinstance(child, list):
				for it in child:
					if isinstance(it, H.HExpr):
						walk_expr(it)
					elif isinstance(it, H.HBlock):
						walk_block(it)

	def walk_block(b: H.HBlock) -> None:
		for st in b.statements:
			if isinstance(st, H.HExprStmt):
				walk_expr(st.expr)
			elif isinstance(st, H.HReturn) and st.value is not None:
				walk_expr(st.value)
			else:
				for child in getattr(st, "__dict__", {}).values():
					if isinstance(child, H.HExpr):
						walk_expr(child)
					elif isinstance(child, H.HBlock):
						walk_block(child)
					elif isinstance(child, list):
						for it in child:
							if isinstance(it, H.HExpr):
								walk_expr(it)
							elif isinstance(it, H.HBlock):
								walk_block(it)

	walk_block(block)
	return calls


def _resolve_main_block(
	tmp_path: Path, files: dict[Path, str], *, main_module: str
) -> tuple[H.HBlock, dict[int, object], dict[FunctionId, object], dict[str, set[str]], dict[object, int]]:
	mod_root = tmp_path / "mods"
	for rel, content in files.items():
		_write_file(mod_root / rel, content)
	paths = sorted(mod_root.rglob("*.drift"))
	func_hirs, signatures, fn_ids_by_name, type_table, _exc_catalog, _exports, module_deps, diagnostics = parse_drift_workspace_to_hir(
		paths,
		module_paths=[mod_root],
	)
	assert diagnostics == []
	registry, module_ids = _build_registry(signatures)
	main_ids = fn_ids_by_name.get(f"{main_module}::main") or []
	assert len(main_ids) == 1
	main_id = main_ids[0]
	main_block = func_hirs[main_id]
	main_sig = signatures.get(main_id)
	param_types = {}
	if main_sig and main_sig.param_names and main_sig.param_type_ids:
		param_types = {pname: pty for pname, pty in zip(main_sig.param_names, main_sig.param_type_ids)}
	current_mod = module_ids.setdefault(main_sig.module, len(module_ids))
	visible_mods = _visible_modules_for(main_module, module_deps, module_ids)
	tc = TypeChecker(type_table=type_table)
	result = tc.check_function(
		main_id,
		main_block,
		param_types=param_types,
		return_type=main_sig.return_type_id if main_sig is not None else None,
		signatures_by_id=signatures,
		callable_registry=registry,
		visible_modules=visible_mods,
		current_module=current_mod,
	)
	return main_block, result.typed_fn.call_resolutions, signatures, module_deps, module_ids


def test_method_resolution_cross_module_success(tmp_path: Path) -> None:
	files = {
		Path("m_box/lib.drift"): """
module m_box

export { Box }

pub struct Box<T> { value: T }

implement<T> Box<T> {
	fn tag(self: Box<T>) returns Int { return 1; }
}
""",
		Path("m_main/main.drift"): """
module m_main

import m_box

fn main() returns Int {
	val b: m_box.Box<Int> = m_box.Box<type Int>(1);
	return b.tag();
}
""",
	}
	main_block, call_resolutions, signatures, _deps, _module_ids = _resolve_main_block(
		tmp_path, files, main_module="m_main"
	)
	calls = _collect_method_calls(main_block)
	assert len(calls) == 1
	res = call_resolutions.get(id(calls[0]))
	assert res is not None and res.decl.fn_id is not None
	assert res.decl.fn_id.module == "m_box"
	assert signatures[res.decl.fn_id].is_method


def test_method_resolution_ambiguity_across_modules(tmp_path: Path) -> None:
	files = {
		Path("m_types/lib.drift"): """
module m_types

export { Box }

pub struct Box<T> { value: T }
""",
		Path("m_a/lib.drift"): """
module m_a

import m_types

implement m_types.Box<Int> {
	fn tag(self: m_types.Box<Int>) returns Int { return 1; }
}
""",
		Path("m_b/lib.drift"): """
module m_b

import m_types

implement m_types.Box<Int> {
	fn tag(self: m_types.Box<Int>) returns Int { return 2; }
}
""",
		Path("m_main/main.drift"): """
module m_main

import m_types
import m_a
import m_b

fn main() returns Int {
	val b: m_types.Box<Int> = m_types.Box<type Int>(1);
	return b.tag();
}
""",
	}
	mod_root = tmp_path / "mods"
	for rel, content in files.items():
		_write_file(mod_root / rel, content)
	paths = sorted(mod_root.rglob("*.drift"))
	func_hirs, signatures, fn_ids_by_name, type_table, _exc_catalog, _exports, module_deps, diagnostics = parse_drift_workspace_to_hir(
		paths,
		module_paths=[mod_root],
	)
	assert diagnostics == []
	registry, module_ids = _build_registry(signatures)
	main_ids = fn_ids_by_name.get("m_main::main") or []
	assert len(main_ids) == 1
	main_id = main_ids[0]
	main_block = func_hirs[main_id]
	main_sig = signatures.get(main_id)
	param_types = {}
	if main_sig and main_sig.param_names and main_sig.param_type_ids:
		param_types = {pname: pty for pname, pty in zip(main_sig.param_names, main_sig.param_type_ids)}
	current_mod = module_ids.setdefault(main_sig.module, len(module_ids))
	visible_mods = _visible_modules_for("m_main", module_deps, module_ids)
	tc = TypeChecker(type_table=type_table)
	result = tc.check_function(
		main_id,
		main_block,
		param_types=param_types,
		return_type=main_sig.return_type_id if main_sig is not None else None,
		signatures_by_id=signatures,
		callable_registry=registry,
		visible_modules=visible_mods,
		current_module=current_mod,
	)
	assert result.diagnostics
	msg = result.diagnostics[0].message
	assert "ambiguous method" in msg
	assert "m_a" in msg and "m_b" in msg


def test_method_visibility_controls_candidates(tmp_path: Path) -> None:
	files = {
		Path("m_types/lib.drift"): """
module m_types

export { Box }

pub struct Box<T> { value: T }
""",
		Path("m_a/lib.drift"): """
module m_a

import m_types

implement m_types.Box<Int> {
	fn tag(self: m_types.Box<Int>) returns Int { return 1; }
}
""",
		Path("m_b/lib.drift"): """
module m_b

import m_types

implement m_types.Box<Int> {
	fn tag(self: m_types.Box<Int>) returns Int { return 2; }
}
""",
		Path("m_main/main.drift"): """
module m_main

import m_types
import m_a

fn main() returns Int {
	val b: m_types.Box<Int> = m_types.Box<type Int>(1);
	return b.tag();
}
""",
	}
	main_block, call_resolutions, signatures, _deps, _module_ids = _resolve_main_block(
		tmp_path, files, main_module="m_main"
	)
	calls = _collect_method_calls(main_block)
	assert len(calls) == 1
	res = call_resolutions.get(id(calls[0]))
	assert res is not None and res.decl.fn_id is not None
	assert res.decl.fn_id.module == "m_a"
	assert signatures[res.decl.fn_id].is_method


def test_method_resolution_generic_impl_across_modules(tmp_path: Path) -> None:
	files = {
		Path("m_box/lib.drift"): """
module m_box

export { Box }

pub struct Box<T> { value: T }
""",
		Path("m_impl/lib.drift"): """
module m_impl

import m_box

implement<T> m_box.Box<Array<T>> {
	fn inner(self: m_box.Box<Array<T>>) returns T { return self.value[0]; }
}
""",
		Path("m_main/main.drift"): """
module m_main

import m_box
import m_impl

fn main() returns Int {
	val b: m_box.Box<Array<Int>> = m_box.Box<type Array<Int>>([1, 2]);
	return b.inner();
}
""",
	}
	main_block, call_resolutions, signatures, _deps, _module_ids = _resolve_main_block(
		tmp_path, files, main_module="m_main"
	)
	calls = _collect_method_calls(main_block)
	assert len(calls) == 1
	res = call_resolutions.get(id(calls[0]))
	assert res is not None and res.decl.fn_id is not None
	assert res.decl.fn_id.module == "m_impl"
	assert signatures[res.decl.fn_id].is_method

[==== File: staged/lang2/tests/driver/tests/test_overload_resolution_multimodule.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

from pathlib import Path

from lang2.driftc import stage1 as H
from lang2.driftc.core.function_id import FunctionId
from lang2.driftc.method_registry import CallableRegistry, CallableSignature, Visibility
from lang2.driftc.parser import parse_drift_workspace_to_hir
from lang2.driftc.type_checker import TypeChecker


def _write_file(path: Path, content: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(content)


def _callable_name(fn_id: FunctionId) -> str:
	return fn_id.name if fn_id.module == "main" else f"{fn_id.module}::{fn_id.name}"


def _build_registry(signatures: dict[FunctionId, object]) -> tuple[CallableRegistry, dict[object, int]]:
	registry = CallableRegistry()
	module_ids: dict[object, int] = {None: 0}
	next_id = 1
	for fn_id, sig in signatures.items():
		if sig.is_method:
			continue
		if sig.param_type_ids is None or sig.return_type_id is None:
			continue
		module_id = module_ids.setdefault(sig.module, len(module_ids))
		registry.register_free_function(
			callable_id=next_id,
			name=_callable_name(fn_id),
			module_id=module_id,
			visibility=Visibility.public(),
			signature=CallableSignature(param_types=tuple(sig.param_type_ids), result_type=sig.return_type_id),
			fn_id=fn_id,
			is_generic=False,
		)
		next_id += 1
	return registry, module_ids


def _visible_modules_for(
	module_name: str, module_deps: dict[str, set[str]], module_ids: dict[object, int]
) -> tuple[int, ...]:
	visible = set(module_deps.get(module_name, set()))
	visible.add(module_name)
	return tuple(sorted(module_ids.get(mod, 0) for mod in visible))


def _collect_calls(block: H.HBlock) -> list[H.HCall]:
	calls: list[H.HCall] = []

	def walk_expr(expr: H.HExpr) -> None:
		if isinstance(expr, H.HCall):
			calls.append(expr)
			walk_expr(expr.fn)
			for a in expr.args:
				walk_expr(a)
			for kw in getattr(expr, "kwargs", []) or []:
				if getattr(kw, "value", None) is not None:
					walk_expr(kw.value)
			return
		for child in getattr(expr, "__dict__", {}).values():
			if isinstance(child, H.HExpr):
				walk_expr(child)
			elif isinstance(child, H.HBlock):
				walk_block(child)
			elif isinstance(child, list):
				for it in child:
					if isinstance(it, H.HExpr):
						walk_expr(it)
					elif isinstance(it, H.HBlock):
						walk_block(it)

	def walk_block(b: H.HBlock) -> None:
		for st in b.statements:
			if isinstance(st, H.HExprStmt):
				walk_expr(st.expr)
			elif isinstance(st, H.HReturn) and st.value is not None:
				walk_expr(st.value)
			else:
				for child in getattr(st, "__dict__", {}).values():
					if isinstance(child, H.HExpr):
						walk_expr(child)
					elif isinstance(child, H.HBlock):
						walk_block(child)
					elif isinstance(child, list):
						for it in child:
							if isinstance(it, H.HExpr):
								walk_expr(it)
							elif isinstance(it, H.HBlock):
								walk_block(it)

	walk_block(block)
	return calls


def _find_fn_id_by_param_type(signatures: dict[FunctionId, object], *, name: str, param_type_id: int) -> FunctionId:
	for fn_id, sig in signatures.items():
		if fn_id.name != name:
			continue
		if sig.param_type_ids and list(sig.param_type_ids) == [param_type_id]:
			return fn_id
	raise AssertionError(f"missing overload for {name}({param_type_id})")


def test_overloads_across_modules_with_qualified_calls(tmp_path: Path) -> None:
	mod_root = tmp_path / "mods"
	_write_file(
		mod_root / "a" / "lib.drift",
		"""
module a

export { f }

pub fn f(x: Int) returns Int { return x + 1; }
pub fn f(x: String) returns Int { return 2; }
""",
	)
	_write_file(
		mod_root / "b" / "main.drift",
		"""
module b

import a

fn main() returns Int {
    val r1: Int = a.f(1);
    val r2: Int = a.f("hi");
    return r1 + r2;
}
""",
	)
	paths = [mod_root / "a" / "lib.drift", mod_root / "b" / "main.drift"]
	func_hirs, signatures, fn_ids_by_name, type_table, _exc_catalog, _exports, module_deps, diagnostics = parse_drift_workspace_to_hir(
		paths,
		module_paths=[mod_root],
	)
	assert diagnostics == []
	registry, module_ids = _build_registry(signatures)
	main_ids = fn_ids_by_name.get("b::main") or []
	assert len(main_ids) == 1
	main_id = main_ids[0]
	main_block = func_hirs[main_id]
	main_sig = signatures.get(main_id)
	param_types = {}
	if main_sig and main_sig.param_names and main_sig.param_type_ids:
		param_types = {pname: pty for pname, pty in zip(main_sig.param_names, main_sig.param_type_ids)}
	current_mod = module_ids.setdefault(main_sig.module, len(module_ids))
	visible_mods = _visible_modules_for("b", module_deps, module_ids)
	tc = TypeChecker(type_table=type_table)
	result = tc.check_function(
		main_id,
		main_block,
		param_types=param_types,
		return_type=main_sig.return_type_id if main_sig is not None else None,
		signatures_by_id=signatures,
		callable_registry=registry,
		visible_modules=visible_mods,
		current_module=current_mod,
	)
	assert result.diagnostics == []
	calls = _collect_calls(main_block)
	assert len(calls) >= 2
	int_ty = type_table.ensure_int()
	string_ty = type_table.ensure_string()
	f_int = _find_fn_id_by_param_type(signatures, name="f", param_type_id=int_ty)
	f_str = _find_fn_id_by_param_type(signatures, name="f", param_type_id=string_ty)
	seen_int = False
	seen_str = False
	for call in calls:
		decl = result.typed_fn.call_resolutions.get(id(call))
		if decl is None or decl.fn_id is None:
			continue
		arg = call.args[0] if call.args else None
		if isinstance(arg, H.HLiteralInt):
			assert decl.fn_id == f_int
			seen_int = True
		if isinstance(arg, H.HLiteralString):
			assert decl.fn_id == f_str
			seen_str = True
	assert seen_int and seen_str

[==== File: staged/lang2/tests/packages/test_package_root_import_compile.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

from pathlib import Path

from lang2.driftc.driftc import main as driftc_main


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_import_from_package_root_compiles_to_ir(tmp_path: Path) -> None:
	# Build a package that provides module `lib`.
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)

	pkg = tmp_path / "lib.dmp"
	assert driftc_main(["-M", str(tmp_path), str(tmp_path / "lib" / "lib.drift"), "--emit-package", str(pkg)]) == 0

	# Compile a main module that imports `lib` from the package root.
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)

	ir_path = tmp_path / "out.ll"
	assert driftc_main(
		[
			"-M",
			str(tmp_path),
			"--package-root",
			str(tmp_path),
			str(tmp_path / "main.drift"),
			"--emit-ir",
			str(ir_path),
		]
	) == 0

	ir = ir_path.read_text(encoding="utf-8")
	assert "define i64 @lib::add" in ir


[==== File: staged/lang2/tests/packages/test_package_v0_determinism.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import zipfile
from pathlib import Path

from lang2.driftc.driftc import main as driftc_main


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_emit_package_is_deterministic(tmp_path: Path) -> None:
	# Layout matches the module-path inference rules:
	# - files directly under the module root infer module_id == "main"
	# - files under <root>/lib infer module_id == "lib"
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)

	out1 = tmp_path / "p1.dmp"
	out2 = tmp_path / "p2.dmp"

	argv = ["-M", str(tmp_path), str(tmp_path / "main.drift"), str(tmp_path / "lib" / "lib.drift"), "--emit-package"]

	assert driftc_main(argv + [str(out1)]) == 0
	assert driftc_main(argv + [str(out2)]) == 0

	b1 = out1.read_bytes()
	b2 = out2.read_bytes()
	assert b1 == b2

	# Sanity: the artifact contains a deterministic manifest with the pinned markers.
	with zipfile.ZipFile(out1) as zf:
		manifest = zf.read("manifest.json").decode("utf-8")
		assert "\"payload_kind\":\"provisional-dmir\"" in manifest
		assert "\"payload_version\":0" in manifest
		assert "\"unstable_format\":true" in manifest


[==== File: staged/lang2/tests/packages/test_package_v0_loader.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

from pathlib import Path

from lang2.driftc.driftc import main as driftc_main
from lang2.driftc.packages.provider_v0 import load_package_v0


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_load_package_v0_round_trip(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)

	out = tmp_path / "p.dmp"
	argv = ["-M", str(tmp_path), str(tmp_path / "main.drift"), str(tmp_path / "lib" / "lib.drift"), "--emit-package", str(out)]
	assert driftc_main(argv) == 0

	pkg = load_package_v0(out)
	assert pkg.manifest["kind"] == "drift-package"
	assert pkg.manifest["payload_kind"] == "provisional-dmir"
	assert set(pkg.modules_by_id.keys()) == {"lib", "main", "lang.core"}

	lib_iface = pkg.modules_by_id["lib"].interface
	assert lib_iface["module_id"] == "lib"
	assert "add" in lib_iface["exports"]["values"]


[==== File: staged/lang2/tests/packages/test_package_v0_negative_cases.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import zipfile
from pathlib import Path

import pytest

from lang2.driftc.driftc import main as driftc_main
from lang2.driftc.packages.provider_v0 import load_package_v0


def _write_file(path: Path, text: str) -> None:
	path.parent.mkdir(parents=True, exist_ok=True)
	path.write_text(text, encoding="utf-8")


def test_load_package_rejects_bad_blob_hash(tmp_path: Path) -> None:
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { add }

pub fn add(a: Int, b: Int) returns Int {
	return a + b
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert driftc_main(["-M", str(tmp_path), str(tmp_path / "lib" / "lib.drift"), "--emit-package", str(pkg)]) == 0

	# Corrupt the manifest in-place (still valid JSON but changes bytes).
	with zipfile.ZipFile(pkg, "a") as zf:
		manifest = zf.read("manifest.json").decode("utf-8")
		zf.writestr("manifest.json", manifest.replace("\"unsigned\":true", "\"unsigned\":false"))

	with pytest.raises(ValueError, match="blob sha256 mismatch|manifest references unknown blob"):
		load_package_v0(pkg)


def test_driftc_rejects_duplicate_module_id_across_packages(tmp_path: Path) -> None:
	# Create two packages that both provide module `lib`.
	for n in (1, 2):
		root = tmp_path / f"p{n}"
		_write_file(
			root / "lib" / "lib.drift",
			f"""
module lib

export {{ add }}

fn add(a: Int, b: Int) returns Int {{
	return a + b + {n}
}}
""".lstrip(),
		)
		pkg = tmp_path / f"lib{n}.dmp"
		assert driftc_main(["-M", str(root), str(root / "lib" / "lib.drift"), "--emit-package", str(pkg)]) == 0

	# Compile a main module with both packages in the same package root.
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	return lib.add(40, 2)
}
""".lstrip(),
	)
	# Point package-root at tmp_path so both lib1.dmp and lib2.dmp are discovered.
	rc = driftc_main(["-M", str(tmp_path), "--package-root", str(tmp_path), str(tmp_path / "main.drift"), "--emit-ir", str(tmp_path / "out.ll")])
	assert rc != 0


def test_driftc_rejects_type_table_fingerprint_mismatch(tmp_path: Path) -> None:
	# Build a package that forces Float into its TypeTable.
	_write_file(
		tmp_path / "lib" / "lib.drift",
		"""
module lib

export { f }

pub fn f() returns Float {
	return 1.0
}
""".lstrip(),
	)
	pkg = tmp_path / "lib.dmp"
	assert driftc_main(["-M", str(tmp_path), str(tmp_path / "lib" / "lib.drift"), "--emit-package", str(pkg)]) == 0

	# Now compile a main module that does not use Float otherwise. MVP rule requires
	# identical type-table fingerprints, so this should be rejected.
	_write_file(
		tmp_path / "main.drift",
		"""
module main

import lib as lib

fn main() returns Int {
	val x = lib.f()
	return 0
}
""".lstrip(),
	)
	rc = driftc_main(["-M", str(tmp_path), "--package-root", str(tmp_path), str(tmp_path / "main.drift"), "--emit-ir", str(tmp_path / "out.ll")])
	assert rc != 0


[==== File: staged/lang2/tests/parser/test_parse_import_export.py =====]
from __future__ import annotations

from lang2.driftc.parser import ast
from lang2.driftc.parser.parser import parse_program


def test_parse_program_collects_imports_exports_and_functions():
	source = """
module a.b
export { foo, Bar, c.d.* }
import c.d as cd

pub fn foo() returns Int { return 1; }
pub struct Bar { }
"""
	prog = parse_program(source)

	assert prog.module == "a.b"

	assert len(prog.exports) == 1
	export_items = prog.exports[0].items
	assert [type(i) for i in export_items] == [ast.ExportName, ast.ExportName, ast.ExportModuleStar]
	assert [i.name for i in export_items if isinstance(i, ast.ExportName)] == ["foo", "Bar"]
	star_item = next(i for i in export_items if isinstance(i, ast.ExportModuleStar))
	assert star_item.module_path == ["c", "d"]

	assert len(prog.imports) == 1
	imp = prog.imports[0]
	assert imp.path == ["c", "d"]
	assert imp.alias == "cd"

	assert [fn.name for fn in prog.functions] == ["foo"]
	assert prog.functions[0].is_pub is True
	assert [s.name for s in prog.structs] == ["Bar"]
	assert prog.structs[0].is_pub is True

[==== File: staged/lang2/tests/parser/tests/test_parser_exports.py =====]
# vim: set noexpandtab: -*- indent-tabs-mode: t -*-
from __future__ import annotations

import pytest
from lark import UnexpectedInput

from lang2.driftc.parser import ast
from lang2.driftc.parser import parser as p


def test_parse_export_single_name() -> None:
	prog = p.parse_program(
		"""
module m
export { a }
"""
	)
	assert len(prog.exports) == 1
	items = prog.exports[0].items
	assert [type(i) for i in items] == [ast.ExportName]
	assert [i.name for i in items] == ["a"]


def test_parse_export_multiple_names() -> None:
	prog = p.parse_program(
		"""
module m
export { a, b }
"""
	)
	assert len(prog.exports) == 1
	items = prog.exports[0].items
	assert [type(i) for i in items] == [ast.ExportName, ast.ExportName]
	assert [i.name for i in items] == ["a", "b"]


def test_export_trailing_comma_rejected() -> None:
	with pytest.raises(UnexpectedInput):
		p.parse_program(
			"""
module m
export { a, }
"""
		)

[==== File: staged/work/method-match-x-module/work-progress.md =====]
# Method impl matching across modules (workspace)

## Status (visibility detour)
- Spec now uses **pub + export list** for visibility and **module-only imports**.
- `export { module.* }` re-exports are the only cross-module surface mechanism.
- This milestone assumes those rules when defining “visible modules” and “public methods”.
- Workspace impl matching now works across modules after:
  - preserving impl type params during module merge,
  - predeclaring struct schemas before per-module lowering (prevents empty-field instantiations),
  - and including external modules in visibility deps.
- `lang2-driver-suite` passes after updating method-resolution tests to use qualified impl targets.

## Goal
Enable method/impl matching across module boundaries using a workspace-wide impl index and call-site visibility, with deterministic ambiguity handling and production-grade diagnostics.

## Non-goals (for this milestone)
- Full trait solving / where-clause enforcement (only the plumbing needed for lookup).
- Specialization / “most specific impl wins”.
- Cross-package inherent impls for foreign types (defer; see coherence).

## Production requirements to lock in now
- **Workspace-wide impl index**: method resolution must not depend on “current module only” scans.
- **Canonical type identity** across modules (`TypeId` must be globally stable within the compilation unit).
- **Visibility correctness**: candidates are filtered by call-site visibility and method/impl visibility.
- **Deterministic resolution**: stable ordering + explicit ambiguity errors; no “first one wins”.
- **Coherence checks** at link time: catch duplicate/conflicting method definitions early with actionable errors.

## Phase M0 — Rules and invariants
- **Inherent impl orphan rule (recommended)**: an inherent `implement <Type> { ... }` is allowed only if the receiver type is defined in the same **package** (or same compilation unit, depending on your packaging model). This permits cross-module methods inside a package while preventing “impl foreign type” chaos across packages.
- **Multiple impl blocks** are allowed, but enforce **no duplicate method signatures** for the same receiver type (after generics are instantiated, this becomes “no overlapping signatures”; for now, exact duplicates).
- Define method lookup order:
  1. inherent methods
  2. trait methods only if/when trait methods are allowed via dot-call and the trait is in scope (UFCS can be added later)

## Phase M1 — Driver exports a workspace view
Driver must emit per module (in HIR export metadata):
- exported types (type defs + canonical `TypeId`)
- exported callable decls (free fns, ctors/qualified members if applicable)
- impl blocks:
  - impl header: `impl_id`, `def_module`, `visibility`, `type_params` (even if empty), `target_type`, optional `trait_ref`, optional `where_clause`
  - methods: name, full signature (including receiver kind), visibility, span

Implementation detail:
- Build a **workspace module graph** and compute, per module, the set of **visible modules** from:
  - current module
  - its imports
  - re-exports (if supported)
  - (do not require tests to inject `visible_modules` manually; tests should build imports that drive visibility)

## Phase M2 — Build the global impl index (link step)
After all modules are lowered and type identities are canonical:
- Build two indexes (even if traits are not fully implemented yet):
  - **Inherent index**: `(canonical_target_type, method_name) -> [method_candidate]`
  - **Trait index** (stub OK): `(trait_id, canonical_target_type) -> [impl]`, and `(trait_id, method_name) -> [decl]`
- Store enough provenance on each candidate for diagnostics:
  - defining module
  - impl header span
  - method span

Add a link-time validation pass:
- enforce the inherent orphan rule
- detect duplicate inherent methods (same receiver type + same method name + same erased signature) across visible impls in the same package
- emit errors that point to both definitions

## Phase M3 — Method call resolution uses the workspace index
Method lookup flow for `recv.m(args...)`:
1. Resolve receiver expression type to a canonical `TypeId` (or best-known shape; generics can be placeholders).
2. Query inherent index by `(recv_base_type, "m")`.
3. Filter candidates by visibility relative to the call-site module:
   - candidate is usable if `pub`, or defined in the same module, or (later) via explicit re-export rules.
   - also apply method-level visibility, not just impl-level.
4. Apply impl-template matching/unification (existing unification):
   - match receiver type against impl target type
   - compute substitutions
5. Instantiate candidate method signature under substitution.
6. Run overload resolution among remaining candidates.
7. If exactly one best candidate: select it.
8. If none: “no method named … for type …” and include near-misses (same name but not visible; same name but receiver mismatch) when available.
9. If multiple tie for best: ambiguity error listing all tied candidates and their defining modules.

Determinism:
- Resolution must be independent of module iteration order. Enforce a stable sort key for candidates, e.g. `(def_module_id, impl_id, method_id)` before scoring, and tie-break only by explicit rules (never by discovery order).

## Phase M4 — Diagnostics quality bar
Ambiguity:
- show each candidate as: `module::Type.method(sig)` with source spans
- state why ambiguous (equal match score / same conversion cost)

Not visible:
- if a matching method exists but is not visible, emit: “method exists but is not visible here” and point to the definition span

Coherence violation:
- duplicate method signature detected at link time: point to both definitions and recommend disambiguation (rename or remove one, or stop importing both modules if that is a supported fix).

## Tests (high signal)
1. Cross-module success (basic)
   - `m_box`: `struct Box<T>`, `implement<T> Box<T> { pub fn tag(self) ... }`
   - `m_main`: `import m_box; Box<Int>{...}.tag()` resolves

2. Impl in different module than type (same package)
   - `m_types`: `struct S`
   - `m_impls`: `import m_types; implement S { pub fn m(self) ... }`
   - `m_main`: imports `m_types` and `m_impls`; `S{}.m()` resolves

3. Import controls visibility
   - `m_a`: `implement S { pub fn m(self) ... }`
   - `m_b`: `implement S { pub fn m(self) ... }`
   - `m_main`: imports only `m_a` → resolves
   - `m_main2`: imports `m_a` and `m_b` → ambiguity error citing both modules

4. Private method blocked
   - `m_a`: `implement S { fn hidden(self) ... }`
   - `m_main`: imports `m_a`; `S{}.hidden()` errors as “exists but not visible”

5. Generic impl across modules (template matching)
   - `m_a`: `implement<T> Box<Array<T>> { pub fn inner(self) returns T }`
   - `m_main`: `Box<Array<Int>>.inner()` typechecks as `Int`

## Exit criteria
- Multi-module method calls resolve using the global workspace index (no per-module scans at call sites).
- Visibility rules are enforced for methods and impls.
- Ambiguity and coherence diagnostics cite contributing modules and point at source spans.
- Existing single-module method/impl tests pass unchanged.

## After this
- Add trait method lookup (dot-call via in-scope traits) + where-clause enforcement on the same impl index.
- Then proceed to borrow/lifetime upgrades (NLL-lite/place-model expansion).
